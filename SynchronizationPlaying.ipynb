{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49fef9c58c59e904"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T21:59:20.244204Z",
     "start_time": "2023-11-10T21:58:58.504544Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def maximum_weight_aggregation(models_states):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [model_state[param_name] for model_state in models_states]\n",
    "        aggregated_weights[param_name] = torch.tensor(np.max(all_weights, axis=0))\n",
    "    return aggregated_weights, \"MaximumWeight\"\n",
    "\n",
    "def minimum_weight_aggregation(models_states):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [model_state[param_name] for model_state in models_states]\n",
    "        aggregated_weights[param_name] = torch.tensor(np.min(all_weights, axis=0))\n",
    "    return aggregated_weights, \"MinimumWeight\"\n",
    "\n",
    "def median_aggregation(models_states):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [model_state[param_name] for model_state in models_states]\n",
    "        aggregated_weights[param_name] = torch.tensor(np.median(all_weights, axis=0))\n",
    "    return aggregated_weights, \"MedianWeight\"\n",
    "\n",
    "def trimmed_mean_aggregation(models_states, trim_fraction=0.2):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [model_state[param_name] for model_state in models_states]\n",
    "        sorted_weights = np.sort(all_weights, axis=0)\n",
    "        trim_size = int(trim_fraction * len(sorted_weights))\n",
    "        trimmed_weights = sorted_weights[trim_size:-trim_size]\n",
    "        aggregated_weights[param_name] = torch.tensor(np.mean(trimmed_weights, axis=0))\n",
    "    return aggregated_weights, \"TrimmedMean\"\n",
    "\n",
    "def geometric_mean_aggregation(models_states):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [model_state[param_name] for model_state in models_states]\n",
    "        aggregated_weights[param_name] = torch.tensor(np.exp(np.mean(np.log(all_weights), axis=0)))\n",
    "    return aggregated_weights, \"GeometricMean\"\n",
    "\n",
    "def harmonic_mean_aggregation(models_states):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [model_state[param_name] for model_state in models_states]\n",
    "        aggregated_weights[param_name] = torch.tensor(len(all_weights) / np.sum(1.0 / all_weights, axis=0))\n",
    "    return aggregated_weights, \"HarmonicMean\"\n",
    "\n",
    "def weighted_average_aggregation(models_states, model_weights):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [model_state[param_name] for model_state in models_states]\n",
    "        weights_array = np.array(model_weights)\n",
    "        normalized_weights = weights_array / np.sum(weights_array)\n",
    "        aggregated_weights[param_name] = torch.tensor(np.sum(normalized_weights * all_weights, axis=0))\n",
    "    return aggregated_weights, \"WeightedAverage\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T21:59:20.265308Z",
     "start_time": "2023-11-10T21:59:20.247009Z"
    }
   },
   "id": "1b1b7bab0cfc1d9"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Define CarDataSet Class\n",
    "class CarDataSet():\n",
    "\n",
    "    # Define The Initialization\n",
    "    def __init__(self, csv_file, root_dir, transform=None, target_transform=None):\n",
    "        self.cars = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.resize = transforms.Resize((150,150))  # Resize images to a uniform size\n",
    "\n",
    "    # Define The Length Function\n",
    "    def __len__(self):\n",
    "        return len(self.cars)\n",
    "\n",
    "    # Define The Get Item Function\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Pull The Image And Check Settings\n",
    "        img_name = os.path.join(self.root_dir, self.cars.iloc[idx, 0])\n",
    "\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        if image.mode != 'RGB':\n",
    "          image = image.convert('RGB')\n",
    "\n",
    "        # Pull The Label, -1 To Normalize To 0\n",
    "        label = (self.cars.iloc[idx, 5]) - 1\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Define The Dictionary\n",
    "        sample = {'image': image, 'cars': label}\n",
    "\n",
    "        # Return\n",
    "        return sample\n",
    "    \n",
    "# Test Accuracy\n",
    "def test_accuracy(model, test_loader_internal, passed_device, loss_fn):\n",
    "\n",
    "    # Set Parameters\n",
    "    model.to(passed_device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    run = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    # Run Tests\n",
    "    with torch.no_grad():\n",
    "        for test_data_internal in test_loader_internal:\n",
    "            images_test_acc, labels_test_acc = test_data_internal['image'].cuda(), test_data_internal['cars'].cuda()\n",
    "            outputs_test_acc = model(images_test_acc)\n",
    "            _, predicted_test_acc = torch.max(outputs_test_acc.data, 1)\n",
    "            val_loss += (loss_fn(outputs_test_acc, labels_test_acc)).item()\n",
    "            total += labels_test_acc.size(0)\n",
    "            correct += (predicted_test_acc == labels_test_acc).sum().item()\n",
    "            run += 1\n",
    "\n",
    "    # Return\n",
    "    return (100 * correct / total), val_loss/run"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T21:59:20.282336Z",
     "start_time": "2023-11-10T21:59:20.276103Z"
    }
   },
   "id": "1402e23ca647c2fd"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Define Transform\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "    \n",
    "# Load The Data\n",
    "test_data = CarDataSet(csv_file='../SynchronizationProject/stanford_cars_eec174/val_make.csv',\n",
    "                                        root_dir='../SynchronizationProject/stanford_cars_eec174/images/val', transform=transform)\n",
    "\n",
    "train_data = CarDataSet(csv_file='../SynchronizationProject/stanford_cars_eec174/train_make.csv',\n",
    "                                        root_dir='../SynchronizationProject/stanford_cars_eec174/images/train', transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T21:59:20.307300Z",
     "start_time": "2023-11-10T21:59:20.284864Z"
    }
   },
   "id": "b13de7699fa1b606"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "InternalSave = pd.read_csv(\"../SynchronizationProject/SaveData/Test/InfoSave.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T21:59:20.320541Z",
     "start_time": "2023-11-10T21:59:20.305315Z"
    }
   },
   "id": "aecd1efa679fa3f5"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "   Run  BatchSize  LearningRate  TrainAccuracy  TrainLoss  \\\n0    0         32        0.0001      29.766119   2.530721   \n1    1         32        0.0001      22.628991   2.912521   \n2    1         32        0.0001      19.352325   3.014417   \n3    1         32        0.0001      14.903829   3.129956   \n4    2         64        0.0001      25.569260   2.713737   \n5    3         64        0.0001      23.005913   3.016223   \n6    3         64        0.0001      25.058370   2.930626   \n7    3         64        0.0001      20.478625   2.953434   \n8    3         64        0.0001      25.610257   2.911658   \n9    3         64        0.0001      22.036984   3.032949   \n\n                                        TrainHistory  ValAccuracy   ValLoss  \\\n0  {'loss': [3.4930564347435444, 3.41162627351050...    16.673579  3.380552   \n1  {'loss': [3.5299864712883444, 3.43312999500947...     9.373704  3.575293   \n2  {'loss': [3.5531147339764764, 3.45249549641328...    12.982165  3.580856   \n3  {'loss': [3.557936040092917, 3.476790214987362...    12.567399  3.436109   \n4  {'loss': [3.4837466087192297, 3.39986238069832...    15.429282  3.255348   \n5  {'loss': [3.600028001345121, 3.422101405950693...     8.336790  3.701210   \n6  {'loss': [3.5564707059126635, 3.39519774913787...    10.991290  3.682562   \n7  {'loss': [3.598517115299518, 3.457311208431537...     7.341352  3.960901   \n8  {'loss': [3.601519465446472, 3.431241613167983...     8.212360  4.160569   \n9  {'loss': [3.5894748889482937, 3.40311352106241...     9.290751  3.795039   \n\n                                          ValHistory  GPUCount  GPUNumber  \\\n0  {'loss': [3.5342165074850382, 3.59506788692976...         1          1   \n1  {'loss': [3.5124278695959794, 3.49547933904748...         3          1   \n2  {'loss': [3.520307826368432, 3.501597238214392...         3          2   \n3  {'loss': [3.5215154917616593, 3.47128234411540...         3          3   \n4  {'loss': [3.4475874649850944, 3.41609198168704...         1          1   \n5  {'loss': [3.5449120433706987, 3.51157576159427...         5          1   \n6  {'loss': [3.5440946942881535, 3.49703889144094...         5          2   \n7  {'loss': [3.4908311680743567, 3.49983194627259...         5          3   \n8  {'loss': [3.5688967642031217, 3.50579348363374...         5          4   \n9  {'loss': [3.5851886084205224, 3.53122147760893...         5          5   \n\n   PreTrain  SavePathInput                                     SavePathOutput  \n0     False            NaN   ../SynchronizationProject/SaveData/Weights/0.pth  \n1     False            NaN  ../SynchronizationProject/SaveData/Weights/1T0...  \n2     False            NaN  ../SynchronizationProject/SaveData/Weights/1T1...  \n3     False            NaN  ../SynchronizationProject/SaveData/Weights/1T2...  \n4     False            NaN   ../SynchronizationProject/SaveData/Weights/2.pth  \n5     False            NaN  ../SynchronizationProject/SaveData/Weights/3T0...  \n6     False            NaN  ../SynchronizationProject/SaveData/Weights/3T1...  \n7     False            NaN  ../SynchronizationProject/SaveData/Weights/3T2...  \n8     False            NaN  ../SynchronizationProject/SaveData/Weights/3T3...  \n9     False            NaN  ../SynchronizationProject/SaveData/Weights/3T4...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Run</th>\n      <th>BatchSize</th>\n      <th>LearningRate</th>\n      <th>TrainAccuracy</th>\n      <th>TrainLoss</th>\n      <th>TrainHistory</th>\n      <th>ValAccuracy</th>\n      <th>ValLoss</th>\n      <th>ValHistory</th>\n      <th>GPUCount</th>\n      <th>GPUNumber</th>\n      <th>PreTrain</th>\n      <th>SavePathInput</th>\n      <th>SavePathOutput</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>29.766119</td>\n      <td>2.530721</td>\n      <td>{'loss': [3.4930564347435444, 3.41162627351050...</td>\n      <td>16.673579</td>\n      <td>3.380552</td>\n      <td>{'loss': [3.5342165074850382, 3.59506788692976...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/0.pth</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>22.628991</td>\n      <td>2.912521</td>\n      <td>{'loss': [3.5299864712883444, 3.43312999500947...</td>\n      <td>9.373704</td>\n      <td>3.575293</td>\n      <td>{'loss': [3.5124278695959794, 3.49547933904748...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/1T0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>19.352325</td>\n      <td>3.014417</td>\n      <td>{'loss': [3.5531147339764764, 3.45249549641328...</td>\n      <td>12.982165</td>\n      <td>3.580856</td>\n      <td>{'loss': [3.520307826368432, 3.501597238214392...</td>\n      <td>3</td>\n      <td>2</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/1T1...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>14.903829</td>\n      <td>3.129956</td>\n      <td>{'loss': [3.557936040092917, 3.476790214987362...</td>\n      <td>12.567399</td>\n      <td>3.436109</td>\n      <td>{'loss': [3.5215154917616593, 3.47128234411540...</td>\n      <td>3</td>\n      <td>3</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/1T2...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>25.569260</td>\n      <td>2.713737</td>\n      <td>{'loss': [3.4837466087192297, 3.39986238069832...</td>\n      <td>15.429282</td>\n      <td>3.255348</td>\n      <td>{'loss': [3.4475874649850944, 3.41609198168704...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/2.pth</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>23.005913</td>\n      <td>3.016223</td>\n      <td>{'loss': [3.600028001345121, 3.422101405950693...</td>\n      <td>8.336790</td>\n      <td>3.701210</td>\n      <td>{'loss': [3.5449120433706987, 3.51157576159427...</td>\n      <td>5</td>\n      <td>1</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/3T0...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>25.058370</td>\n      <td>2.930626</td>\n      <td>{'loss': [3.5564707059126635, 3.39519774913787...</td>\n      <td>10.991290</td>\n      <td>3.682562</td>\n      <td>{'loss': [3.5440946942881535, 3.49703889144094...</td>\n      <td>5</td>\n      <td>2</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/3T1...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>20.478625</td>\n      <td>2.953434</td>\n      <td>{'loss': [3.598517115299518, 3.457311208431537...</td>\n      <td>7.341352</td>\n      <td>3.960901</td>\n      <td>{'loss': [3.4908311680743567, 3.49983194627259...</td>\n      <td>5</td>\n      <td>3</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/3T2...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>25.610257</td>\n      <td>2.911658</td>\n      <td>{'loss': [3.601519465446472, 3.431241613167983...</td>\n      <td>8.212360</td>\n      <td>4.160569</td>\n      <td>{'loss': [3.5688967642031217, 3.50579348363374...</td>\n      <td>5</td>\n      <td>4</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/3T3...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>22.036984</td>\n      <td>3.032949</td>\n      <td>{'loss': [3.5894748889482937, 3.40311352106241...</td>\n      <td>9.290751</td>\n      <td>3.795039</td>\n      <td>{'loss': [3.5851886084205224, 3.53122147760893...</td>\n      <td>5</td>\n      <td>5</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/3T4...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InternalSave"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T21:59:20.351335Z",
     "start_time": "2023-11-10T21:59:20.315331Z"
    }
   },
   "id": "39cb2c50cdfb5588"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m ProcessedData \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mDataFrame(columns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRun\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBatchSize\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLearningRate\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpochs\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrainAccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrainLoss\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValAccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValLoss\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGPUCount\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMergeType\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPreTrain\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrainTime\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSavePathOutput\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m      3\u001B[0m total_runs \u001B[38;5;241m=\u001B[39m InternalSave[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRun\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39miloc[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m run_number \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, total_runs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "ProcessedData = pd.DataFrame(columns = [\"Run\", \"BatchSize\", \"LearningRate\", \"Epochs\", \"TrainAccuracy\", \"TrainLoss\", \"ValAccuracy\", \"ValLoss\", \"GPUCount\", \"MergeType\", \"PreTrain\", \"TrainTime\", \"SavePathOutput\"])\n",
    "\n",
    "total_runs = InternalSave['Run'].iloc[-1]\n",
    "\n",
    "for run_number in range(0, total_runs + 1):\n",
    "    specific_run_rows = InternalSave[InternalSave['Run'] == run_number]\n",
    "    learning_rate = specific_run_rows[\"LearningRate\"].iloc[0]\n",
    "    batch_size = specific_run_rows[\"BatchSize\"].iloc[0]\n",
    "    epochs = specific_run_rows[\"Epochs\"].iloc[0]\n",
    "    train_time = specific_run_rows[\"TrainTime\"].iloc[0]\n",
    "    pretrain = specific_run_rows[\"PreTrain\"].iloc[0]\n",
    "    \n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "    if len(specific_run_rows) == 1:\n",
    "        \n",
    "        # Model Definition and Final Layer Edit\n",
    "        net = models.resnet50()\n",
    "        net.fc = nn.Linear(net.fc.in_features, 49)\n",
    "        \n",
    "        net.load_state_dict(torch.load(specific_run_rows[\"SavePathOutput\"]))\n",
    "        \n",
    "         # Load Model Onto GPU\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        net.to(device)\n",
    "        \n",
    "        # Loss Function and Optimizer\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        \n",
    "        trainacc, trainloss = test_accuracy(net, train_loader, device, loss_function)\n",
    "        valacc, valloss = test_accuracy(net, test_loader, device, loss_function)\n",
    "        \n",
    "        PATH = '../SynchronizationProject/SaveData/ProcessedWeights/' + str(run_number) + '.pth'\n",
    "        torch.save(net.state_dict(), PATH)\n",
    "        \n",
    "        new_data_row = {\n",
    "            \"Run\": run_number,\n",
    "            \"BatchSize\": batch_size,\n",
    "            \"LearningRate\": learning_rate, \n",
    "            \"Epochs\": epochs,\n",
    "            \"TrainAccuracy\": trainacc,\n",
    "            \"TrainLoss\": trainloss,\n",
    "            \"ValAccuracy\": valacc,\n",
    "            \"ValLoss\": valloss,\n",
    "            \"GPUCount\": 1,\n",
    "            \"MergeType\": \"NONE\",\n",
    "            \"PreTrain\": pretrain, \n",
    "            \"TrainTime\": train_time,\n",
    "            \"SavePathOutput\": PATH\n",
    "        }\n",
    "        \n",
    "        ProcessedData.loc[len(ProcessedData)] = new_data_row\n",
    "        \n",
    "    else:\n",
    "        weightsList = []\n",
    "        for pathindex in range(0,len(specific_run_rows)):\n",
    "            weightsList.append(torch.load(list(specific_run_rows[\"SavePathOutput\"])[pathindex]))\n",
    "        \n",
    "        aggregatedData = []\n",
    "        aggregatedData.append(maximum_weight_aggregation(weightsList))\n",
    "        aggregatedData.append(minimum_weight_aggregation(weightsList))\n",
    "        aggregatedData.append(median_aggregation(weightsList))\n",
    "        aggregatedData.append(trimmed_mean_aggregation(weightsList, trim_fraction=0.2))\n",
    "        aggregatedData.append(geometric_mean_aggregation(weightsList))\n",
    "        aggregatedData.append(harmonic_mean_aggregation(weightsList))\n",
    "        #aggregatedData.append(weighted_average_aggregation(weightsList, model_weights))  Decide on this later    \n",
    "        \n",
    "        for item in aggregatedData:\n",
    "            # Model Definition and Final Layer Edit\n",
    "            net = models.resnet50()\n",
    "            net.fc = nn.Linear(net.fc.in_features, 49)\n",
    "            \n",
    "            net.load_state_dict(item[0])\n",
    "            \n",
    "             # Load Model Onto GPU\n",
    "            device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "            net.to(device)\n",
    "            \n",
    "            # Loss Function and Optimizer\n",
    "            loss_function = nn.CrossEntropyLoss()\n",
    "            \n",
    "            trainacc, trainloss = test_accuracy(net, train_loader, device, loss_function)\n",
    "            valacc, valloss = test_accuracy(net, test_loader, device, loss_function)\n",
    "            \n",
    "            PATH = '../SynchronizationProject/SaveData/ProcessedWeights/' + str(run_number) + str(item[1]) + '.pth'\n",
    "            torch.save(net.state_dict(), PATH)\n",
    "            \n",
    "            new_data_row = {\n",
    "                \"Run\": run_number, \n",
    "                \"BatchSize\": batch_size, \n",
    "                \"LearningRate\": learning_rate, \n",
    "                \"Epochs\": epochs, \n",
    "                \"TrainAccuracy\": trainacc, \n",
    "                \"TrainLoss\": trainloss, \n",
    "                \"ValAccuracy\": valacc, \n",
    "                \"ValLoss\": valloss, \n",
    "                \"GPUCount\": len(specific_run_rows), \n",
    "                \"MergeType\": item[1], \n",
    "                \"PreTrain\": pretrain, \n",
    "                \"TrainTime\": train_time,\n",
    "                \"SavePathOutput\": PATH\n",
    "            }\n",
    "            \n",
    "            ProcessedData.loc[len(ProcessedData)] = new_data_row\n",
    "\n",
    "file_path = '../SynchronizationProject/SaveData/DataFrames/ProcessedData.csv'\n",
    "\n",
    "ProcessedData.to_csv(file_path, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T22:42:17.224261Z",
     "start_time": "2023-11-10T22:42:16.778628Z"
    }
   },
   "id": "36dd4a79de1b0fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8b8e26dae3479492"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

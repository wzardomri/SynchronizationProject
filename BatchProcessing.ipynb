{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f90d5f7a24a79b5b"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set GPU Settings\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "#TURN OFF WHEN ACTUALLY TESTING CODE\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T02:22:15.821513300Z",
     "start_time": "2023-11-15T02:22:11.797823400Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6be1d18d03f78ee"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Define CarDataSet Class\n",
    "class CarDataSet():\n",
    "\n",
    "    # Define The Initialization\n",
    "    def __init__(self, csv_file, root_dir, transform=None, target_transform=None):\n",
    "        self.cars = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.resize = transforms.Resize((150,150))  # Resize images to a uniform size\n",
    "\n",
    "    # Define The Length Function\n",
    "    def __len__(self):\n",
    "        return len(self.cars)\n",
    "\n",
    "    # Define The Get Item Function\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Pull The Image And Check Settings\n",
    "        img_name = os.path.join(self.root_dir, self.cars.iloc[idx, 0])\n",
    "\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        if image.mode != 'RGB':\n",
    "          image = image.convert('RGB')\n",
    "\n",
    "        # Pull The Label, -1 To Normalize To 0\n",
    "        label = (self.cars.iloc[idx, 5]) - 1\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Define The Dictionary\n",
    "        sample = {'image': image, 'cars': label}\n",
    "\n",
    "        # Return\n",
    "        return sample"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T02:22:15.836527Z",
     "start_time": "2023-11-15T02:22:15.826018100Z"
    }
   },
   "id": "759e0bf9fe9a388b"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Define Transform\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "    \n",
    "# Load The Data\n",
    "train_data = CarDataSet(csv_file='../SynchronizationProject/stanford_cars_eec174/train_make.csv',\n",
    "                                        root_dir='../SynchronizationProject/stanford_cars_eec174/images/train', transform=transform)\n",
    "test_data = CarDataSet(csv_file='../SynchronizationProject/stanford_cars_eec174/val_make.csv',\n",
    "                                        root_dir='../SynchronizationProject/stanford_cars_eec174/images/val', transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T02:22:15.882066700Z",
     "start_time": "2023-11-15T02:22:15.837528200Z"
    }
   },
   "id": "c9f8990b2487605"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def show_imgs(dataloader):\n",
    "    # Load Names From The File\n",
    "    with open('../SynchronizationProject/stanford_cars_eec174/names_make.txt', 'r') as file:\n",
    "        names = file.read().splitlines()\n",
    "\n",
    "    # Retrieve The Images And Labels\n",
    "    dataiter = iter(dataloader)\n",
    "    samples = next(dataiter)\n",
    "\n",
    "    images_show_img, labels_show_img = samples['image'], samples['cars']\n",
    "\n",
    "    # Grab Ten Random Indices\n",
    "    shuffled_indices = np.random.permutation(len(images_show_img))\n",
    "    indices = shuffled_indices[:10]\n",
    "\n",
    "    # Create Subplots\n",
    "    figure, axes = plt.subplots(1, 10, figsize=(20, 10))\n",
    "\n",
    "    for i, ax in zip(indices, axes):\n",
    "\n",
    "        # Rearrange Dimensions For Display\n",
    "        cur_image = images_show_img[i].permute(1, 2, 0)\n",
    "\n",
    "        # Convert The Label To An Integer\n",
    "        label_index = int(labels_show_img[i].item())\n",
    "\n",
    "        # Assign The Name Corresponding To The Label Index\n",
    "        cur_label = names[label_index]\n",
    "\n",
    "        # Display The Image\n",
    "        ax.imshow(cur_image)\n",
    "\n",
    "        # Display The Label As Title\n",
    "        ax.set_title(cur_label)\n",
    "\n",
    "        # Turn Off The Axis\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Test Accuracy\n",
    "def test_accuracy(model, test_loader_internal, passed_device, loss_fn):\n",
    "\n",
    "    # Set Parameters\n",
    "    model.to(passed_device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    run = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    # Run Tests\n",
    "    with torch.no_grad():\n",
    "        for test_data_internal in test_loader_internal:\n",
    "            images_test_acc, labels_test_acc = test_data_internal['image'].cuda(), test_data_internal['cars'].cuda()\n",
    "            outputs_test_acc = model(images_test_acc)\n",
    "            _, predicted_test_acc = torch.max(outputs_test_acc.data, 1)\n",
    "            val_loss += (loss_fn(outputs_test_acc, labels_test_acc)).item()\n",
    "            total += labels_test_acc.size(0)\n",
    "            correct += (predicted_test_acc == labels_test_acc).sum().item()\n",
    "            run += 1\n",
    "\n",
    "    # Return\n",
    "    return (100 * correct / total), val_loss/run\n",
    "\n",
    "\n",
    "# Plotting Function\n",
    "def general_plot(data_dict1: dict, data_dict2: dict, param: str, label1: str, label2: str, ylabel: str, reduce_noise: bool):\n",
    "    param_list1 = data_dict1[param]\n",
    "    param_list2 = data_dict2[param]\n",
    "\n",
    "    if reduce_noise:\n",
    "        param_list1.pop(0)\n",
    "        param_list2.pop(0)\n",
    "\n",
    "    # Prepare Plot\n",
    "    xs = [x for x in range(min(len(param_list1), len(param_list2)))]\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.plot(xs, param_list1[:len(xs)], label=label1)\n",
    "    plt.plot(xs, param_list2[:len(xs)], label=label2)\n",
    "    plt.xlabel('Iterations (in batches)')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(f'Training and Validation {ylabel} Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# Plot Learning Curves\n",
    "def plot_learning_curve(train_history_plt_curve: dict, val_history_plt_curve: dict, reduce_noise=True):\n",
    "\n",
    "    # Plot The Info\n",
    "    general_plot(train_history_plt_curve, val_history_plt_curve, 'loss', 'Training Loss', 'Validation Loss', 'Loss', reduce_noise)\n",
    "    general_plot(train_history_plt_curve, val_history_plt_curve, 'accuracy', 'Training Accuracy', 'Validation Accuracy', 'Accuracy', reduce_noise)\n",
    "    \n",
    "# Define and use confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names_confusion):\n",
    "\n",
    "    # Prepare Plot And Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix Frozen')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names_confusion))\n",
    "    plt.xticks(tick_marks, class_names_confusion, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names_confusion)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    # Show Everything\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define Train\n",
    "def train(model, loss_fn, optimizer_train, train_loader_train, val_loader, num_epochs_train, device_train):\n",
    "\n",
    "    # Set Parameters\n",
    "    model.train()\n",
    "    BLUE = '\\033[94m'\n",
    "    RESET = '\\033[0m'\n",
    "\n",
    "    train_history_train = {'loss': [], 'accuracy': []}\n",
    "    val_history_train = {'loss': [], 'accuracy': []}\n",
    "\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    # Iterate Through All Epochs\n",
    "    for epoch in range(num_epochs_train):\n",
    "\n",
    "        # For Loop Parameters\n",
    "        start_time = time.time()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        # Iterate Through The Training Dataset\n",
    "        for i, data_train in enumerate(train_loader_train, 0):\n",
    "            \n",
    "            # Flatten Images And Load Data\n",
    "            images_train, labels_train = data_train['image'].cuda(), data_train['cars'].cuda()\n",
    "\n",
    "            # Zero Collected Gradients At Each Step (basically cleaning)\n",
    "            optimizer_train.zero_grad()\n",
    "\n",
    "            # Forward Propagate\n",
    "            outputs_train = model(images_train)\n",
    "\n",
    "            # Calculate Loss\n",
    "            loss = loss_fn(outputs_train, labels_train)\n",
    "\n",
    "            # Back Propagate\n",
    "            loss.backward()\n",
    "\n",
    "            # Update Weigh Gradsts\n",
    "            optimizer_train.step()\n",
    "\n",
    "            # Calculate Accuracy\n",
    "            _, predicted_train = torch.max(outputs_train.data, 1)\n",
    "            total += labels_train.size(0)\n",
    "            correct += (predicted_train == labels_train).sum().item()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_correct += 100 * correct / total\n",
    "            train_total += 1\n",
    "            \n",
    "        # Evaluate Model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_acc, val_loss = test_accuracy(model, val_loader, device_train, loss_fn)\n",
    "\n",
    "            val_history_train['loss'].append(val_loss)\n",
    "            val_history_train['accuracy'].append(val_acc)\n",
    "\n",
    "            train_history_train['loss'].append(train_loss / train_total)\n",
    "            train_history_train['accuracy'].append(train_correct / train_total)\n",
    "\n",
    "            train_loss = 0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        # At End Of Each Epoch Print Duration And Accuracy\n",
    "        print(f'{BLUE}Epoch [{epoch+1}/{num_epochs_train}]:'f' Duration: {round(time.time() - start_time, 2)}s |'f' Train Acc: {round(train_history_train[\"accuracy\"][-1], 2)} |'f' Train Loss: {round(train_history_train[\"loss\"][-1], 5)}'f' Val Acc: {round(val_history_train[\"accuracy\"][-1], 2)} |'f' Val Loss: {round(val_history_train[\"loss\"][-1], 5)}{RESET}')\n",
    "        print('------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "        # Save Checkpoint\n",
    "        #PATH = '/content/drive/My Drive/WEIGHTSAVES/run' + str('78.8save') + 'epoch' + str(epoch+1) + 'ta' + str(train_history_train[\"accuracy\"][-1])[0:6] + 'tl' + str(train_history_train[\"loss\"][-1])[0:6] + 'va' + str(val_history_train[\"accuracy\"][-1])[0:6] + 'vl' + str(val_history_train[\"loss\"][-1])[0:6] + '.pth'\n",
    "        #print(PATH)\n",
    "        #torch.save(net.state_dict(), PATH)\n",
    "\n",
    "    # Print Final Statistics\n",
    "    #print(f'{BLUE}Total Duration:{round(time.time() - total_start_time, 2)}s |',f'Final Train Acc: {round(train_history_train[\"accuracy\"][-1], 2)} |',f'Final Train Loss: {round(train_history_train[\"loss\"][-1], 5)} |',f'Final Val Acc: {round(val_history_train[\"accuracy\"][-1], 2)} |',f'Final Val Loss: {round(val_history_train[\"loss\"][-1], 5)}{RESET}')\n",
    "\n",
    "    # Return\n",
    "    return train_history_train, val_history_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T02:22:15.885569700Z",
     "start_time": "2023-11-15T02:22:15.868555200Z"
    }
   },
   "id": "c1fa020743f3bc96"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.cpu().numpy()\n",
    " \n",
    "def maximum_weight_aggregation(models_states):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [to_numpy(model_state[param_name]) for model_state in models_states]\n",
    "        aggregated_weights[param_name] = torch.tensor(np.max(all_weights, axis=0))\n",
    "    return aggregated_weights, \"MaximumWeight\"\n",
    "\n",
    "def minimum_weight_aggregation(models_states):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [to_numpy(model_state[param_name]) for model_state in models_states]\n",
    "        aggregated_weights[param_name] = torch.tensor(np.min(all_weights, axis=0))\n",
    "    return aggregated_weights, \"MinimumWeight\"\n",
    "\n",
    "def mean_aggregation(models_states):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [to_numpy(model_state[param_name]) for model_state in models_states]\n",
    "        aggregated_weights[param_name] = torch.tensor(np.mean(all_weights, axis=0))\n",
    "    return aggregated_weights, \"MeanWeight\"\n",
    "\n",
    "def median_aggregation(models_states):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [to_numpy(model_state[param_name]) for model_state in models_states]\n",
    "        aggregated_weights[param_name] = torch.tensor(np.median(all_weights, axis=0))\n",
    "    return aggregated_weights, \"MedianWeight\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T02:22:15.899582300Z",
     "start_time": "2023-11-15T02:22:15.888071600Z"
    }
   },
   "id": "ad4b7d75db541b7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unchanging Definitions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2fc42535bf12767"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#No Pretrained\n",
    "    \"0\": [1, 1, 1, False, 1], #Run 0: batch size 32, learning rate 0.0001, 1 GPU, 50 epochs\n",
    "    \"1\": [1, 1, 3, False, 1], #Run 1: batch size 32, learning rate 0.0001, 3 GPUs, 50 Epochs\n",
    "    \"2\": [1, 1, 6, False, 1], #Run 2: batch size 32, learning rate 0.0001, 6 GPU, 50 Epochs\n",
    "    \"3\": [1, 1, 10, False, 1], #Run 3: batch size 32, learning rate 0.0001, 10 GPUs, 50 Epochs\n",
    "    \n",
    "    \"4\": [1, 10, 1, False, 1], #Run 4: batch size 32, learning rate 0.001, 1 GPU, 50 epochs\n",
    "    \"5\": [1, 10, 3, False, 1], #Run 5: batch size 32, learning rate 0.001, 3 GPUs, 50 Epochs\n",
    "    \"6\": [1, 10, 6, False, 1], #Run 6: batch size 32, learning rate 0.001, 6 GPU, 50 Epochs\n",
    "    \"7\": [1, 10, 10, False, 1], #Run 7: batch size 32, learning rate 0.001, 10 GPUs, 50 Epochs\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"8\": [2, 1, 1, False, 1], #Run 8: batch size 64, learning rate 0.0001, 1 GPU, 50 epochs\n",
    "    \"9\": [2, 1, 3, False, 1], #Run 9: batch size 64, learning rate 0.0001, 3 GPUs, 50 Epochs\n",
    "    \"10\": [2, 1, 6, False, 1], #Run 10: batch size 64, learning rate 0.0001, 6 GPU, 50 Epochs\n",
    "    \"11\": [2, 1, 10, False, 1], #Run 11: batch size 64, learning rate 0.0001, 10 GPUs, 50 Epochs\n",
    "    \n",
    "    \"12\": [2, 10, 1, False, 1], #Run 12: batch size 64, learning rate 0.001, 1 GPU, 50 epochs\n",
    "    \"13\": [2, 10, 3, False, 1], #Run 13: batch size 64, learning rate 0.001, 3 GPUs, 50 Epochs\n",
    "    \"14\": [2, 10, 6, False, 1], #Run 14: batch size 64, learning rate 0.001, 6 GPU, 50 Epochs\n",
    "    \"15\": [2, 10, 10, False, 1], #Run 15: batch size 64, learning rate 0.001, 10 GPUs, 50 Epochs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"16\": [1, 1, 1, False, 2], #Run 16: batch size 32, learning rate 0.0001, 1 GPU, 100 epochs\n",
    "\"17\": [1, 1, 3, False, 2], #Run 17: batch size 32, learning rate 0.0001, 3 GPUs, 100 Epochs\n",
    "\"18\": [1, 1, 6, False, 2], #Run 18: batch size 32, learning rate 0.0001, 6 GPU, 100 Epochs\n",
    "\"19\": [1, 1, 10, False, 2], #Run 19: batch size 32, learning rate 0.0001, 10 GPUs, 100 Epochs\n",
    "\"20\": [1, 10, 1, False, 2], #Run 20: batch size 32, learning rate 0.001, 1 GPU, 100 epochs\n",
    "\"21\": [1, 10, 3, False, 2], #Run 21: batch size 32, learning rate 0.001, 3 GPUs, 100 Epochs\n",
    "\"22\": [1, 10, 6, False, 2], #Run 22: batch size 32, learning rate 0.001, 6 GPU, 100 Epochs\n",
    "\"23\": [1, 10, 10, False, 2], #Run 23: batch size 32, learning rate 0.001, 10 GPUs, 100 Epochs\n",
    "\"24\": [2, 1, 1, False, 2], #Run 24: batch size 64, learning rate 0.0001, 1 GPU, 100 epochs\n",
    "\"25\": [2, 1, 3, False, 2], #Run 25: batch size 64, learning rate 0.0001, 3 GPUs, 100 Epochs\n",
    "\"26\": [2, 1, 6, False, 2], #Run 26: batch size 64, learning rate 0.0001, 6 GPU, 100 Epochs\n",
    "\"27\": [2, 1, 10, False, 2], #Run 27: batch size 64, learning rate 0.0001, 10 GPUs, 100 Epochs\n",
    "\"28\": [2, 10, 1, False, 2], #Run 28: batch size 64, learning rate 0.001, 1 GPU, 100 epochs\n",
    "\"29\": [2, 10, 3, False, 2], #Run 29: batch size 64, learning rate 0.001, 3 GPUs, 100 Epochs\n",
    "\"30\": [2, 10, 6, False, 2], #Run 30: batch size 64, learning rate 0.001, 6 GPU, 100 Epochs\n",
    "\"31\": [2, 10, 10, False, 2] #Run 31: batch size 64, learning rate 0.001, 10 GPUs, 100 Epochs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b970a5230750d972"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#[GPUcount num, pretrained (True=yes, False=no)] , savepath, sync amount\n",
    "\n",
    "operations_dictionary = {\n",
    "    \"0\":[1, True, 'PreSave15', 0],\n",
    "    \n",
    "    \"1\":[3, True, 'PreSave15', 1],\n",
    "    \"2\":[3, True, 'PreSave15', 3],\n",
    "    \"3\":[3, True, 'PreSave15', 5],\n",
    "    \"4\":[3, True, 'PreSave15', 10],\n",
    "    \n",
    "    \"5\":[5, True, 'PreSave15', 1],\n",
    "    \"6\":[5, True, 'PreSave15', 3],\n",
    "    \"7\":[5, True, 'PreSave15', 5],\n",
    "    \"8\":[5, True, 'PreSave15', 10],\n",
    "    \n",
    "    \"9\":[8, True, 'PreSave15', 1],\n",
    "    \"10\":[8, True, 'PreSave15', 3],\n",
    "    \"11\":[8, True, 'PreSave15', 5],\n",
    "    \"12\":[8, True, 'PreSave15', 10]\n",
    "    \n",
    "                         }\n",
    "# Dataset Size\n",
    "DATASET_SIZE = 8192\n",
    "\n",
    "# Define Parameters\n",
    "input_size = (768 * 1024)\n",
    "num_classes = 49\n",
    "\n",
    "InfoSave = pd.DataFrame(columns = [\"Run\", \"BatchSize\", \"LearningRate\", \"Epochs\", \"TrainAccuracy\", \"TrainLoss\", \"TrainHistory\", \"ValAccuracy\", \"ValLoss\", \"ValHistory\", \"GPUCount\", \"PreTrain\", \"SyncAmount\", \"MergeType\", \"TrainTime\", \"SavePathInput\", \"SavePathOutput\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T02:22:15.928613800Z",
     "start_time": "2023-11-15T02:22:15.901083Z"
    }
   },
   "id": "35a43b541b55111f"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# net = models.resnet50()\n",
    "# net.fc = nn.Linear(net.fc.in_features, 49)\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# net.to(device)\n",
    "# \n",
    "# batch_size = 32\n",
    "# \n",
    "# # Import To Dataloaders\n",
    "# train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "# \n",
    "# # Specify Factors\n",
    "# lr = 0.0001\n",
    "# \n",
    "# # Loss Function and Optimizer\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "# \n",
    "# # Run Train\n",
    "# train_history, val_history = train(net, loss_function, optimizer, train_loader, test_loader, 10, device)\n",
    "# \n",
    "# PATH = '../SynchronizationProject/SaveData/PreSave15.pth'\n",
    "# torch.save(net.state_dict(), PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T02:22:15.936620Z",
     "start_time": "2023-11-15T02:22:15.916098500Z"
    }
   },
   "id": "436b9f69919512ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model Setup and Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28c4faa3661b0397"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\u001B[94mEpoch [1/1]: Duration: 111.21s | Train Acc: 36.46 | Train Loss: 2.21354 Val Acc: 18.13 | Val Loss: 3.25462\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "1\n",
      "\u001B[94mEpoch [1/2]: Duration: 51.53s | Train Acc: 39.05 | Train Loss: 2.18804 Val Acc: 15.43 | Val Loss: 3.3857\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 49.53s | Train Acc: 58.3 | Train Loss: 1.47623 Val Acc: 14.56 | Val Loss: 4.12894\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 48.31s | Train Acc: 36.13 | Train Loss: 2.28856 Val Acc: 17.83 | Val Loss: 3.23495\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 48.31s | Train Acc: 60.93 | Train Loss: 1.50716 Val Acc: 14.97 | Val Loss: 3.80633\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.64s | Train Acc: 32.28 | Train Loss: 2.39166 Val Acc: 16.26 | Val Loss: 3.48358\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.62s | Train Acc: 60.13 | Train Loss: 1.44013 Val Acc: 15.8 | Val Loss: 3.64275\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.83s | Train Acc: 51.13 | Train Loss: 1.67464 Val Acc: 17.5 | Val Loss: 3.64178\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 49.15s | Train Acc: 82.2 | Train Loss: 0.73122 Val Acc: 18.37 | Val Loss: 3.65714\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.83s | Train Acc: 58.76 | Train Loss: 1.47262 Val Acc: 16.84 | Val Loss: 3.83328\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 48.59s | Train Acc: 84.64 | Train Loss: 0.57799 Val Acc: 18.46 | Val Loss: 3.73366\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.88s | Train Acc: 57.76 | Train Loss: 1.52756 Val Acc: 15.14 | Val Loss: 3.93024\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.64s | Train Acc: 85.15 | Train Loss: 0.57979 Val Acc: 18.54 | Val Loss: 3.72306\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 48.45s | Train Acc: 68.67 | Train Loss: 1.00742 Val Acc: 16.34 | Val Loss: 4.05746\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.5s | Train Acc: 93.45 | Train Loss: 0.31556 Val Acc: 18.54 | Val Loss: 3.88467\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 48.13s | Train Acc: 82.21 | Train Loss: 0.70751 Val Acc: 12.9 | Val Loss: 4.743\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.87s | Train Acc: 94.08 | Train Loss: 0.24904 Val Acc: 19.16 | Val Loss: 4.07223\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 49.63s | Train Acc: 81.42 | Train Loss: 0.64354 Val Acc: 11.36 | Val Loss: 6.25561\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 49.76s | Train Acc: 95.18 | Train Loss: 0.19505 Val Acc: 19.33 | Val Loss: 3.97923\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 46.96s | Train Acc: 37.38 | Train Loss: 2.16061 Val Acc: 16.51 | Val Loss: 3.37968\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 46.96s | Train Acc: 60.0 | Train Loss: 1.46372 Val Acc: 15.06 | Val Loss: 3.882\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 48.08s | Train Acc: 35.06 | Train Loss: 2.27522 Val Acc: 17.25 | Val Loss: 3.30838\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 48.04s | Train Acc: 62.14 | Train Loss: 1.41752 Val Acc: 16.51 | Val Loss: 3.61178\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.68s | Train Acc: 34.7 | Train Loss: 2.38724 Val Acc: 19.37 | Val Loss: 3.27719\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.45s | Train Acc: 61.24 | Train Loss: 1.41083 Val Acc: 16.8 | Val Loss: 3.57659\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.42s | Train Acc: 49.98 | Train Loss: 1.66257 Val Acc: 17.3 | Val Loss: 3.34469\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.57s | Train Acc: 83.25 | Train Loss: 0.71367 Val Acc: 19.08 | Val Loss: 3.76005\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 48.65s | Train Acc: 58.02 | Train Loss: 1.50009 Val Acc: 16.05 | Val Loss: 3.89566\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.95s | Train Acc: 82.24 | Train Loss: 0.63102 Val Acc: 19.04 | Val Loss: 3.80066\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.55s | Train Acc: 54.83 | Train Loss: 1.51071 Val Acc: 18.46 | Val Loss: 3.96487\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.15s | Train Acc: 83.94 | Train Loss: 0.5505 Val Acc: 19.33 | Val Loss: 3.79284\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.69s | Train Acc: 73.41 | Train Loss: 0.91026 Val Acc: 13.98 | Val Loss: 4.91949\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.34s | Train Acc: 93.13 | Train Loss: 0.27833 Val Acc: 16.88 | Val Loss: 4.10991\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 48.18s | Train Acc: 83.25 | Train Loss: 0.64169 Val Acc: 16.22 | Val Loss: 4.32553\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 48.13s | Train Acc: 95.18 | Train Loss: 0.23947 Val Acc: 16.38 | Val Loss: 4.08265\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.31s | Train Acc: 84.91 | Train Loss: 0.59016 Val Acc: 14.64 | Val Loss: 4.68524\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.5s | Train Acc: 94.99 | Train Loss: 0.20629 Val Acc: 18.95 | Val Loss: 4.00737\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.4s | Train Acc: 36.67 | Train Loss: 2.15792 Val Acc: 16.92 | Val Loss: 3.27709\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.48s | Train Acc: 61.87 | Train Loss: 1.47603 Val Acc: 17.59 | Val Loss: 3.76484\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.88s | Train Acc: 35.98 | Train Loss: 2.26492 Val Acc: 15.39 | Val Loss: 3.38281\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.85s | Train Acc: 60.48 | Train Loss: 1.45776 Val Acc: 15.55 | Val Loss: 3.63925\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 49.13s | Train Acc: 34.19 | Train Loss: 2.4059 Val Acc: 18.13 | Val Loss: 3.34259\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.9s | Train Acc: 57.35 | Train Loss: 1.47117 Val Acc: 16.84 | Val Loss: 3.52074\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 48.1s | Train Acc: 50.32 | Train Loss: 1.67766 Val Acc: 15.93 | Val Loss: 3.8021\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.96s | Train Acc: 81.41 | Train Loss: 0.74521 Val Acc: 17.54 | Val Loss: 3.68782\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.95s | Train Acc: 57.1 | Train Loss: 1.47131 Val Acc: 17.05 | Val Loss: 3.84924\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.92s | Train Acc: 84.17 | Train Loss: 0.57833 Val Acc: 15.64 | Val Loss: 4.11518\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.61s | Train Acc: 56.09 | Train Loss: 1.48119 Val Acc: 18.66 | Val Loss: 3.98074\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 49.02s | Train Acc: 84.66 | Train Loss: 0.57638 Val Acc: 15.89 | Val Loss: 3.96951\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.48s | Train Acc: 71.54 | Train Loss: 0.95406 Val Acc: 16.05 | Val Loss: 4.29938\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.5s | Train Acc: 93.12 | Train Loss: 0.3123 Val Acc: 19.08 | Val Loss: 3.87424\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.94s | Train Acc: 80.0 | Train Loss: 0.70428 Val Acc: 14.64 | Val Loss: 4.43299\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 48.08s | Train Acc: 95.28 | Train Loss: 0.21887 Val Acc: 19.08 | Val Loss: 3.92809\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.46s | Train Acc: 82.36 | Train Loss: 0.63387 Val Acc: 16.47 | Val Loss: 4.5029\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.94s | Train Acc: 94.69 | Train Loss: 0.21684 Val Acc: 18.66 | Val Loss: 3.91946\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.07s | Train Acc: 36.98 | Train Loss: 2.1468 Val Acc: 14.85 | Val Loss: 3.65487\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.39s | Train Acc: 56.94 | Train Loss: 1.50315 Val Acc: 16.3 | Val Loss: 3.64928\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 48.04s | Train Acc: 34.55 | Train Loss: 2.30633 Val Acc: 16.09 | Val Loss: 3.45914\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.86s | Train Acc: 60.3 | Train Loss: 1.46591 Val Acc: 14.43 | Val Loss: 3.94193\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.7s | Train Acc: 32.39 | Train Loss: 2.43763 Val Acc: 18.25 | Val Loss: 3.20263\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.19s | Train Acc: 59.38 | Train Loss: 1.49599 Val Acc: 17.3 | Val Loss: 3.71299\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.5s | Train Acc: 49.7 | Train Loss: 1.61799 Val Acc: 18.75 | Val Loss: 3.58099\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.62s | Train Acc: 83.39 | Train Loss: 0.70319 Val Acc: 16.3 | Val Loss: 3.68666\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.97s | Train Acc: 60.16 | Train Loss: 1.46959 Val Acc: 17.38 | Val Loss: 3.88304\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 48.07s | Train Acc: 83.42 | Train Loss: 0.63475 Val Acc: 17.83 | Val Loss: 3.80055\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 48.57s | Train Acc: 53.93 | Train Loss: 1.65022 Val Acc: 14.56 | Val Loss: 4.33315\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.43s | Train Acc: 82.23 | Train Loss: 0.61889 Val Acc: 16.22 | Val Loss: 3.85998\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.6s | Train Acc: 72.16 | Train Loss: 0.97815 Val Acc: 16.47 | Val Loss: 3.96481\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.51s | Train Acc: 92.87 | Train Loss: 0.3015 Val Acc: 18.46 | Val Loss: 4.17748\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 47.86s | Train Acc: 79.72 | Train Loss: 0.73002 Val Acc: 15.55 | Val Loss: 4.5619\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 48.14s | Train Acc: 95.78 | Train Loss: 0.23114 Val Acc: 19.33 | Val Loss: 3.82797\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [1/2]: Duration: 48.2s | Train Acc: 78.77 | Train Loss: 0.69766 Val Acc: 17.67 | Val Loss: 4.03368\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/2]: Duration: 47.17s | Train Acc: 93.97 | Train Loss: 0.24255 Val Acc: 19.62 | Val Loss: 3.87264\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run Each Set of Models \n",
    "for run_number_key in operations_dictionary:\n",
    "    print(run_number_key)\n",
    "    \n",
    "    # Specify Factors\n",
    "    lr = 0.0001\n",
    "    batch_size = 32\n",
    "    num_epochs = 30\n",
    "    \n",
    "    # Import To Dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "        \n",
    "    # Single GPU\n",
    "    if (operations_dictionary[run_number_key])[0] == 1:\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Model Definition and Final Layer Edit\n",
    "        net = models.resnet50()\n",
    "        net.fc = nn.Linear(net.fc.in_features, 49)\n",
    "\n",
    "        saved_model_path = \"\"\n",
    "\n",
    "        if (operations_dictionary[run_number_key])[1]:\n",
    "            # IF NEEDED TO IMPORT\n",
    "            saved_model_path = '../SynchronizationProject/SaveData/' + str((operations_dictionary[run_number_key])[2]) + '.pth'\n",
    "            net.load_state_dict(torch.load(saved_model_path))\n",
    "\n",
    "        # Load Model Onto GPU\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        net.to(device)\n",
    "\n",
    "        # Loss Function and Optimizer\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "        # Run Train\n",
    "        train_history, val_history = train(net, loss_function, optimizer, train_loader, test_loader, 1, device)\n",
    "\n",
    "        # Save Model\n",
    "        PATH = '../SynchronizationProject/SaveData/Weights/' + str(run_number_key) + '.pth'\n",
    "        torch.save(net.state_dict(), PATH)\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "        trainacc, trainloss = test_accuracy(net, train_loader, device, loss_function)\n",
    "        valacc, valloss = test_accuracy(net, test_loader, device, loss_function)\n",
    "\n",
    "        new_row_data = {\n",
    "                \"Run\": run_number_key,\n",
    "                \"BatchSize\": batch_size,\n",
    "                \"LearningRate\": lr,\n",
    "                \"Epochs\": num_epochs,\n",
    "                \"TrainAccuracy\": trainacc,\n",
    "                \"TrainLoss\": trainloss,\n",
    "                \"ValAccuracy\": valacc,\n",
    "                \"ValLoss\": valloss,\n",
    "                \"GPUCount\": 1,\n",
    "                \"PreTrain\": (operations_dictionary[run_number_key])[1],\n",
    "                \"MergeType\": \"NONE\",\n",
    "                \"SyncAmount\": 1,\n",
    "                \"TrainTime\": total_time,\n",
    "                \"SavePathInput\": saved_model_path,\n",
    "                \"SavePathOutput\": PATH\n",
    "        }\n",
    "\n",
    "        # Adding the new row to the DataFrame\n",
    "        InfoSave.loc[len(InfoSave)] = new_row_data\n",
    "\n",
    "    # Multiple GPU\n",
    "    else:\n",
    "        #amount of gpus to run\n",
    "        amount_of_GPUs_to_run = (operations_dictionary[run_number_key])[0]\n",
    "\n",
    "        #make an array of input data to access\n",
    "        inputDataTrain = []\n",
    "        for datasetNum in range(amount_of_GPUs_to_run):\n",
    "            start_idx = (len(train_loader.dataset) // amount_of_GPUs_to_run) * datasetNum\n",
    "\n",
    "            if datasetNum < (amount_of_GPUs_to_run - 1):\n",
    "                end_idx = (len(train_loader.dataset) // amount_of_GPUs_to_run) * (datasetNum + 1)\n",
    "            else:\n",
    "                end_idx = len(train_loader.dataset)\n",
    "\n",
    "            split_data_set = torch.utils.data.Subset(train_loader.dataset, range(start_idx, end_idx))\n",
    "            split_train_loader = torch.utils.data.DataLoader(dataset=split_data_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            inputDataTrain.append(split_train_loader)\n",
    "\n",
    "        #iterate through each type of synchronizatyion, there are 4\n",
    "        for syncType in range(4):\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Model Definition and Final Layer Edit\n",
    "            net = models.resnet50()\n",
    "            net.fc = nn.Linear(net.fc.in_features, 49)\n",
    "\n",
    "            saved_model_path = \"\"\n",
    "\n",
    "            if (operations_dictionary[run_number_key])[1]:\n",
    "                # IF NEEDED TO IMPORT\n",
    "                saved_model_path = '../SynchronizationProject/SaveData/' + str((operations_dictionary[run_number_key])[2]) + '.pth'\n",
    "                net.load_state_dict(torch.load(saved_model_path))\n",
    "\n",
    "            # Loss Function and Optimizer\n",
    "            loss_function = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "            #iterate through all the syncs\n",
    "            for syncNum in range((operations_dictionary[run_number_key])[3]):\n",
    "                print(\"SyncNum: \" + str(syncNum))\n",
    "                weightsList = []\n",
    "                #iterate through all the GPU trains\n",
    "                for gpuNumRun in range(amount_of_GPUs_to_run):\n",
    "                    print(\"gpuNumRun: \" + str(gpuNumRun))\n",
    "                    # Load Model Onto GPU\n",
    "                    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "                    net.to(device)\n",
    "\n",
    "                    #train\n",
    "                    train_history, val_history = train(net, loss_function, optimizer, inputDataTrain[gpuNumRun], test_loader, num_epochs//(operations_dictionary[run_number_key])[3], device)\n",
    "                    \n",
    "                    weightsList.append(net.state_dict())\n",
    "\n",
    "                    #sync them\n",
    "                if syncType == 0:\n",
    "                    weightOutput, name = maximum_weight_aggregation(weightsList)\n",
    "                elif syncType == 1:\n",
    "                    weightOutput, name = minimum_weight_aggregation(weightsList)\n",
    "                elif syncType == 2:\n",
    "                    weightOutput, name = median_aggregation(weightsList)\n",
    "                elif syncType == 3:\n",
    "                    weightOutput, name = mean_aggregation(weightsList)\n",
    "\n",
    "                net.load_state_dict(weightOutput)\n",
    "                \n",
    "\n",
    "            #save final sync output and move onto next sync\n",
    "\n",
    "            # Save Model\n",
    "            PATH = '../SynchronizationProject/SaveData/Weights/' + str(run_number_key) + str(name) + '.pth'\n",
    "            torch.save(net.state_dict(), PATH)\n",
    "            \n",
    "            print(\"That was \" + name)\n",
    "\n",
    "            total_time = time.time() - start_time\n",
    "\n",
    "            trainacc, trainloss = test_accuracy(net, train_loader, device, loss_function)\n",
    "            valacc, valloss = test_accuracy(net, test_loader, device, loss_function)\n",
    "\n",
    "            new_row_data = {\n",
    "                        \"Run\": run_number_key,\n",
    "                        \"BatchSize\": batch_size,\n",
    "                        \"LearningRate\": lr,\n",
    "                        \"Epochs\": num_epochs,\n",
    "                        \"TrainAccuracy\": trainacc,\n",
    "                        \"TrainLoss\": trainloss,\n",
    "                        \"ValAccuracy\": valacc,\n",
    "                        \"ValLoss\": valloss,\n",
    "                        \"GPUCount\": amount_of_GPUs_to_run,\n",
    "                        \"PreTrain\": (operations_dictionary[run_number_key])[1],\n",
    "                        \"SyncAmount\": (operations_dictionary[run_number_key])[3],\n",
    "                        \"MergeType\": name,\n",
    "                        \"TrainTime\": total_time/amount_of_GPUs_to_run,\n",
    "                        \"SavePathInput\": saved_model_path,\n",
    "                        \"SavePathOutput\": PATH\n",
    "                }\n",
    "\n",
    "            # Adding the new row to the DataFrame\n",
    "            InfoSave.loc[len(InfoSave)] = new_row_data\n",
    "\n",
    "file_path = '../SynchronizationProject/SaveData/DataFrames/InfoSave.csv'\n",
    "\n",
    "InfoSave.to_csv(file_path, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T03:28:30.501992100Z",
     "start_time": "2023-11-15T02:22:15.930615100Z"
    }
   },
   "id": "bb955564ff30a129"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluate The Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "684e622953828794"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "file_path = '../SynchronizationProject/SaveData/DataFrames/InfoSave.csv'\n",
    "\n",
    "InfoSave = pd.read_csv(file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T03:28:30.517505500Z",
     "start_time": "2023-11-15T03:28:30.505495200Z"
    }
   },
   "id": "558a8f968704739e"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "   Run  BatchSize  LearningRate  Epochs  TrainAccuracy  TrainLoss  \\\n0    0         32        0.0001       6      51.651725   1.729311   \n1    1         32        0.0001       6      92.938720   0.291946   \n2    1         32        0.0001       6      93.000123   0.291885   \n3    1         32        0.0001       6      92.828196   0.285888   \n4    1         32        0.0001       6      92.410659   0.309013   \n\n   TrainHistory  ValAccuracy   ValLoss  ValHistory  GPUCount  PreTrain  \\\n0           NaN    19.618416  3.161591         NaN         1      True   \n1           NaN    18.788884  4.010283         NaN         3      True   \n2           NaN    19.411033  3.942265         NaN         3      True   \n3           NaN    18.664455  4.036034         NaN         3      True   \n4           NaN    20.406470  3.894296         NaN         3      True   \n\n   SyncAmount      MergeType   TrainTime  \\\n0           1           NONE  113.309709   \n1           3  MaximumWeight  874.396159   \n2           3  MinimumWeight  858.775420   \n3           3   MedianWeight  866.233554   \n4           3     MeanWeight  860.089021   \n\n                                      SavePathInput  \\\n0  ../SynchronizationProject/SaveData/PreSave15.pth   \n1  ../SynchronizationProject/SaveData/PreSave15.pth   \n2  ../SynchronizationProject/SaveData/PreSave15.pth   \n3  ../SynchronizationProject/SaveData/PreSave15.pth   \n4  ../SynchronizationProject/SaveData/PreSave15.pth   \n\n                                      SavePathOutput  \n0   ../SynchronizationProject/SaveData/Weights/0.pth  \n1  ../SynchronizationProject/SaveData/Weights/1Ma...  \n2  ../SynchronizationProject/SaveData/Weights/1Mi...  \n3  ../SynchronizationProject/SaveData/Weights/1Me...  \n4  ../SynchronizationProject/SaveData/Weights/1Me...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Run</th>\n      <th>BatchSize</th>\n      <th>LearningRate</th>\n      <th>Epochs</th>\n      <th>TrainAccuracy</th>\n      <th>TrainLoss</th>\n      <th>TrainHistory</th>\n      <th>ValAccuracy</th>\n      <th>ValLoss</th>\n      <th>ValHistory</th>\n      <th>GPUCount</th>\n      <th>PreTrain</th>\n      <th>SyncAmount</th>\n      <th>MergeType</th>\n      <th>TrainTime</th>\n      <th>SavePathInput</th>\n      <th>SavePathOutput</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>6</td>\n      <td>51.651725</td>\n      <td>1.729311</td>\n      <td>NaN</td>\n      <td>19.618416</td>\n      <td>3.161591</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>True</td>\n      <td>1</td>\n      <td>NONE</td>\n      <td>113.309709</td>\n      <td>../SynchronizationProject/SaveData/PreSave15.pth</td>\n      <td>../SynchronizationProject/SaveData/Weights/0.pth</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>6</td>\n      <td>92.938720</td>\n      <td>0.291946</td>\n      <td>NaN</td>\n      <td>18.788884</td>\n      <td>4.010283</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>True</td>\n      <td>3</td>\n      <td>MaximumWeight</td>\n      <td>874.396159</td>\n      <td>../SynchronizationProject/SaveData/PreSave15.pth</td>\n      <td>../SynchronizationProject/SaveData/Weights/1Ma...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>6</td>\n      <td>93.000123</td>\n      <td>0.291885</td>\n      <td>NaN</td>\n      <td>19.411033</td>\n      <td>3.942265</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>True</td>\n      <td>3</td>\n      <td>MinimumWeight</td>\n      <td>858.775420</td>\n      <td>../SynchronizationProject/SaveData/PreSave15.pth</td>\n      <td>../SynchronizationProject/SaveData/Weights/1Mi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>6</td>\n      <td>92.828196</td>\n      <td>0.285888</td>\n      <td>NaN</td>\n      <td>18.664455</td>\n      <td>4.036034</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>True</td>\n      <td>3</td>\n      <td>MedianWeight</td>\n      <td>866.233554</td>\n      <td>../SynchronizationProject/SaveData/PreSave15.pth</td>\n      <td>../SynchronizationProject/SaveData/Weights/1Me...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>6</td>\n      <td>92.410659</td>\n      <td>0.309013</td>\n      <td>NaN</td>\n      <td>20.406470</td>\n      <td>3.894296</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>True</td>\n      <td>3</td>\n      <td>MeanWeight</td>\n      <td>860.089021</td>\n      <td>../SynchronizationProject/SaveData/PreSave15.pth</td>\n      <td>../SynchronizationProject/SaveData/Weights/1Me...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InfoSave"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T03:28:30.560542600Z",
     "start_time": "2023-11-15T03:28:30.518506400Z"
    }
   },
   "id": "fc156538971f59c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "14141d9dfb624fae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

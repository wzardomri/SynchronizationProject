{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f90d5f7a24a79b5b"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set GPU Settings\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "#TURN OFF WHEN ACTUALLY TESTING CODE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T00:55:54.824931600Z",
     "start_time": "2023-11-15T00:55:50.930957300Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6be1d18d03f78ee"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Define CarDataSet Class\n",
    "class CarDataSet():\n",
    "\n",
    "    # Define The Initialization\n",
    "    def __init__(self, csv_file, root_dir, transform=None, target_transform=None):\n",
    "        self.cars = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.resize = transforms.Resize((150,150))  # Resize images to a uniform size\n",
    "\n",
    "    # Define The Length Function\n",
    "    def __len__(self):\n",
    "        return len(self.cars)\n",
    "\n",
    "    # Define The Get Item Function\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Pull The Image And Check Settings\n",
    "        img_name = os.path.join(self.root_dir, self.cars.iloc[idx, 0])\n",
    "\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        if image.mode != 'RGB':\n",
    "          image = image.convert('RGB')\n",
    "\n",
    "        # Pull The Label, -1 To Normalize To 0\n",
    "        label = (self.cars.iloc[idx, 5]) - 1\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Define The Dictionary\n",
    "        sample = {'image': image, 'cars': label}\n",
    "\n",
    "        # Return\n",
    "        return sample"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T00:55:54.840445100Z",
     "start_time": "2023-11-15T00:55:54.828934900Z"
    }
   },
   "id": "759e0bf9fe9a388b"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Define Transform\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "    \n",
    "# Load The Data\n",
    "train_data = CarDataSet(csv_file='../SynchronizationProject/stanford_cars_eec174/train_make.csv',\n",
    "                                        root_dir='../SynchronizationProject/stanford_cars_eec174/images/train', transform=transform)\n",
    "test_data = CarDataSet(csv_file='../SynchronizationProject/stanford_cars_eec174/val_make.csv',\n",
    "                                        root_dir='../SynchronizationProject/stanford_cars_eec174/images/val', transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T00:55:54.873974200Z",
     "start_time": "2023-11-15T00:55:54.840945700Z"
    }
   },
   "id": "c9f8990b2487605"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def show_imgs(dataloader):\n",
    "    # Load Names From The File\n",
    "    with open('../SynchronizationProject/stanford_cars_eec174/names_make.txt', 'r') as file:\n",
    "        names = file.read().splitlines()\n",
    "\n",
    "    # Retrieve The Images And Labels\n",
    "    dataiter = iter(dataloader)\n",
    "    samples = next(dataiter)\n",
    "\n",
    "    images_show_img, labels_show_img = samples['image'], samples['cars']\n",
    "\n",
    "    # Grab Ten Random Indices\n",
    "    shuffled_indices = np.random.permutation(len(images_show_img))\n",
    "    indices = shuffled_indices[:10]\n",
    "\n",
    "    # Create Subplots\n",
    "    figure, axes = plt.subplots(1, 10, figsize=(20, 10))\n",
    "\n",
    "    for i, ax in zip(indices, axes):\n",
    "\n",
    "        # Rearrange Dimensions For Display\n",
    "        cur_image = images_show_img[i].permute(1, 2, 0)\n",
    "\n",
    "        # Convert The Label To An Integer\n",
    "        label_index = int(labels_show_img[i].item())\n",
    "\n",
    "        # Assign The Name Corresponding To The Label Index\n",
    "        cur_label = names[label_index]\n",
    "\n",
    "        # Display The Image\n",
    "        ax.imshow(cur_image)\n",
    "\n",
    "        # Display The Label As Title\n",
    "        ax.set_title(cur_label)\n",
    "\n",
    "        # Turn Off The Axis\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Test Accuracy\n",
    "def test_accuracy(model, test_loader_internal, passed_device, loss_fn):\n",
    "\n",
    "    # Set Parameters\n",
    "    model.to(passed_device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    run = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    # Run Tests\n",
    "    with torch.no_grad():\n",
    "        for test_data_internal in test_loader_internal:\n",
    "            images_test_acc, labels_test_acc = test_data_internal['image'].cuda(), test_data_internal['cars'].cuda()\n",
    "            outputs_test_acc = model(images_test_acc)\n",
    "            _, predicted_test_acc = torch.max(outputs_test_acc.data, 1)\n",
    "            val_loss += (loss_fn(outputs_test_acc, labels_test_acc)).item()\n",
    "            total += labels_test_acc.size(0)\n",
    "            correct += (predicted_test_acc == labels_test_acc).sum().item()\n",
    "            run += 1\n",
    "\n",
    "    # Return\n",
    "    return (100 * correct / total), val_loss/run\n",
    "\n",
    "\n",
    "# Plotting Function\n",
    "def general_plot(data_dict1: dict, data_dict2: dict, param: str, label1: str, label2: str, ylabel: str, reduce_noise: bool):\n",
    "    param_list1 = data_dict1[param]\n",
    "    param_list2 = data_dict2[param]\n",
    "\n",
    "    if reduce_noise:\n",
    "        param_list1.pop(0)\n",
    "        param_list2.pop(0)\n",
    "\n",
    "    # Prepare Plot\n",
    "    xs = [x for x in range(min(len(param_list1), len(param_list2)))]\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.plot(xs, param_list1[:len(xs)], label=label1)\n",
    "    plt.plot(xs, param_list2[:len(xs)], label=label2)\n",
    "    plt.xlabel('Iterations (in batches)')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(f'Training and Validation {ylabel} Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# Plot Learning Curves\n",
    "def plot_learning_curve(train_history_plt_curve: dict, val_history_plt_curve: dict, reduce_noise=True):\n",
    "\n",
    "    # Plot The Info\n",
    "    general_plot(train_history_plt_curve, val_history_plt_curve, 'loss', 'Training Loss', 'Validation Loss', 'Loss', reduce_noise)\n",
    "    general_plot(train_history_plt_curve, val_history_plt_curve, 'accuracy', 'Training Accuracy', 'Validation Accuracy', 'Accuracy', reduce_noise)\n",
    "    \n",
    "# Define and use confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names_confusion):\n",
    "\n",
    "    # Prepare Plot And Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix Frozen')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names_confusion))\n",
    "    plt.xticks(tick_marks, class_names_confusion, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names_confusion)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    # Show Everything\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define Train\n",
    "def train(model, loss_fn, optimizer_train, train_loader_train, val_loader, num_epochs_train, device_train):\n",
    "\n",
    "    # Set Parameters\n",
    "    model.train()\n",
    "    BLUE = '\\033[94m'\n",
    "    RESET = '\\033[0m'\n",
    "\n",
    "    train_history_train = {'loss': [], 'accuracy': []}\n",
    "    val_history_train = {'loss': [], 'accuracy': []}\n",
    "\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    # Iterate Through All Epochs\n",
    "    for epoch in range(num_epochs_train):\n",
    "\n",
    "        # For Loop Parameters\n",
    "        start_time = time.time()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        # Iterate Through The Training Dataset\n",
    "        for i, data_train in enumerate(train_loader_train, 0):\n",
    "            \n",
    "            # Flatten Images And Load Data\n",
    "            images_train, labels_train = data_train['image'].cuda(), data_train['cars'].cuda()\n",
    "\n",
    "            # Zero Collected Gradients At Each Step (basically cleaning)\n",
    "            optimizer_train.zero_grad()\n",
    "\n",
    "            # Forward Propagate\n",
    "            outputs_train = model(images_train)\n",
    "\n",
    "            # Calculate Loss\n",
    "            loss = loss_function(outputs_train, labels_train)\n",
    "\n",
    "            # Back Propagate\n",
    "            loss.backward()\n",
    "\n",
    "            # Update Weigh Gradsts\n",
    "            optimizer_train.step()\n",
    "\n",
    "            # Calculate Accuracy\n",
    "            _, predicted_train = torch.max(outputs_train.data, 1)\n",
    "            total += labels_train.size(0)\n",
    "            correct += (predicted_train == labels_train).sum().item()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_correct += 100 * correct / total\n",
    "            train_total += 1\n",
    "            \n",
    "        # Evaluate Model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_acc, val_loss = test_accuracy(model, val_loader, device_train, loss_fn)\n",
    "\n",
    "            val_history_train['loss'].append(val_loss)\n",
    "            val_history_train['accuracy'].append(val_acc)\n",
    "\n",
    "            train_history_train['loss'].append(train_loss / train_total)\n",
    "            train_history_train['accuracy'].append(train_correct / train_total)\n",
    "\n",
    "            train_loss = 0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        # At End Of Each Epoch Print Duration And Accuracy\n",
    "        print(f'{BLUE}Epoch [{epoch+1}/{num_epochs_train}]:'f' Duration: {round(time.time() - start_time, 2)}s |'f' Train Acc: {round(train_history_train[\"accuracy\"][-1], 2)} |'f' Train Loss: {round(train_history_train[\"loss\"][-1], 5)}'f' Val Acc: {round(val_history_train[\"accuracy\"][-1], 2)} |'f' Val Loss: {round(val_history_train[\"loss\"][-1], 5)}{RESET}')\n",
    "        print('------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "        # Save Checkpoint\n",
    "        #PATH = '/content/drive/My Drive/WEIGHTSAVES/run' + str('78.8save') + 'epoch' + str(epoch+1) + 'ta' + str(train_history_train[\"accuracy\"][-1])[0:6] + 'tl' + str(train_history_train[\"loss\"][-1])[0:6] + 'va' + str(val_history_train[\"accuracy\"][-1])[0:6] + 'vl' + str(val_history_train[\"loss\"][-1])[0:6] + '.pth'\n",
    "        #print(PATH)\n",
    "        #torch.save(net.state_dict(), PATH)\n",
    "\n",
    "    # Print Final Statistics\n",
    "    #print(f'{BLUE}Total Duration:{round(time.time() - total_start_time, 2)}s |',f'Final Train Acc: {round(train_history_train[\"accuracy\"][-1], 2)} |',f'Final Train Loss: {round(train_history_train[\"loss\"][-1], 5)} |',f'Final Val Acc: {round(val_history_train[\"accuracy\"][-1], 2)} |',f'Final Val Loss: {round(val_history_train[\"loss\"][-1], 5)}{RESET}')\n",
    "\n",
    "    # Return\n",
    "    return train_history_train, val_history_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T00:55:54.918011800Z",
     "start_time": "2023-11-15T00:55:54.872472700Z"
    }
   },
   "id": "c1fa020743f3bc96"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.cpu().numpy()\n",
    " \n",
    "def maximum_weight_aggregation(models_states):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [to_numpy(model_state[param_name]) for model_state in models_states]\n",
    "        aggregated_weights[param_name] = torch.tensor(np.max(all_weights, axis=0))\n",
    "    return aggregated_weights, \"MaximumWeight\"\n",
    "\n",
    "def minimum_weight_aggregation(models_states):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [to_numpy(model_state[param_name]) for model_state in models_states]\n",
    "        aggregated_weights[param_name] = torch.tensor(np.min(all_weights, axis=0))\n",
    "    return aggregated_weights, \"MinimumWeight\"\n",
    "\n",
    "def mean_aggregation(models_states):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [to_numpy(model_state[param_name]) for model_state in models_states]\n",
    "        aggregated_weights[param_name] = torch.tensor(np.mean(all_weights, axis=0))\n",
    "    return aggregated_weights, \"MeanWeight\"\n",
    "\n",
    "def median_aggregation(models_states):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [to_numpy(model_state[param_name]) for model_state in models_states]\n",
    "        aggregated_weights[param_name] = torch.tensor(np.median(all_weights, axis=0))\n",
    "    return aggregated_weights, \"MedianWeight\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T00:55:54.918512200Z",
     "start_time": "2023-11-15T00:55:54.903499700Z"
    }
   },
   "id": "ad4b7d75db541b7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unchanging Definitions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2fc42535bf12767"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#No Pretrained\n",
    "    \"0\": [1, 1, 1, False, 1], #Run 0: batch size 32, learning rate 0.0001, 1 GPU, 50 epochs\n",
    "    \"1\": [1, 1, 3, False, 1], #Run 1: batch size 32, learning rate 0.0001, 3 GPUs, 50 Epochs\n",
    "    \"2\": [1, 1, 6, False, 1], #Run 2: batch size 32, learning rate 0.0001, 6 GPU, 50 Epochs\n",
    "    \"3\": [1, 1, 10, False, 1], #Run 3: batch size 32, learning rate 0.0001, 10 GPUs, 50 Epochs\n",
    "    \n",
    "    \"4\": [1, 10, 1, False, 1], #Run 4: batch size 32, learning rate 0.001, 1 GPU, 50 epochs\n",
    "    \"5\": [1, 10, 3, False, 1], #Run 5: batch size 32, learning rate 0.001, 3 GPUs, 50 Epochs\n",
    "    \"6\": [1, 10, 6, False, 1], #Run 6: batch size 32, learning rate 0.001, 6 GPU, 50 Epochs\n",
    "    \"7\": [1, 10, 10, False, 1], #Run 7: batch size 32, learning rate 0.001, 10 GPUs, 50 Epochs\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"8\": [2, 1, 1, False, 1], #Run 8: batch size 64, learning rate 0.0001, 1 GPU, 50 epochs\n",
    "    \"9\": [2, 1, 3, False, 1], #Run 9: batch size 64, learning rate 0.0001, 3 GPUs, 50 Epochs\n",
    "    \"10\": [2, 1, 6, False, 1], #Run 10: batch size 64, learning rate 0.0001, 6 GPU, 50 Epochs\n",
    "    \"11\": [2, 1, 10, False, 1], #Run 11: batch size 64, learning rate 0.0001, 10 GPUs, 50 Epochs\n",
    "    \n",
    "    \"12\": [2, 10, 1, False, 1], #Run 12: batch size 64, learning rate 0.001, 1 GPU, 50 epochs\n",
    "    \"13\": [2, 10, 3, False, 1], #Run 13: batch size 64, learning rate 0.001, 3 GPUs, 50 Epochs\n",
    "    \"14\": [2, 10, 6, False, 1], #Run 14: batch size 64, learning rate 0.001, 6 GPU, 50 Epochs\n",
    "    \"15\": [2, 10, 10, False, 1], #Run 15: batch size 64, learning rate 0.001, 10 GPUs, 50 Epochs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"16\": [1, 1, 1, False, 2], #Run 16: batch size 32, learning rate 0.0001, 1 GPU, 100 epochs\n",
    "\"17\": [1, 1, 3, False, 2], #Run 17: batch size 32, learning rate 0.0001, 3 GPUs, 100 Epochs\n",
    "\"18\": [1, 1, 6, False, 2], #Run 18: batch size 32, learning rate 0.0001, 6 GPU, 100 Epochs\n",
    "\"19\": [1, 1, 10, False, 2], #Run 19: batch size 32, learning rate 0.0001, 10 GPUs, 100 Epochs\n",
    "\"20\": [1, 10, 1, False, 2], #Run 20: batch size 32, learning rate 0.001, 1 GPU, 100 epochs\n",
    "\"21\": [1, 10, 3, False, 2], #Run 21: batch size 32, learning rate 0.001, 3 GPUs, 100 Epochs\n",
    "\"22\": [1, 10, 6, False, 2], #Run 22: batch size 32, learning rate 0.001, 6 GPU, 100 Epochs\n",
    "\"23\": [1, 10, 10, False, 2], #Run 23: batch size 32, learning rate 0.001, 10 GPUs, 100 Epochs\n",
    "\"24\": [2, 1, 1, False, 2], #Run 24: batch size 64, learning rate 0.0001, 1 GPU, 100 epochs\n",
    "\"25\": [2, 1, 3, False, 2], #Run 25: batch size 64, learning rate 0.0001, 3 GPUs, 100 Epochs\n",
    "\"26\": [2, 1, 6, False, 2], #Run 26: batch size 64, learning rate 0.0001, 6 GPU, 100 Epochs\n",
    "\"27\": [2, 1, 10, False, 2], #Run 27: batch size 64, learning rate 0.0001, 10 GPUs, 100 Epochs\n",
    "\"28\": [2, 10, 1, False, 2], #Run 28: batch size 64, learning rate 0.001, 1 GPU, 100 epochs\n",
    "\"29\": [2, 10, 3, False, 2], #Run 29: batch size 64, learning rate 0.001, 3 GPUs, 100 Epochs\n",
    "\"30\": [2, 10, 6, False, 2], #Run 30: batch size 64, learning rate 0.001, 6 GPU, 100 Epochs\n",
    "\"31\": [2, 10, 10, False, 2] #Run 31: batch size 64, learning rate 0.001, 10 GPUs, 100 Epochs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b970a5230750d972"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#[GPUcount num, pretrained (True=yes, False=no)] \n",
    "\n",
    "operations_dictionary = {\n",
    "    \"0\":[1, True, 'PreSave15', 0],\n",
    "    \"1\":[3, True, 'PreSave15', 3]\n",
    "                         }\n",
    "# Dataset Size\n",
    "DATASET_SIZE = 8192\n",
    "\n",
    "# Define Parameters\n",
    "input_size = (768 * 1024)\n",
    "num_classes = 49\n",
    "\n",
    "InfoSave = pd.DataFrame(columns = [\"Run\", \"BatchSize\", \"LearningRate\", \"Epochs\", \"TrainAccuracy\", \"TrainLoss\", \"TrainHistory\", \"ValAccuracy\", \"ValLoss\", \"ValHistory\", \"GPUCount\", \"PreTrain\", \"MergeType\", \"TrainTime\", \"SavePathInput\", \"SavePathOutput\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T00:55:54.933024700Z",
     "start_time": "2023-11-15T00:55:54.920514200Z"
    }
   },
   "id": "35a43b541b55111f"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[94mEpoch [1/10]: Duration: 111.61s | Train Acc: 10.08 | Train Loss: 3.49497 Val Acc: 11.24 | Val Loss: 3.46975\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [2/10]: Duration: 111.53s | Train Acc: 12.17 | Train Loss: 3.40937 Val Acc: 8.71 | Val Loss: 3.54531\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [3/10]: Duration: 114.72s | Train Acc: 11.55 | Train Loss: 3.34455 Val Acc: 12.61 | Val Loss: 3.39154\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [4/10]: Duration: 113.13s | Train Acc: 14.19 | Train Loss: 3.27431 Val Acc: 11.99 | Val Loss: 3.35498\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [5/10]: Duration: 111.28s | Train Acc: 16.04 | Train Loss: 3.18982 Val Acc: 13.81 | Val Loss: 3.25399\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [6/10]: Duration: 112.19s | Train Acc: 16.52 | Train Loss: 3.09459 Val Acc: 13.56 | Val Loss: 3.35888\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [7/10]: Duration: 112.34s | Train Acc: 18.9 | Train Loss: 2.98295 Val Acc: 13.69 | Val Loss: 3.30637\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [8/10]: Duration: 107.48s | Train Acc: 21.33 | Train Loss: 2.86161 Val Acc: 15.01 | Val Loss: 3.24475\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [9/10]: Duration: 111.34s | Train Acc: 25.52 | Train Loss: 2.69922 Val Acc: 14.93 | Val Loss: 3.2864\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "\u001B[94mEpoch [10/10]: Duration: 106.68s | Train Acc: 30.3 | Train Loss: 2.51917 Val Acc: 15.68 | Val Loss: 3.30946\u001B[0m\n",
      "------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# net = models.resnet50()\n",
    "# net.fc = nn.Linear(net.fc.in_features, 49)\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# net.to(device)\n",
    "# \n",
    "# batch_size = 32\n",
    "# \n",
    "# # Import To Dataloaders\n",
    "# train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "# \n",
    "# # Specify Factors\n",
    "# lr = 0.0001\n",
    "# \n",
    "# # Loss Function and Optimizer\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "# \n",
    "# # Run Train\n",
    "# train_history, val_history = train(net, loss_function, optimizer, train_loader, test_loader, 10, device)\n",
    "# \n",
    "# PATH = '../SynchronizationProject/SaveData/PreSave15.pth'\n",
    "# torch.save(net.state_dict(), PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T01:14:29.281625800Z",
     "start_time": "2023-11-15T00:55:54.937028700Z"
    }
   },
   "id": "436b9f69919512ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model Setup and Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28c4faa3661b0397"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Run Each Set of Models \n",
    "for run_number_key in operations_dictionary:\n",
    "    print(run_number_key)\n",
    "    \n",
    "    # Specify Factors\n",
    "    lr = 0.0001\n",
    "    batch_size = 32\n",
    "    num_epochs = 30\n",
    "    \n",
    "    # Import To Dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "        \n",
    "    # Single GPU\n",
    "    if (operations_dictionary[run_number_key])[0] == 1:\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Model Definition and Final Layer Edit\n",
    "        net = models.resnet50()\n",
    "        net.fc = nn.Linear(net.fc.in_features, 49)\n",
    "        \n",
    "        saved_model_path = \"\"\n",
    "        \n",
    "        if (operations_dictionary[run_number_key])[1]:\n",
    "            # IF NEEDED TO IMPORT\n",
    "            saved_model_path = '../SynchronizationProject/SaveData/' + str((operations_dictionary[run_number_key])[2]) + '.pth'\n",
    "            net.load_state_dict(torch.load(saved_model_path))\n",
    "        \n",
    "        # Load Model Onto GPU\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        net.to(device)\n",
    "        \n",
    "        # Loss Function and Optimizer\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "        \n",
    "        # Run Train\n",
    "        train_history, val_history = train(net, loss_function, optimizer, train_loader, test_loader, num_epochs, device)\n",
    "        \n",
    "        # Save Model\n",
    "        PATH = '../SynchronizationProject/SaveData/Weights/' + str(run_number_key) + '.pth'\n",
    "        torch.save(net.state_dict(), PATH)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        trainacc, trainloss = test_accuracy(net, train_loader, device, loss_function)\n",
    "        valacc, valloss = test_accuracy(net, test_loader, device, loss_function)\n",
    "        \n",
    "        new_row_data = {\n",
    "                \"Run\": run_number_key,\n",
    "                \"BatchSize\": batch_size,\n",
    "                \"LearningRate\": lr,\n",
    "                \"Epochs\": num_epochs,\n",
    "                \"TrainAccuracy\": trainacc,\n",
    "                \"TrainLoss\": trainloss,\n",
    "                \"ValAccuracy\": valacc,\n",
    "                \"ValLoss\": valloss,\n",
    "                \"GPUCount\": 1,\n",
    "                \"PreTrain\": (operations_dictionary[run_number_key])[1],\n",
    "                \"MergeType\": \"NONE\",\n",
    "                \"TrainTime\": total_time,\n",
    "                \"SavePathInput\": saved_model_path,\n",
    "                \"SavePathOutput\": PATH\n",
    "        }\n",
    "            \n",
    "        # Adding the new row to the DataFrame\n",
    "        InfoSave.loc[len(InfoSave)] = new_row_data\n",
    "    \n",
    "    # Multiple GPU\n",
    "    else:\n",
    "        #amount of gpus to run\n",
    "        amount_of_GPUs_to_run = (operations_dictionary[run_number_key])[0]\n",
    "        \n",
    "        #make an array of input data to access\n",
    "        inputDataTrain = []\n",
    "        for datasetNum in range(amount_of_GPUs_to_run):\n",
    "            start_idx = (len(train_loader.dataset) // amount_of_GPUs_to_run) * datasetNum\n",
    "            \n",
    "            if datasetNum < (amount_of_GPUs_to_run - 1):\n",
    "                end_idx = (len(train_loader.dataset) // amount_of_GPUs_to_run) * (datasetNum + 1)\n",
    "            else:\n",
    "                end_idx = len(train_loader.dataset)\n",
    "                \n",
    "            split_data_set = torch.utils.data.Subset(train_loader.dataset, range(start_idx, end_idx))\n",
    "            split_train_loader = torch.utils.data.DataLoader(dataset=split_data_set, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            inputDataTrain.appenmd(split_train_loader)\n",
    "        \n",
    "        #iterate through each type of synchronizatyion, there are 4\n",
    "            for type in range(4):\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Model Definition and Final Layer Edit\n",
    "                net = models.resnet50()\n",
    "                net.fc = nn.Linear(net.fc.in_features, 49)\n",
    "                \n",
    "                saved_model_path = \"\"\n",
    "                \n",
    "                if (operations_dictionary[run_number_key])[1]:\n",
    "                    # IF NEEDED TO IMPORT\n",
    "                    saved_model_path = '../SynchronizationProject/SaveData/' + str((operations_dictionary[run_number_key])[2]) + '.pth'\n",
    "                    net.load_state_dict(torch.load(saved_model_path))\n",
    "                    \n",
    "                # Loss Function and Optimizer\n",
    "                loss_function = nn.CrossEntropyLoss()\n",
    "                optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "                \n",
    "                #iterate through all the syncs\n",
    "                for syncNum in range((operations_dictionary[run_number_key])[3]):\n",
    "                    weightsList = []\n",
    "                    #iterate through all the GPU trains\n",
    "                    for gpuNumRun in range(amount_of_GPUs_to_run):\n",
    "                        # Load Model Onto GPU\n",
    "                        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "                        net.to(device)\n",
    "                        \n",
    "                        #train\n",
    "                        train_history, val_history = train(net, loss_function, optimizer, inputDataTrain[gpuNumRun], test_loader, num_epochs/(operations_dictionary[run_number_key])[3], device)\n",
    "                        \n",
    "                \n",
    "                    #sync them\n",
    "                    if type == 0:\n",
    "                        weightOutput, name = maximum_weight_aggregation(weightsList)\n",
    "                    elif type == 1:\n",
    "                        weightOutput, name = minimum_weight_aggregation(weightsList)\n",
    "                    elif type == 2:\n",
    "                        weightOutput, name = median_aggregation(weightsList)\n",
    "                    elif type == 3:\n",
    "                        weightOutput, name = mean_aggregation(weightsList)\n",
    "                        \n",
    "                    net.load_state_dict(weightOutput)\n",
    "                \n",
    "                #save final sync output and move onto next sync\n",
    "                \n",
    "                \n",
    "                # Save Model\n",
    "                PATH = '../SynchronizationProject/SaveData/Weights/' + str(run_number_key) + str(name) + '.pth'\n",
    "                torch.save(net.state_dict(), PATH)\n",
    "            \n",
    "                total_time = time.time() - start_time\n",
    "                \n",
    "                trainacc, trainloss = test_accuracy(net, train_loader, device, loss_function)\n",
    "                valacc, valloss = test_accuracy(net, test_loader, device, loss_function)\n",
    "                \n",
    "                new_row_data = {\n",
    "                        \"Run\": run_number_key,\n",
    "                        \"BatchSize\": batch_size,\n",
    "                        \"LearningRate\": lr,\n",
    "                        \"Epochs\": num_epochs,\n",
    "                        \"TrainAccuracy\": trainacc,\n",
    "                        \"TrainLoss\": trainloss,\n",
    "                        \"ValAccuracy\": valacc,\n",
    "                        \"ValLoss\": valloss,\n",
    "                        \"GPUCount\": 1,\n",
    "                        \"PreTrain\": (operations_dictionary[run_number_key])[1],\n",
    "                        \"MergeType\": \"NONE\",\n",
    "                        \"TrainTime\": total_time,\n",
    "                        \"SavePathInput\": saved_model_path,\n",
    "                        \"SavePathOutput\": PATH\n",
    "                }\n",
    "                    \n",
    "                # Adding the new row to the DataFrame\n",
    "                InfoSave.loc[len(InfoSave)] = new_row_data\n",
    "\n",
    "file_path = '../SynchronizationProject/SaveData/DataFrames/InfoSave.csv'\n",
    "\n",
    "InfoSave.to_csv(file_path, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T00:17:10.115485300Z",
     "start_time": "2023-11-15T00:17:10.080454500Z"
    }
   },
   "id": "bb955564ff30a129"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluate The Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "684e622953828794"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "file_path = '../SynchronizationProject/SaveData/DataFrames/InfoSave.csv'\n",
    "\n",
    "InfoSave = pd.read_csv(file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T00:06:35.813054800Z",
     "start_time": "2023-11-13T00:06:35.792926600Z"
    }
   },
   "id": "558a8f968704739e"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "    Run  BatchSize  LearningRate  Epochs  TrainAccuracy  TrainLoss  \\\n0     0         32        0.0001      50      98.891674   0.046048   \n1     1         32        0.0001      50      99.954355   0.006749   \n2     1         32        0.0001      50      99.989637   0.011231   \n3     1         32        0.0001      50      99.990760   0.004735   \n4     2         32        0.0001      50      99.991356   0.025019   \n5     2         32        0.0001      50      99.996572   0.023236   \n6     2         32        0.0001      50      99.766458   0.029173   \n7     2         32        0.0001      50      81.397344   0.835543   \n8     2         32        0.0001      50     100.000000   0.005080   \n9     2         32        0.0001      50      99.510467   0.040453   \n10    3         32        0.0001      50      99.899592   0.020606   \n11    3         32        0.0001      50     100.000000   0.029210   \n12    3         32        0.0001      50      99.863292   0.015140   \n13    3         32        0.0001      50     100.000000   0.008125   \n14    3         32        0.0001      50     100.000000   0.024888   \n15    3         32        0.0001      50      99.935452   0.022765   \n16    3         32        0.0001      50      99.684896   0.041907   \n17    3         32        0.0001      50      97.832002   0.165315   \n18    3         32        0.0001      50     100.000000   0.023033   \n19    3         32        0.0001      50     100.000000   0.013836   \n20    4         32        0.0010      50      97.537730   0.085037   \n21    5         32        0.0010      50      94.768947   0.150173   \n22    5         32        0.0010      50      96.535242   0.137044   \n23    5         32        0.0010      50      94.824229   0.168941   \n24    6         32        0.0010      50      97.171346   0.092408   \n25    6         32        0.0010      50      94.168974   0.166791   \n26    6         32        0.0010      50      96.538913   0.108578   \n27    6         32        0.0010      50      92.137014   0.220413   \n28    6         32        0.0010      50      94.401722   0.180709   \n29    6         32        0.0010      50      94.009839   0.250023   \n30    7         32        0.0010      50      96.866027   0.138168   \n31    7         32        0.0010      50      97.872623   0.082456   \n32    7         32        0.0010      50      93.278629   0.213592   \n33    7         32        0.0010      50      98.190797   0.087813   \n34    7         32        0.0010      50      97.568609   0.113359   \n35    7         32        0.0010      50      99.540984   0.035162   \n36    7         32        0.0010      50      90.278062   0.398487   \n37    7         32        0.0010      50      99.303146   0.032642   \n38    7         32        0.0010      50      98.957139   0.064208   \n39    7         32        0.0010      50      97.433897   0.118367   \n40    8         64        0.0001      50      99.457141   0.021969   \n41    9         64        0.0001      50     100.000000   0.003627   \n42    9         64        0.0001      50     100.000000   0.004016   \n43    9         64        0.0001      50     100.000000   0.006278   \n44   10         64        0.0001      50      98.782213   0.096661   \n45   10         64        0.0001      50      97.697999   0.176410   \n46   10         64        0.0001      50      97.470786   0.155531   \n47   10         64        0.0001      50      97.559694   0.149069   \n48   10         64        0.0001      50      98.079977   0.108215   \n49   10         64        0.0001      50      97.935115   0.156430   \n50   11         64        0.0001      50     100.000000   0.007525   \n51   11         64        0.0001      50     100.000000   0.007456   \n52   11         64        0.0001      50     100.000000   0.007139   \n53   11         64        0.0001      50     100.000000   0.008293   \n54   11         64        0.0001      50     100.000000   0.015677   \n55   11         64        0.0001      50     100.000000   0.007040   \n56   11         64        0.0001      50     100.000000   0.014781   \n57   11         64        0.0001      50     100.000000   0.010684   \n58   11         64        0.0001      50     100.000000   0.008014   \n59   11         64        0.0001      50     100.000000   0.008087   \n60   12         64        0.0010      50      98.160709   0.054901   \n61   13         64        0.0010      50      89.058745   0.301060   \n62   13         64        0.0010      50      98.745270   0.056588   \n63   13         64        0.0010      50      93.750424   0.210002   \n64   14         64        0.0010      50      97.361525   0.122231   \n65   14         64        0.0010      50      97.649043   0.090952   \n66   14         64        0.0010      50      97.791566   0.088232   \n67   14         64        0.0010      50      91.951909   0.292520   \n68   14         64        0.0010      50      96.125900   0.103884   \n69   14         64        0.0010      50      96.847342   0.151132   \n70   15         64        0.0010      50      96.682942   0.158768   \n71   15         64        0.0010      50      98.542794   0.044917   \n72   15         64        0.0010      50      98.195094   0.070662   \n73   15         64        0.0010      50      96.488739   0.156664   \n74   15         64        0.0010      50      96.125224   0.136881   \n75   15         64        0.0010      50      94.594299   0.204781   \n76   15         64        0.0010      50      94.982639   0.172121   \n77   15         64        0.0010      50      96.576995   0.137701   \n78   15         64        0.0010      50      97.842856   0.118109   \n79   15         64        0.0010      50      99.275489   0.030685   \n\n                                         TrainHistory  ValAccuracy    ValLoss  \\\n0   {'loss': [3.4871723436841777, 3.42232744647007...    18.830361   5.840313   \n1   {'loss': [3.5152197248795454, 3.43448365435880...    12.691829   5.167353   \n2   {'loss': [3.5432352767271156, 3.46236025024862...    11.489009   4.928459   \n3   {'loss': [3.547464345483219, 3.464133195316090...    11.198673   5.037350   \n4   {'loss': [3.5846809398296267, 3.42887252985045...     8.917462   5.132797   \n5   {'loss': [3.5873300252958784, 3.44102504641510...     9.498134   5.192931   \n6   {'loss': [3.5875701793404513, 3.41547107142071...     7.839071   5.590741   \n7   {'loss': [3.6037130965742956, 3.48660362598507...     3.649938   9.772663   \n8   {'loss': [3.582980466443439, 3.435957720113355...     8.087930   4.891955   \n9   {'loss': [3.6127316729966985, 3.47479107213574...     8.087930   5.406475   \n10  {'loss': [3.6125582456588745, 3.39769766880915...     9.041891   5.156554   \n11  {'loss': [3.668741042797382, 3.441540094522329...     6.221485   5.501412   \n12  {'loss': [3.5987603114201474, 3.41880879035362...     8.958938   4.938786   \n13  {'loss': [3.588631134766799, 3.385852199334365...     7.507258   4.874160   \n14  {'loss': [3.628707766532898, 3.420784803537222...     8.710079   5.081364   \n15  {'loss': [3.6366372108459473, 3.45038312215071...     6.677727   5.269615   \n16  {'loss': [3.6222604421468882, 3.43511062401991...     6.926586   5.260940   \n17  {'loss': [3.687379213479849, 3.444622534971970...     8.129407   5.923596   \n18  {'loss': [3.548947838636545, 3.375088765070988...     7.051016   5.134748   \n19  {'loss': [3.6292005502260647, 3.47899485551393...     7.880547   5.189722   \n20  {'loss': [3.6168541281831033, 3.45860700513802...    31.812526   5.730566   \n21  {'loss': [3.808353884079877, 3.509502374424654...    10.369141   8.603995   \n22  {'loss': [3.7974901592030244, 3.55690659354714...    10.700954   9.377623   \n23  {'loss': [3.785868022021125, 3.508251164941227...     9.954376   9.199483   \n24  {'loss': [3.9322668009026107, 3.54847223259681...     7.009540   9.579520   \n25  {'loss': [3.9486590762471043, 3.61438806112422...     7.216922   9.222950   \n26  {'loss': [3.9161641154178355, 3.56763345696205...     7.548735  10.045925   \n27  {'loss': [4.1068325375401695, 3.63496881862019...     6.511821  10.308159   \n28  {'loss': [4.073365083960599, 3.623105004776356...     8.834509   9.623161   \n29  {'loss': [4.007464142732842, 3.615209596101627...     7.175446   9.093355   \n30  {'loss': [4.167975664138794, 3.576490484751188...     5.806719   8.707117   \n31  {'loss': [4.209131360054016, 3.58985921052786,...     8.585649  10.608445   \n32  {'loss': [4.2022236218819256, 3.61457456992222...     7.963501   8.754409   \n33  {'loss': [4.0980051664205694, 3.51899213974292...     7.382829  11.087759   \n34  {'loss': [4.1707654641224785, 3.54601085186004...     6.636251   9.113133   \n35  {'loss': [4.23661914238563, 3.6967040300369263...     6.138532   8.660320   \n36  {'loss': [4.0844886761445265, 3.60786463664128...     6.304438   8.718630   \n37  {'loss': [4.205202276890095, 3.638095938242398...     7.797594   9.251209   \n38  {'loss': [4.126786369543809, 3.597488935177142...     7.880547   8.815552   \n39  {'loss': [4.182262090536264, 3.682885876068702...     6.677727  10.055173   \n40  {'loss': [3.4752673469483852, 3.39874230511486...    18.830361   4.804764   \n41  {'loss': [3.5225765372431557, 3.40615989995557...    11.157196   4.892034   \n42  {'loss': [3.5832704277925713, 3.45209615729575...     7.963501   4.955969   \n43  {'loss': [3.5689740790877234, 3.44723749715228...     9.829946   4.982513   \n44  {'loss': [3.5926091454245825, 3.39263166080821...     6.760680   5.653191   \n45  {'loss': [3.5956325964494185, 3.39129951867190...     9.083368   6.002106   \n46  {'loss': [3.615307081829418, 3.41537589376623,...     6.055579   5.783238   \n47  {'loss': [3.603962941603227, 3.44621001590382,...     7.009540   6.092728   \n48  {'loss': [3.5903378291563555, 3.40492072972384...     7.548735   5.669337   \n49  {'loss': [3.6111992705952036, 3.45011018623005...     6.677727   5.738493   \n50  {'loss': [3.6100356395428, 3.3843195621783915,...     7.299876   4.790921   \n51  {'loss': [3.671144650532649, 3.435975551605224...     6.387391   4.697752   \n52  {'loss': [3.649013427587656, 3.377856969833374...     5.101618   4.878855   \n53  {'loss': [3.661108530484713, 3.370904280589177...     5.516383   4.850372   \n54  {'loss': [3.6427544997288632, 3.40572105921231...     4.313563   4.903630   \n55  {'loss': [3.6608412632575402, 3.45740446677574...     5.931149   4.744386   \n56  {'loss': [3.639709179217999, 3.428526144761306...     6.843633   4.919161   \n57  {'loss': [3.7211640431330752, 3.43239654027498...     5.184571   4.910506   \n58  {'loss': [3.6287692876962514, 3.35380545029273...     6.097055   4.813862   \n59  {'loss': [3.7414001868321347, 3.49255295900198...     6.262961   4.891678   \n60  {'loss': [3.6334255151450634, 3.41435468755662...    35.628370   5.086396   \n61  {'loss': [3.8128339745277584, 3.51083443885625...    10.120282   9.795279   \n62  {'loss': [3.790277503257574, 3.464707917945329...     9.124844   9.029173   \n63  {'loss': [3.8421653093293657, 3.54173849904259...    10.576524   9.894145   \n64  {'loss': [4.062078205021945, 3.478994532064958...     8.875985   8.928290   \n65  {'loss': [4.000607035376809, 3.519213123755021...     7.548735   8.714418   \n66  {'loss': [3.9022229476408525, 3.54341476613825...     6.885110   8.845630   \n67  {'loss': [4.005617423491045, 3.55902192809365,...     8.129407   8.676417   \n68  {'loss': [4.151533647017046, 3.505302440036426...     6.511821   9.649386   \n69  {'loss': [3.9594743685288862, 3.56205655228007...     7.714641   8.785065   \n70  {'loss': [4.394792098265428, 3.619070199819711...     7.631688   9.610570   \n71  {'loss': [4.289457632945134, 3.630182009476882...     6.760680   9.599498   \n72  {'loss': [4.214317321777344, 3.568120021086472...     6.677727   9.553718   \n73  {'loss': [4.229607600432176, 3.647032205875103...     5.848196  10.752244   \n74  {'loss': [4.170141696929932, 3.644883284201988...     5.309000  10.351395   \n75  {'loss': [4.1057514777550335, 3.74745035171508...     3.815844  10.712571   \n76  {'loss': [4.1091077877924995, 3.72035032052260...     7.009540   8.698870   \n77  {'loss': [4.296231306516207, 3.586264573610746...     6.885110   9.257041   \n78  {'loss': [4.12328287271353, 3.6071051634274998...     5.723766   8.788563   \n79  {'loss': [4.1716841000777025, 3.59731839253352...     5.350477  10.164243   \n\n                                           ValHistory  GPUCount  GPUNumber  \\\n0   {'loss': [3.490916973666141, 3.408613242601093...         1          1   \n1   {'loss': [3.5044365682099996, 3.46959972381591...         3          1   \n2   {'loss': [3.492495696795614, 3.476980432083732...         3          2   \n3   {'loss': [3.549210316256473, 3.463777341340717...         3          3   \n4   {'loss': [3.548779399771439, 3.598160019046382...         6          1   \n5   {'loss': [3.5128325819969177, 3.52327824580042...         6          2   \n6   {'loss': [3.5278035370927108, 3.51810194002954...         6          3   \n7   {'loss': [3.5434660221401013, 3.52629286364505...         6          4   \n8   {'loss': [3.5835642438185844, 3.55822451804813...         6          5   \n9   {'loss': [3.551482981757114, 3.532131151149147...         6          6   \n10  {'loss': [3.5582837744763025, 3.55816134653593...        10          1   \n11  {'loss': [3.5678286489687467, 3.55979183473085...        10          2   \n12  {'loss': [3.532979798944373, 3.528331069569839...        10          3   \n13  {'loss': [3.5156548525157727, 3.52418160124828...        10          4   \n14  {'loss': [3.549185015653309, 3.510552083191118...        10          5   \n15  {'loss': [3.5802198522969295, 3.54667249792500...        10          6   \n16  {'loss': [3.531464601817884, 3.535331813912643...        10          7   \n17  {'loss': [3.5404006995652852, 3.55056489141363...        10          8   \n18  {'loss': [3.510953046773609, 3.569566312589143...        10          9   \n19  {'loss': [3.580647951678226, 3.582242269265024...        10         10   \n20  {'loss': [3.571258755106675, 3.484293950231452...         1          1   \n21  {'loss': [3.578121157068955, 7.02315125026201,...         3          1   \n22  {'loss': [3.501602204222428, 3.544102637391341...         3          2   \n23  {'loss': [3.869841145841699, 4.187156739987825...         3          3   \n24  {'loss': [3.503973342870411, 3.514798901583019...         6          1   \n25  {'loss': [3.49925907975749, 3.4879414533313953...         6          2   \n26  {'loss': [3.7113213476381803, 3.47077982362947...         6          3   \n27  {'loss': [4.467963315938649, 3.52200365380237,...         6          4   \n28  {'loss': [3.4979868844935766, 3.92850737508974...         6          5   \n29  {'loss': [5.124311252644188, 144.4161839736135...         6          6   \n30  {'loss': [3.5062876092760185, 3.49071860940832...        10          1   \n31  {'loss': [3.492552233369727, 3.483126555618486...        10          2   \n32  {'loss': [3.691865397127051, 4.966937018068213...        10          3   \n33  {'loss': [3.499444045518574, 3.470241885436208...        10          4   \n34  {'loss': [3.498315004925979, 3.494650618026131...        10          5   \n35  {'loss': [4.119960242196133, 3.836904488111797...        10          6   \n36  {'loss': [3.4965221317190873, 4.08426018765098...        10          7   \n37  {'loss': [3.5215980253721537, 3.51303099017394...        10          8   \n38  {'loss': [3.51830792427063, 4.419459339819457,...        10          9   \n39  {'loss': [3.5591002263520894, 48.1680102348327...        10         10   \n40  {'loss': [3.451139857894496, 3.558351115176552...         1          1   \n41  {'loss': [3.5103508986924825, 3.48718911723086...         3          1   \n42  {'loss': [3.523044222279599, 3.478426719966688...         3          2   \n43  {'loss': [3.509210837514777, 3.541169643402099...         3          3   \n44  {'loss': [3.5297789134477315, 3.57121585544786...         6          1   \n45  {'loss': [3.514100275541607, 3.527647438802217...         6          2   \n46  {'loss': [3.511612183169315, 3.532908220040171...         6          3   \n47  {'loss': [3.5301240431635, 3.510372331267909, ...         6          4   \n48  {'loss': [3.510721539196215, 3.507230143798025...         6          5   \n49  {'loss': [3.5914768607992875, 3.57863617570776...         6          6   \n50  {'loss': [3.593373505692733, 3.537259233625311...        10          1   \n51  {'loss': [3.5953395554893897, 3.58015107481103...        10          2   \n52  {'loss': [3.594554436834235, 3.528131472437005...        10          3   \n53  {'loss': [3.5921983342421684, 3.51235741690585...        10          4   \n54  {'loss': [3.5688486099243164, 3.55388704099153...        10          5   \n55  {'loss': [3.577138386274639, 3.547462231234500...        10          6   \n56  {'loss': [3.5989096541153756, 3.55005622537512...        10          7   \n57  {'loss': [3.602614296110053, 3.514524384548789...        10          8   \n58  {'loss': [3.56782855485615, 3.5582790500239323...        10          9   \n59  {'loss': [3.643012429538526, 3.641953449500234...        10         10   \n60  {'loss': [3.461279166372199, 4.086497300549557...         1          1   \n61  {'loss': [3.4595940050325895, 4.47043458411568...         3          1   \n62  {'loss': [3.4643074085837915, 3.40596359027059...         3          2   \n63  {'loss': [3.4910862759539953, 3.64446068437475...         3          3   \n64  {'loss': [3.535175869339391, 3.978757996308176...         6          1   \n65  {'loss': [3.515540072792455, 3.486195953268753...         6          2   \n66  {'loss': [3.5788046811756336, 3.49449615729482...         6          3   \n67  {'loss': [3.4950587435772547, 3.51957102825767...         6          4   \n68  {'loss': [3.5118322874370373, 3.47049676117144...         6          5   \n69  {'loss': [3.5451922730395666, 3.49317868759757...         6          6   \n70  {'loss': [3.7630655640049984, 3.57485403512653...        10          1   \n71  {'loss': [3.722934277434098, 3.619631026920519...        10          2   \n72  {'loss': [3.644758657405251, 3.532710131845976...        10          3   \n73  {'loss': [3.6207344782979867, 3.48568734369779...        10          4   \n74  {'loss': [3.5438591053611352, 3.64079531870390...        10          5   \n75  {'loss': [3.54023992387872, 3.5036307259609827...        10          6   \n76  {'loss': [3.5532967040413306, 3.48865530992809...        10          7   \n77  {'loss': [3.596015873708223, 3.476873178231089...        10          8   \n78  {'loss': [3.575199949113946, 3.524588001401801...        10          9   \n79  {'loss': [3.5736288396935714, 3.61792811594511...        10         10   \n\n    PreTrain     TrainTime  SavePathInput  \\\n0      False   6513.744140            NaN   \n1      False   2789.132076            NaN   \n2      False   2805.588549            NaN   \n3      False   2776.819473            NaN   \n4      False   1857.319781            NaN   \n5      False   1863.976217            NaN   \n6      False   1875.593079            NaN   \n7      False   1859.868212            NaN   \n8      False   1857.146929            NaN   \n9      False   1851.784572            NaN   \n10     False   1481.568472            NaN   \n11     False   1493.983501            NaN   \n12     False   1484.139128            NaN   \n13     False   1507.645696            NaN   \n14     False   1480.427212            NaN   \n15     False   1497.356544            NaN   \n16     False   1485.220817            NaN   \n17     False   1478.054139            NaN   \n18     False   1481.537151            NaN   \n19     False   1473.713882            NaN   \n20     False   6429.806071            NaN   \n21     False   2754.441091            NaN   \n22     False   2775.852890            NaN   \n23     False   2750.779142            NaN   \n24     False   1838.804984            NaN   \n25     False   1844.396796            NaN   \n26     False   1839.868320            NaN   \n27     False   1802.637554            NaN   \n28     False   1799.800363            NaN   \n29     False   1793.753424            NaN   \n30     False   1445.178545            NaN   \n31     False   1448.681885            NaN   \n32     False   1446.714885            NaN   \n33     False   1455.176610            NaN   \n34     False   1431.521641            NaN   \n35     False   1446.992846            NaN   \n36     False   1434.215472            NaN   \n37     False   1434.797303            NaN   \n38     False   1438.525306            NaN   \n39     False   1441.168944            NaN   \n40     False  11854.723287            NaN   \n41     False   4368.144659            NaN   \n42     False   5156.463375            NaN   \n43     False   4641.796921            NaN   \n44     False   2652.433984            NaN   \n45     False   2581.104477            NaN   \n46     False   2609.458617            NaN   \n47     False   2945.735850            NaN   \n48     False   2955.119725            NaN   \n49     False   3230.108186            NaN   \n50     False   2103.584454            NaN   \n51     False   2408.626962            NaN   \n52     False   2359.894165            NaN   \n53     False   2478.288173            NaN   \n54     False   2143.834130            NaN   \n55     False   2166.662323            NaN   \n56     False   2619.348330            NaN   \n57     False   2543.535410            NaN   \n58     False   2567.699404            NaN   \n59     False   2436.853647            NaN   \n60     False  12974.268857            NaN   \n61     False   5298.145567            NaN   \n62     False   5324.182109            NaN   \n63     False   4733.387727            NaN   \n64     False   3260.636246            NaN   \n65     False   2803.745543            NaN   \n66     False   3180.376168            NaN   \n67     False   3154.852710            NaN   \n68     False   3040.439545            NaN   \n69     False   3148.455096            NaN   \n70     False   2707.442865            NaN   \n71     False   1962.843593            NaN   \n72     False   1934.127262            NaN   \n73     False   1989.132240            NaN   \n74     False   1930.447009            NaN   \n75     False   1980.333326            NaN   \n76     False   1936.754572            NaN   \n77     False   1963.545982            NaN   \n78     False   1930.976732            NaN   \n79     False   1963.606548            NaN   \n\n                                       SavePathOutput  \n0    ../SynchronizationProject/SaveData/Weights/0.pth  \n1   ../SynchronizationProject/SaveData/Weights/1T0...  \n2   ../SynchronizationProject/SaveData/Weights/1T1...  \n3   ../SynchronizationProject/SaveData/Weights/1T2...  \n4   ../SynchronizationProject/SaveData/Weights/2T0...  \n5   ../SynchronizationProject/SaveData/Weights/2T1...  \n6   ../SynchronizationProject/SaveData/Weights/2T2...  \n7   ../SynchronizationProject/SaveData/Weights/2T3...  \n8   ../SynchronizationProject/SaveData/Weights/2T4...  \n9   ../SynchronizationProject/SaveData/Weights/2T5...  \n10  ../SynchronizationProject/SaveData/Weights/3T0...  \n11  ../SynchronizationProject/SaveData/Weights/3T1...  \n12  ../SynchronizationProject/SaveData/Weights/3T2...  \n13  ../SynchronizationProject/SaveData/Weights/3T3...  \n14  ../SynchronizationProject/SaveData/Weights/3T4...  \n15  ../SynchronizationProject/SaveData/Weights/3T5...  \n16  ../SynchronizationProject/SaveData/Weights/3T6...  \n17  ../SynchronizationProject/SaveData/Weights/3T7...  \n18  ../SynchronizationProject/SaveData/Weights/3T8...  \n19  ../SynchronizationProject/SaveData/Weights/3T9...  \n20   ../SynchronizationProject/SaveData/Weights/4.pth  \n21  ../SynchronizationProject/SaveData/Weights/5T0...  \n22  ../SynchronizationProject/SaveData/Weights/5T1...  \n23  ../SynchronizationProject/SaveData/Weights/5T2...  \n24  ../SynchronizationProject/SaveData/Weights/6T0...  \n25  ../SynchronizationProject/SaveData/Weights/6T1...  \n26  ../SynchronizationProject/SaveData/Weights/6T2...  \n27  ../SynchronizationProject/SaveData/Weights/6T3...  \n28  ../SynchronizationProject/SaveData/Weights/6T4...  \n29  ../SynchronizationProject/SaveData/Weights/6T5...  \n30  ../SynchronizationProject/SaveData/Weights/7T0...  \n31  ../SynchronizationProject/SaveData/Weights/7T1...  \n32  ../SynchronizationProject/SaveData/Weights/7T2...  \n33  ../SynchronizationProject/SaveData/Weights/7T3...  \n34  ../SynchronizationProject/SaveData/Weights/7T4...  \n35  ../SynchronizationProject/SaveData/Weights/7T5...  \n36  ../SynchronizationProject/SaveData/Weights/7T6...  \n37  ../SynchronizationProject/SaveData/Weights/7T7...  \n38  ../SynchronizationProject/SaveData/Weights/7T8...  \n39  ../SynchronizationProject/SaveData/Weights/7T9...  \n40   ../SynchronizationProject/SaveData/Weights/8.pth  \n41  ../SynchronizationProject/SaveData/Weights/9T0...  \n42  ../SynchronizationProject/SaveData/Weights/9T1...  \n43  ../SynchronizationProject/SaveData/Weights/9T2...  \n44  ../SynchronizationProject/SaveData/Weights/10T...  \n45  ../SynchronizationProject/SaveData/Weights/10T...  \n46  ../SynchronizationProject/SaveData/Weights/10T...  \n47  ../SynchronizationProject/SaveData/Weights/10T...  \n48  ../SynchronizationProject/SaveData/Weights/10T...  \n49  ../SynchronizationProject/SaveData/Weights/10T...  \n50  ../SynchronizationProject/SaveData/Weights/11T...  \n51  ../SynchronizationProject/SaveData/Weights/11T...  \n52  ../SynchronizationProject/SaveData/Weights/11T...  \n53  ../SynchronizationProject/SaveData/Weights/11T...  \n54  ../SynchronizationProject/SaveData/Weights/11T...  \n55  ../SynchronizationProject/SaveData/Weights/11T...  \n56  ../SynchronizationProject/SaveData/Weights/11T...  \n57  ../SynchronizationProject/SaveData/Weights/11T...  \n58  ../SynchronizationProject/SaveData/Weights/11T...  \n59  ../SynchronizationProject/SaveData/Weights/11T...  \n60  ../SynchronizationProject/SaveData/Weights/12.pth  \n61  ../SynchronizationProject/SaveData/Weights/13T...  \n62  ../SynchronizationProject/SaveData/Weights/13T...  \n63  ../SynchronizationProject/SaveData/Weights/13T...  \n64  ../SynchronizationProject/SaveData/Weights/14T...  \n65  ../SynchronizationProject/SaveData/Weights/14T...  \n66  ../SynchronizationProject/SaveData/Weights/14T...  \n67  ../SynchronizationProject/SaveData/Weights/14T...  \n68  ../SynchronizationProject/SaveData/Weights/14T...  \n69  ../SynchronizationProject/SaveData/Weights/14T...  \n70  ../SynchronizationProject/SaveData/Weights/15T...  \n71  ../SynchronizationProject/SaveData/Weights/15T...  \n72  ../SynchronizationProject/SaveData/Weights/15T...  \n73  ../SynchronizationProject/SaveData/Weights/15T...  \n74  ../SynchronizationProject/SaveData/Weights/15T...  \n75  ../SynchronizationProject/SaveData/Weights/15T...  \n76  ../SynchronizationProject/SaveData/Weights/15T...  \n77  ../SynchronizationProject/SaveData/Weights/15T...  \n78  ../SynchronizationProject/SaveData/Weights/15T...  \n79  ../SynchronizationProject/SaveData/Weights/15T...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Run</th>\n      <th>BatchSize</th>\n      <th>LearningRate</th>\n      <th>Epochs</th>\n      <th>TrainAccuracy</th>\n      <th>TrainLoss</th>\n      <th>TrainHistory</th>\n      <th>ValAccuracy</th>\n      <th>ValLoss</th>\n      <th>ValHistory</th>\n      <th>GPUCount</th>\n      <th>GPUNumber</th>\n      <th>PreTrain</th>\n      <th>TrainTime</th>\n      <th>SavePathInput</th>\n      <th>SavePathOutput</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>98.891674</td>\n      <td>0.046048</td>\n      <td>{'loss': [3.4871723436841777, 3.42232744647007...</td>\n      <td>18.830361</td>\n      <td>5.840313</td>\n      <td>{'loss': [3.490916973666141, 3.408613242601093...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>False</td>\n      <td>6513.744140</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/0.pth</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>99.954355</td>\n      <td>0.006749</td>\n      <td>{'loss': [3.5152197248795454, 3.43448365435880...</td>\n      <td>12.691829</td>\n      <td>5.167353</td>\n      <td>{'loss': [3.5044365682099996, 3.46959972381591...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>False</td>\n      <td>2789.132076</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/1T0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>99.989637</td>\n      <td>0.011231</td>\n      <td>{'loss': [3.5432352767271156, 3.46236025024862...</td>\n      <td>11.489009</td>\n      <td>4.928459</td>\n      <td>{'loss': [3.492495696795614, 3.476980432083732...</td>\n      <td>3</td>\n      <td>2</td>\n      <td>False</td>\n      <td>2805.588549</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/1T1...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>99.990760</td>\n      <td>0.004735</td>\n      <td>{'loss': [3.547464345483219, 3.464133195316090...</td>\n      <td>11.198673</td>\n      <td>5.037350</td>\n      <td>{'loss': [3.549210316256473, 3.463777341340717...</td>\n      <td>3</td>\n      <td>3</td>\n      <td>False</td>\n      <td>2776.819473</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/1T2...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>99.991356</td>\n      <td>0.025019</td>\n      <td>{'loss': [3.5846809398296267, 3.42887252985045...</td>\n      <td>8.917462</td>\n      <td>5.132797</td>\n      <td>{'loss': [3.548779399771439, 3.598160019046382...</td>\n      <td>6</td>\n      <td>1</td>\n      <td>False</td>\n      <td>1857.319781</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/2T0...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>99.996572</td>\n      <td>0.023236</td>\n      <td>{'loss': [3.5873300252958784, 3.44102504641510...</td>\n      <td>9.498134</td>\n      <td>5.192931</td>\n      <td>{'loss': [3.5128325819969177, 3.52327824580042...</td>\n      <td>6</td>\n      <td>2</td>\n      <td>False</td>\n      <td>1863.976217</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/2T1...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>99.766458</td>\n      <td>0.029173</td>\n      <td>{'loss': [3.5875701793404513, 3.41547107142071...</td>\n      <td>7.839071</td>\n      <td>5.590741</td>\n      <td>{'loss': [3.5278035370927108, 3.51810194002954...</td>\n      <td>6</td>\n      <td>3</td>\n      <td>False</td>\n      <td>1875.593079</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/2T2...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>81.397344</td>\n      <td>0.835543</td>\n      <td>{'loss': [3.6037130965742956, 3.48660362598507...</td>\n      <td>3.649938</td>\n      <td>9.772663</td>\n      <td>{'loss': [3.5434660221401013, 3.52629286364505...</td>\n      <td>6</td>\n      <td>4</td>\n      <td>False</td>\n      <td>1859.868212</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/2T3...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>100.000000</td>\n      <td>0.005080</td>\n      <td>{'loss': [3.582980466443439, 3.435957720113355...</td>\n      <td>8.087930</td>\n      <td>4.891955</td>\n      <td>{'loss': [3.5835642438185844, 3.55822451804813...</td>\n      <td>6</td>\n      <td>5</td>\n      <td>False</td>\n      <td>1857.146929</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/2T4...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>99.510467</td>\n      <td>0.040453</td>\n      <td>{'loss': [3.6127316729966985, 3.47479107213574...</td>\n      <td>8.087930</td>\n      <td>5.406475</td>\n      <td>{'loss': [3.551482981757114, 3.532131151149147...</td>\n      <td>6</td>\n      <td>6</td>\n      <td>False</td>\n      <td>1851.784572</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/2T5...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>99.899592</td>\n      <td>0.020606</td>\n      <td>{'loss': [3.6125582456588745, 3.39769766880915...</td>\n      <td>9.041891</td>\n      <td>5.156554</td>\n      <td>{'loss': [3.5582837744763025, 3.55816134653593...</td>\n      <td>10</td>\n      <td>1</td>\n      <td>False</td>\n      <td>1481.568472</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/3T0...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>3</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>100.000000</td>\n      <td>0.029210</td>\n      <td>{'loss': [3.668741042797382, 3.441540094522329...</td>\n      <td>6.221485</td>\n      <td>5.501412</td>\n      <td>{'loss': [3.5678286489687467, 3.55979183473085...</td>\n      <td>10</td>\n      <td>2</td>\n      <td>False</td>\n      <td>1493.983501</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/3T1...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>3</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>99.863292</td>\n      <td>0.015140</td>\n      <td>{'loss': [3.5987603114201474, 3.41880879035362...</td>\n      <td>8.958938</td>\n      <td>4.938786</td>\n      <td>{'loss': [3.532979798944373, 3.528331069569839...</td>\n      <td>10</td>\n      <td>3</td>\n      <td>False</td>\n      <td>1484.139128</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/3T2...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>3</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>100.000000</td>\n      <td>0.008125</td>\n      <td>{'loss': [3.588631134766799, 3.385852199334365...</td>\n      <td>7.507258</td>\n      <td>4.874160</td>\n      <td>{'loss': [3.5156548525157727, 3.52418160124828...</td>\n      <td>10</td>\n      <td>4</td>\n      <td>False</td>\n      <td>1507.645696</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/3T3...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>3</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>100.000000</td>\n      <td>0.024888</td>\n      <td>{'loss': [3.628707766532898, 3.420784803537222...</td>\n      <td>8.710079</td>\n      <td>5.081364</td>\n      <td>{'loss': [3.549185015653309, 3.510552083191118...</td>\n      <td>10</td>\n      <td>5</td>\n      <td>False</td>\n      <td>1480.427212</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/3T4...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>3</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>99.935452</td>\n      <td>0.022765</td>\n      <td>{'loss': [3.6366372108459473, 3.45038312215071...</td>\n      <td>6.677727</td>\n      <td>5.269615</td>\n      <td>{'loss': [3.5802198522969295, 3.54667249792500...</td>\n      <td>10</td>\n      <td>6</td>\n      <td>False</td>\n      <td>1497.356544</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/3T5...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>3</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>99.684896</td>\n      <td>0.041907</td>\n      <td>{'loss': [3.6222604421468882, 3.43511062401991...</td>\n      <td>6.926586</td>\n      <td>5.260940</td>\n      <td>{'loss': [3.531464601817884, 3.535331813912643...</td>\n      <td>10</td>\n      <td>7</td>\n      <td>False</td>\n      <td>1485.220817</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/3T6...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>3</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>97.832002</td>\n      <td>0.165315</td>\n      <td>{'loss': [3.687379213479849, 3.444622534971970...</td>\n      <td>8.129407</td>\n      <td>5.923596</td>\n      <td>{'loss': [3.5404006995652852, 3.55056489141363...</td>\n      <td>10</td>\n      <td>8</td>\n      <td>False</td>\n      <td>1478.054139</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/3T7...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>3</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>100.000000</td>\n      <td>0.023033</td>\n      <td>{'loss': [3.548947838636545, 3.375088765070988...</td>\n      <td>7.051016</td>\n      <td>5.134748</td>\n      <td>{'loss': [3.510953046773609, 3.569566312589143...</td>\n      <td>10</td>\n      <td>9</td>\n      <td>False</td>\n      <td>1481.537151</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/3T8...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>3</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>100.000000</td>\n      <td>0.013836</td>\n      <td>{'loss': [3.6292005502260647, 3.47899485551393...</td>\n      <td>7.880547</td>\n      <td>5.189722</td>\n      <td>{'loss': [3.580647951678226, 3.582242269265024...</td>\n      <td>10</td>\n      <td>10</td>\n      <td>False</td>\n      <td>1473.713882</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/3T9...</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>4</td>\n      <td>32</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>97.537730</td>\n      <td>0.085037</td>\n      <td>{'loss': [3.6168541281831033, 3.45860700513802...</td>\n      <td>31.812526</td>\n      <td>5.730566</td>\n      <td>{'loss': [3.571258755106675, 3.484293950231452...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>False</td>\n      <td>6429.806071</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/4.pth</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>5</td>\n      <td>32</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>94.768947</td>\n      <td>0.150173</td>\n      <td>{'loss': [3.808353884079877, 3.509502374424654...</td>\n      <td>10.369141</td>\n      <td>8.603995</td>\n      <td>{'loss': [3.578121157068955, 7.02315125026201,...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>False</td>\n      <td>2754.441091</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/5T0...</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>5</td>\n      <td>32</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>96.535242</td>\n      <td>0.137044</td>\n      <td>{'loss': [3.7974901592030244, 3.55690659354714...</td>\n      <td>10.700954</td>\n      <td>9.377623</td>\n      <td>{'loss': [3.501602204222428, 3.544102637391341...</td>\n      <td>3</td>\n      <td>2</td>\n      <td>False</td>\n      <td>2775.852890</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/5T1...</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>5</td>\n      <td>32</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>94.824229</td>\n      <td>0.168941</td>\n      <td>{'loss': [3.785868022021125, 3.508251164941227...</td>\n      <td>9.954376</td>\n      <td>9.199483</td>\n      <td>{'loss': [3.869841145841699, 4.187156739987825...</td>\n      <td>3</td>\n      <td>3</td>\n      <td>False</td>\n      <td>2750.779142</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/5T2...</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>6</td>\n      <td>32</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>97.171346</td>\n      <td>0.092408</td>\n      <td>{'loss': [3.9322668009026107, 3.54847223259681...</td>\n      <td>7.009540</td>\n      <td>9.579520</td>\n      <td>{'loss': [3.503973342870411, 3.514798901583019...</td>\n      <td>6</td>\n      <td>1</td>\n      <td>False</td>\n      <td>1838.804984</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/6T0...</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>6</td>\n      <td>32</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>94.168974</td>\n      <td>0.166791</td>\n      <td>{'loss': [3.9486590762471043, 3.61438806112422...</td>\n      <td>7.216922</td>\n      <td>9.222950</td>\n      <td>{'loss': [3.49925907975749, 3.4879414533313953...</td>\n      <td>6</td>\n      <td>2</td>\n      <td>False</td>\n      <td>1844.396796</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/6T1...</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>6</td>\n      <td>32</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>96.538913</td>\n      <td>0.108578</td>\n      <td>{'loss': [3.9161641154178355, 3.56763345696205...</td>\n      <td>7.548735</td>\n      <td>10.045925</td>\n      <td>{'loss': [3.7113213476381803, 3.47077982362947...</td>\n      <td>6</td>\n      <td>3</td>\n      <td>False</td>\n      <td>1839.868320</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/6T2...</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>6</td>\n      <td>32</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>92.137014</td>\n      <td>0.220413</td>\n      <td>{'loss': [4.1068325375401695, 3.63496881862019...</td>\n      <td>6.511821</td>\n      <td>10.308159</td>\n      <td>{'loss': [4.467963315938649, 3.52200365380237,...</td>\n      <td>6</td>\n      <td>4</td>\n      <td>False</td>\n      <td>1802.637554</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/6T3...</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>6</td>\n      <td>32</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>94.401722</td>\n      <td>0.180709</td>\n      <td>{'loss': [4.073365083960599, 3.623105004776356...</td>\n      <td>8.834509</td>\n      <td>9.623161</td>\n      <td>{'loss': [3.4979868844935766, 3.92850737508974...</td>\n      <td>6</td>\n      <td>5</td>\n      <td>False</td>\n      <td>1799.800363</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/6T4...</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>6</td>\n      <td>32</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>94.009839</td>\n      <td>0.250023</td>\n      <td>{'loss': [4.007464142732842, 3.615209596101627...</td>\n      <td>7.175446</td>\n      <td>9.093355</td>\n      <td>{'loss': [5.124311252644188, 144.4161839736135...</td>\n      <td>6</td>\n      <td>6</td>\n      <td>False</td>\n      <td>1793.753424</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/6T5...</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>7</td>\n      <td>32</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>96.866027</td>\n      <td>0.138168</td>\n      <td>{'loss': [4.167975664138794, 3.576490484751188...</td>\n      <td>5.806719</td>\n      <td>8.707117</td>\n      <td>{'loss': [3.5062876092760185, 3.49071860940832...</td>\n      <td>10</td>\n      <td>1</td>\n      <td>False</td>\n      <td>1445.178545</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/7T0...</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>7</td>\n      <td>32</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>97.872623</td>\n      <td>0.082456</td>\n      <td>{'loss': [4.209131360054016, 3.58985921052786,...</td>\n      <td>8.585649</td>\n      <td>10.608445</td>\n      <td>{'loss': [3.492552233369727, 3.483126555618486...</td>\n      <td>10</td>\n      <td>2</td>\n      <td>False</td>\n      <td>1448.681885</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/7T1...</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>7</td>\n      <td>32</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>93.278629</td>\n      <td>0.213592</td>\n      <td>{'loss': [4.2022236218819256, 3.61457456992222...</td>\n      <td>7.963501</td>\n      <td>8.754409</td>\n      <td>{'loss': [3.691865397127051, 4.966937018068213...</td>\n      <td>10</td>\n      <td>3</td>\n      <td>False</td>\n      <td>1446.714885</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/7T2...</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>7</td>\n      <td>32</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>98.190797</td>\n      <td>0.087813</td>\n      <td>{'loss': [4.0980051664205694, 3.51899213974292...</td>\n      <td>7.382829</td>\n      <td>11.087759</td>\n      <td>{'loss': [3.499444045518574, 3.470241885436208...</td>\n      <td>10</td>\n      <td>4</td>\n      <td>False</td>\n      <td>1455.176610</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/7T3...</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>7</td>\n      <td>32</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>97.568609</td>\n      <td>0.113359</td>\n      <td>{'loss': [4.1707654641224785, 3.54601085186004...</td>\n      <td>6.636251</td>\n      <td>9.113133</td>\n      <td>{'loss': [3.498315004925979, 3.494650618026131...</td>\n      <td>10</td>\n      <td>5</td>\n      <td>False</td>\n      <td>1431.521641</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/7T4...</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>7</td>\n      <td>32</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>99.540984</td>\n      <td>0.035162</td>\n      <td>{'loss': [4.23661914238563, 3.6967040300369263...</td>\n      <td>6.138532</td>\n      <td>8.660320</td>\n      <td>{'loss': [4.119960242196133, 3.836904488111797...</td>\n      <td>10</td>\n      <td>6</td>\n      <td>False</td>\n      <td>1446.992846</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/7T5...</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>7</td>\n      <td>32</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>90.278062</td>\n      <td>0.398487</td>\n      <td>{'loss': [4.0844886761445265, 3.60786463664128...</td>\n      <td>6.304438</td>\n      <td>8.718630</td>\n      <td>{'loss': [3.4965221317190873, 4.08426018765098...</td>\n      <td>10</td>\n      <td>7</td>\n      <td>False</td>\n      <td>1434.215472</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/7T6...</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>7</td>\n      <td>32</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>99.303146</td>\n      <td>0.032642</td>\n      <td>{'loss': [4.205202276890095, 3.638095938242398...</td>\n      <td>7.797594</td>\n      <td>9.251209</td>\n      <td>{'loss': [3.5215980253721537, 3.51303099017394...</td>\n      <td>10</td>\n      <td>8</td>\n      <td>False</td>\n      <td>1434.797303</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/7T7...</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>7</td>\n      <td>32</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>98.957139</td>\n      <td>0.064208</td>\n      <td>{'loss': [4.126786369543809, 3.597488935177142...</td>\n      <td>7.880547</td>\n      <td>8.815552</td>\n      <td>{'loss': [3.51830792427063, 4.419459339819457,...</td>\n      <td>10</td>\n      <td>9</td>\n      <td>False</td>\n      <td>1438.525306</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/7T8...</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>7</td>\n      <td>32</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>97.433897</td>\n      <td>0.118367</td>\n      <td>{'loss': [4.182262090536264, 3.682885876068702...</td>\n      <td>6.677727</td>\n      <td>10.055173</td>\n      <td>{'loss': [3.5591002263520894, 48.1680102348327...</td>\n      <td>10</td>\n      <td>10</td>\n      <td>False</td>\n      <td>1441.168944</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/7T9...</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>8</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>99.457141</td>\n      <td>0.021969</td>\n      <td>{'loss': [3.4752673469483852, 3.39874230511486...</td>\n      <td>18.830361</td>\n      <td>4.804764</td>\n      <td>{'loss': [3.451139857894496, 3.558351115176552...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>False</td>\n      <td>11854.723287</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/8.pth</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>9</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>100.000000</td>\n      <td>0.003627</td>\n      <td>{'loss': [3.5225765372431557, 3.40615989995557...</td>\n      <td>11.157196</td>\n      <td>4.892034</td>\n      <td>{'loss': [3.5103508986924825, 3.48718911723086...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>False</td>\n      <td>4368.144659</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/9T0...</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>9</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>100.000000</td>\n      <td>0.004016</td>\n      <td>{'loss': [3.5832704277925713, 3.45209615729575...</td>\n      <td>7.963501</td>\n      <td>4.955969</td>\n      <td>{'loss': [3.523044222279599, 3.478426719966688...</td>\n      <td>3</td>\n      <td>2</td>\n      <td>False</td>\n      <td>5156.463375</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/9T1...</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>9</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>100.000000</td>\n      <td>0.006278</td>\n      <td>{'loss': [3.5689740790877234, 3.44723749715228...</td>\n      <td>9.829946</td>\n      <td>4.982513</td>\n      <td>{'loss': [3.509210837514777, 3.541169643402099...</td>\n      <td>3</td>\n      <td>3</td>\n      <td>False</td>\n      <td>4641.796921</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/9T2...</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>10</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>98.782213</td>\n      <td>0.096661</td>\n      <td>{'loss': [3.5926091454245825, 3.39263166080821...</td>\n      <td>6.760680</td>\n      <td>5.653191</td>\n      <td>{'loss': [3.5297789134477315, 3.57121585544786...</td>\n      <td>6</td>\n      <td>1</td>\n      <td>False</td>\n      <td>2652.433984</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/10T...</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>10</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>97.697999</td>\n      <td>0.176410</td>\n      <td>{'loss': [3.5956325964494185, 3.39129951867190...</td>\n      <td>9.083368</td>\n      <td>6.002106</td>\n      <td>{'loss': [3.514100275541607, 3.527647438802217...</td>\n      <td>6</td>\n      <td>2</td>\n      <td>False</td>\n      <td>2581.104477</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/10T...</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>10</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>97.470786</td>\n      <td>0.155531</td>\n      <td>{'loss': [3.615307081829418, 3.41537589376623,...</td>\n      <td>6.055579</td>\n      <td>5.783238</td>\n      <td>{'loss': [3.511612183169315, 3.532908220040171...</td>\n      <td>6</td>\n      <td>3</td>\n      <td>False</td>\n      <td>2609.458617</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/10T...</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>10</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>97.559694</td>\n      <td>0.149069</td>\n      <td>{'loss': [3.603962941603227, 3.44621001590382,...</td>\n      <td>7.009540</td>\n      <td>6.092728</td>\n      <td>{'loss': [3.5301240431635, 3.510372331267909, ...</td>\n      <td>6</td>\n      <td>4</td>\n      <td>False</td>\n      <td>2945.735850</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/10T...</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>10</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>98.079977</td>\n      <td>0.108215</td>\n      <td>{'loss': [3.5903378291563555, 3.40492072972384...</td>\n      <td>7.548735</td>\n      <td>5.669337</td>\n      <td>{'loss': [3.510721539196215, 3.507230143798025...</td>\n      <td>6</td>\n      <td>5</td>\n      <td>False</td>\n      <td>2955.119725</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/10T...</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>10</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>97.935115</td>\n      <td>0.156430</td>\n      <td>{'loss': [3.6111992705952036, 3.45011018623005...</td>\n      <td>6.677727</td>\n      <td>5.738493</td>\n      <td>{'loss': [3.5914768607992875, 3.57863617570776...</td>\n      <td>6</td>\n      <td>6</td>\n      <td>False</td>\n      <td>3230.108186</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/10T...</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>11</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>100.000000</td>\n      <td>0.007525</td>\n      <td>{'loss': [3.6100356395428, 3.3843195621783915,...</td>\n      <td>7.299876</td>\n      <td>4.790921</td>\n      <td>{'loss': [3.593373505692733, 3.537259233625311...</td>\n      <td>10</td>\n      <td>1</td>\n      <td>False</td>\n      <td>2103.584454</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/11T...</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>11</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>100.000000</td>\n      <td>0.007456</td>\n      <td>{'loss': [3.671144650532649, 3.435975551605224...</td>\n      <td>6.387391</td>\n      <td>4.697752</td>\n      <td>{'loss': [3.5953395554893897, 3.58015107481103...</td>\n      <td>10</td>\n      <td>2</td>\n      <td>False</td>\n      <td>2408.626962</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/11T...</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>11</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>100.000000</td>\n      <td>0.007139</td>\n      <td>{'loss': [3.649013427587656, 3.377856969833374...</td>\n      <td>5.101618</td>\n      <td>4.878855</td>\n      <td>{'loss': [3.594554436834235, 3.528131472437005...</td>\n      <td>10</td>\n      <td>3</td>\n      <td>False</td>\n      <td>2359.894165</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/11T...</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>11</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>100.000000</td>\n      <td>0.008293</td>\n      <td>{'loss': [3.661108530484713, 3.370904280589177...</td>\n      <td>5.516383</td>\n      <td>4.850372</td>\n      <td>{'loss': [3.5921983342421684, 3.51235741690585...</td>\n      <td>10</td>\n      <td>4</td>\n      <td>False</td>\n      <td>2478.288173</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/11T...</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>11</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>100.000000</td>\n      <td>0.015677</td>\n      <td>{'loss': [3.6427544997288632, 3.40572105921231...</td>\n      <td>4.313563</td>\n      <td>4.903630</td>\n      <td>{'loss': [3.5688486099243164, 3.55388704099153...</td>\n      <td>10</td>\n      <td>5</td>\n      <td>False</td>\n      <td>2143.834130</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/11T...</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>11</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>100.000000</td>\n      <td>0.007040</td>\n      <td>{'loss': [3.6608412632575402, 3.45740446677574...</td>\n      <td>5.931149</td>\n      <td>4.744386</td>\n      <td>{'loss': [3.577138386274639, 3.547462231234500...</td>\n      <td>10</td>\n      <td>6</td>\n      <td>False</td>\n      <td>2166.662323</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/11T...</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>11</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>100.000000</td>\n      <td>0.014781</td>\n      <td>{'loss': [3.639709179217999, 3.428526144761306...</td>\n      <td>6.843633</td>\n      <td>4.919161</td>\n      <td>{'loss': [3.5989096541153756, 3.55005622537512...</td>\n      <td>10</td>\n      <td>7</td>\n      <td>False</td>\n      <td>2619.348330</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/11T...</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>11</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>100.000000</td>\n      <td>0.010684</td>\n      <td>{'loss': [3.7211640431330752, 3.43239654027498...</td>\n      <td>5.184571</td>\n      <td>4.910506</td>\n      <td>{'loss': [3.602614296110053, 3.514524384548789...</td>\n      <td>10</td>\n      <td>8</td>\n      <td>False</td>\n      <td>2543.535410</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/11T...</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>11</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>100.000000</td>\n      <td>0.008014</td>\n      <td>{'loss': [3.6287692876962514, 3.35380545029273...</td>\n      <td>6.097055</td>\n      <td>4.813862</td>\n      <td>{'loss': [3.56782855485615, 3.5582790500239323...</td>\n      <td>10</td>\n      <td>9</td>\n      <td>False</td>\n      <td>2567.699404</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/11T...</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>11</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>100.000000</td>\n      <td>0.008087</td>\n      <td>{'loss': [3.7414001868321347, 3.49255295900198...</td>\n      <td>6.262961</td>\n      <td>4.891678</td>\n      <td>{'loss': [3.643012429538526, 3.641953449500234...</td>\n      <td>10</td>\n      <td>10</td>\n      <td>False</td>\n      <td>2436.853647</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/11T...</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>12</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>98.160709</td>\n      <td>0.054901</td>\n      <td>{'loss': [3.6334255151450634, 3.41435468755662...</td>\n      <td>35.628370</td>\n      <td>5.086396</td>\n      <td>{'loss': [3.461279166372199, 4.086497300549557...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>False</td>\n      <td>12974.268857</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/12.pth</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>13</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>89.058745</td>\n      <td>0.301060</td>\n      <td>{'loss': [3.8128339745277584, 3.51083443885625...</td>\n      <td>10.120282</td>\n      <td>9.795279</td>\n      <td>{'loss': [3.4595940050325895, 4.47043458411568...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>False</td>\n      <td>5298.145567</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/13T...</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>13</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>98.745270</td>\n      <td>0.056588</td>\n      <td>{'loss': [3.790277503257574, 3.464707917945329...</td>\n      <td>9.124844</td>\n      <td>9.029173</td>\n      <td>{'loss': [3.4643074085837915, 3.40596359027059...</td>\n      <td>3</td>\n      <td>2</td>\n      <td>False</td>\n      <td>5324.182109</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/13T...</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>13</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>93.750424</td>\n      <td>0.210002</td>\n      <td>{'loss': [3.8421653093293657, 3.54173849904259...</td>\n      <td>10.576524</td>\n      <td>9.894145</td>\n      <td>{'loss': [3.4910862759539953, 3.64446068437475...</td>\n      <td>3</td>\n      <td>3</td>\n      <td>False</td>\n      <td>4733.387727</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/13T...</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>14</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>97.361525</td>\n      <td>0.122231</td>\n      <td>{'loss': [4.062078205021945, 3.478994532064958...</td>\n      <td>8.875985</td>\n      <td>8.928290</td>\n      <td>{'loss': [3.535175869339391, 3.978757996308176...</td>\n      <td>6</td>\n      <td>1</td>\n      <td>False</td>\n      <td>3260.636246</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/14T...</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>14</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>97.649043</td>\n      <td>0.090952</td>\n      <td>{'loss': [4.000607035376809, 3.519213123755021...</td>\n      <td>7.548735</td>\n      <td>8.714418</td>\n      <td>{'loss': [3.515540072792455, 3.486195953268753...</td>\n      <td>6</td>\n      <td>2</td>\n      <td>False</td>\n      <td>2803.745543</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/14T...</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>14</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>97.791566</td>\n      <td>0.088232</td>\n      <td>{'loss': [3.9022229476408525, 3.54341476613825...</td>\n      <td>6.885110</td>\n      <td>8.845630</td>\n      <td>{'loss': [3.5788046811756336, 3.49449615729482...</td>\n      <td>6</td>\n      <td>3</td>\n      <td>False</td>\n      <td>3180.376168</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/14T...</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>14</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>91.951909</td>\n      <td>0.292520</td>\n      <td>{'loss': [4.005617423491045, 3.55902192809365,...</td>\n      <td>8.129407</td>\n      <td>8.676417</td>\n      <td>{'loss': [3.4950587435772547, 3.51957102825767...</td>\n      <td>6</td>\n      <td>4</td>\n      <td>False</td>\n      <td>3154.852710</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/14T...</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>14</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>96.125900</td>\n      <td>0.103884</td>\n      <td>{'loss': [4.151533647017046, 3.505302440036426...</td>\n      <td>6.511821</td>\n      <td>9.649386</td>\n      <td>{'loss': [3.5118322874370373, 3.47049676117144...</td>\n      <td>6</td>\n      <td>5</td>\n      <td>False</td>\n      <td>3040.439545</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/14T...</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>14</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>96.847342</td>\n      <td>0.151132</td>\n      <td>{'loss': [3.9594743685288862, 3.56205655228007...</td>\n      <td>7.714641</td>\n      <td>8.785065</td>\n      <td>{'loss': [3.5451922730395666, 3.49317868759757...</td>\n      <td>6</td>\n      <td>6</td>\n      <td>False</td>\n      <td>3148.455096</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/14T...</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>15</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>96.682942</td>\n      <td>0.158768</td>\n      <td>{'loss': [4.394792098265428, 3.619070199819711...</td>\n      <td>7.631688</td>\n      <td>9.610570</td>\n      <td>{'loss': [3.7630655640049984, 3.57485403512653...</td>\n      <td>10</td>\n      <td>1</td>\n      <td>False</td>\n      <td>2707.442865</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/15T...</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>15</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>98.542794</td>\n      <td>0.044917</td>\n      <td>{'loss': [4.289457632945134, 3.630182009476882...</td>\n      <td>6.760680</td>\n      <td>9.599498</td>\n      <td>{'loss': [3.722934277434098, 3.619631026920519...</td>\n      <td>10</td>\n      <td>2</td>\n      <td>False</td>\n      <td>1962.843593</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/15T...</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>15</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>98.195094</td>\n      <td>0.070662</td>\n      <td>{'loss': [4.214317321777344, 3.568120021086472...</td>\n      <td>6.677727</td>\n      <td>9.553718</td>\n      <td>{'loss': [3.644758657405251, 3.532710131845976...</td>\n      <td>10</td>\n      <td>3</td>\n      <td>False</td>\n      <td>1934.127262</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/15T...</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>15</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>96.488739</td>\n      <td>0.156664</td>\n      <td>{'loss': [4.229607600432176, 3.647032205875103...</td>\n      <td>5.848196</td>\n      <td>10.752244</td>\n      <td>{'loss': [3.6207344782979867, 3.48568734369779...</td>\n      <td>10</td>\n      <td>4</td>\n      <td>False</td>\n      <td>1989.132240</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/15T...</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>15</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>96.125224</td>\n      <td>0.136881</td>\n      <td>{'loss': [4.170141696929932, 3.644883284201988...</td>\n      <td>5.309000</td>\n      <td>10.351395</td>\n      <td>{'loss': [3.5438591053611352, 3.64079531870390...</td>\n      <td>10</td>\n      <td>5</td>\n      <td>False</td>\n      <td>1930.447009</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/15T...</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>15</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>94.594299</td>\n      <td>0.204781</td>\n      <td>{'loss': [4.1057514777550335, 3.74745035171508...</td>\n      <td>3.815844</td>\n      <td>10.712571</td>\n      <td>{'loss': [3.54023992387872, 3.5036307259609827...</td>\n      <td>10</td>\n      <td>6</td>\n      <td>False</td>\n      <td>1980.333326</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/15T...</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>15</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>94.982639</td>\n      <td>0.172121</td>\n      <td>{'loss': [4.1091077877924995, 3.72035032052260...</td>\n      <td>7.009540</td>\n      <td>8.698870</td>\n      <td>{'loss': [3.5532967040413306, 3.48865530992809...</td>\n      <td>10</td>\n      <td>7</td>\n      <td>False</td>\n      <td>1936.754572</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/15T...</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>15</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>96.576995</td>\n      <td>0.137701</td>\n      <td>{'loss': [4.296231306516207, 3.586264573610746...</td>\n      <td>6.885110</td>\n      <td>9.257041</td>\n      <td>{'loss': [3.596015873708223, 3.476873178231089...</td>\n      <td>10</td>\n      <td>8</td>\n      <td>False</td>\n      <td>1963.545982</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/15T...</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>15</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>97.842856</td>\n      <td>0.118109</td>\n      <td>{'loss': [4.12328287271353, 3.6071051634274998...</td>\n      <td>5.723766</td>\n      <td>8.788563</td>\n      <td>{'loss': [3.575199949113946, 3.524588001401801...</td>\n      <td>10</td>\n      <td>9</td>\n      <td>False</td>\n      <td>1930.976732</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/15T...</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>15</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>99.275489</td>\n      <td>0.030685</td>\n      <td>{'loss': [4.1716841000777025, 3.59731839253352...</td>\n      <td>5.350477</td>\n      <td>10.164243</td>\n      <td>{'loss': [3.5736288396935714, 3.61792811594511...</td>\n      <td>10</td>\n      <td>10</td>\n      <td>False</td>\n      <td>1963.606548</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/15T...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InfoSave"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T00:08:31.431309500Z",
     "start_time": "2023-11-13T00:08:31.363751800Z"
    }
   },
   "id": "fc156538971f59c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot Learning Curves\n",
    "plot_learning_curve(train_history, val_history)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ba09af163040460"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Put Model In Eval Mode\n",
    "net.eval()\n",
    "\n",
    "# Create Lists To Store The Predictions And True Labels\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Iterate Through The Test Data\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "\n",
    "        # Load Data\n",
    "        images, labels = data['image'].cuda(), data['cars'].cuda()\n",
    "        outputs = net(images)\n",
    "\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Get Class With Highest Probability As Predicted Class\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Convert Predictions To List And Append\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Convert Predictions List And true_labels To NumPy Arrays\n",
    "predictions = np.array(predictions)\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "# Pull Class Names\n",
    "class_names = (['AM'] + (pd.read_csv('../SynchronizationProject/stanford_cars_eec174/names_make.txt')['AM'].to_list()))\n",
    "\n",
    "# Plot Everything\n",
    "plot_confusion_matrix(true_labels, predictions, class_names)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcae55caff2b1a87"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

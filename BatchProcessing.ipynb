{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f90d5f7a24a79b5b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set GPU Settings\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "#TURN OFF WHEN ACTUALLY TESTING CODE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T21:13:15.600915400Z",
     "start_time": "2023-11-05T21:13:13.408556Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6be1d18d03f78ee"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Define CarDataSet Class\n",
    "class CarDataSet():\n",
    "\n",
    "    # Define The Initialization\n",
    "    def __init__(self, csv_file, root_dir, transform=None, target_transform=None):\n",
    "        self.cars = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.resize = transforms.Resize((150,150))  # Resize images to a uniform size\n",
    "\n",
    "    # Define The Length Function\n",
    "    def __len__(self):\n",
    "        return len(self.cars)\n",
    "\n",
    "    # Define The Get Item Function\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Pull The Image And Check Settings\n",
    "        img_name = os.path.join(self.root_dir, self.cars.iloc[idx, 0])\n",
    "\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        if image.mode != 'RGB':\n",
    "          image = image.convert('RGB')\n",
    "\n",
    "        # Pull The Label, -1 To Normalize To 0\n",
    "        label = (self.cars.iloc[idx, 5]) - 1\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Define The Dictionary\n",
    "        sample = {'image': image, 'cars': label}\n",
    "\n",
    "        # Return\n",
    "        return sample"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T21:13:15.621432900Z",
     "start_time": "2023-11-05T21:13:15.604418300Z"
    }
   },
   "id": "759e0bf9fe9a388b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Define Transform\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "    \n",
    "# Load The Data\n",
    "train_data = CarDataSet(csv_file='../SynchronizationProject/stanford_cars_eec174/train_make.csv',\n",
    "                                        root_dir='../SynchronizationProject/stanford_cars_eec174/images/train', transform=transform)\n",
    "test_data = CarDataSet(csv_file='../SynchronizationProject/stanford_cars_eec174/val_make.csv',\n",
    "                                        root_dir='../SynchronizationProject/stanford_cars_eec174/images/val', transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T21:13:15.670474500Z",
     "start_time": "2023-11-05T21:13:15.617930400Z"
    }
   },
   "id": "c9f8990b2487605"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def show_imgs(dataloader):\n",
    "    # Load Names From The File\n",
    "    with open('../SynchronizationProject/stanford_cars_eec174/names_make.txt', 'r') as file:\n",
    "        names = file.read().splitlines()\n",
    "\n",
    "    # Retrieve The Images And Labels\n",
    "    dataiter = iter(dataloader)\n",
    "    samples = next(dataiter)\n",
    "\n",
    "    images_show_img, labels_show_img = samples['image'], samples['cars']\n",
    "\n",
    "    # Grab Ten Random Indices\n",
    "    shuffled_indices = np.random.permutation(len(images_show_img))\n",
    "    indices = shuffled_indices[:10]\n",
    "\n",
    "    # Create Subplots\n",
    "    figure, axes = plt.subplots(1, 10, figsize=(20, 10))\n",
    "\n",
    "    for i, ax in zip(indices, axes):\n",
    "\n",
    "        # Rearrange Dimensions For Display\n",
    "        cur_image = images_show_img[i].permute(1, 2, 0)\n",
    "\n",
    "        # Convert The Label To An Integer\n",
    "        label_index = int(labels_show_img[i].item())\n",
    "\n",
    "        # Assign The Name Corresponding To The Label Index\n",
    "        cur_label = names[label_index]\n",
    "\n",
    "        # Display The Image\n",
    "        ax.imshow(cur_image)\n",
    "\n",
    "        # Display The Label As Title\n",
    "        ax.set_title(cur_label)\n",
    "\n",
    "        # Turn Off The Axis\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Test Accuracy\n",
    "def test_accuracy(model, test_loader_internal, passed_device, loss_fn):\n",
    "\n",
    "    # Set Parameters\n",
    "    model.to(passed_device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    run = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    # Run Tests\n",
    "    with torch.no_grad():\n",
    "        for test_data_internal in test_loader_internal:\n",
    "            images_test_acc, labels_test_acc = test_data_internal['image'].cuda(), test_data_internal['cars'].cuda()\n",
    "            outputs_test_acc = model(images_test_acc)\n",
    "            _, predicted_test_acc = torch.max(outputs_test_acc.data, 1)\n",
    "            val_loss += (loss_fn(outputs_test_acc, labels_test_acc)).item()\n",
    "            total += labels_test_acc.size(0)\n",
    "            correct += (predicted_test_acc == labels_test_acc).sum().item()\n",
    "            run += 1\n",
    "\n",
    "    # Return\n",
    "    return (100 * correct / total), val_loss/run\n",
    "\n",
    "\n",
    "# Plotting Function\n",
    "def general_plot(data_dict1: dict, data_dict2: dict, param: str, label1: str, label2: str, ylabel: str, reduce_noise: bool):\n",
    "    param_list1 = data_dict1[param]\n",
    "    param_list2 = data_dict2[param]\n",
    "\n",
    "    if reduce_noise:\n",
    "        param_list1.pop(0)\n",
    "        param_list2.pop(0)\n",
    "\n",
    "    # Prepare Plot\n",
    "    xs = [x for x in range(min(len(param_list1), len(param_list2)))]\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.plot(xs, param_list1[:len(xs)], label=label1)\n",
    "    plt.plot(xs, param_list2[:len(xs)], label=label2)\n",
    "    plt.xlabel('Iterations (in batches)')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(f'Training and Validation {ylabel} Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# Plot Learning Curves\n",
    "def plot_learning_curve(train_history_plt_curve: dict, val_history_plt_curve: dict, reduce_noise=True):\n",
    "\n",
    "    # Plot The Info\n",
    "    general_plot(train_history_plt_curve, val_history_plt_curve, 'loss', 'Training Loss', 'Validation Loss', 'Loss', reduce_noise)\n",
    "    general_plot(train_history_plt_curve, val_history_plt_curve, 'accuracy', 'Training Accuracy', 'Validation Accuracy', 'Accuracy', reduce_noise)\n",
    "    \n",
    "# Define and use confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names_confusion):\n",
    "\n",
    "    # Prepare Plot And Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix Frozen')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names_confusion))\n",
    "    plt.xticks(tick_marks, class_names_confusion, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names_confusion)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    # Show Everything\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define Train\n",
    "def train(model, loss_fn, optimizer_train, train_loader_train, val_loader, num_epochs_train, device_train):\n",
    "\n",
    "    # Set Parameters\n",
    "    model.train()\n",
    "    BLUE = '\\033[94m'\n",
    "    RESET = '\\033[0m'\n",
    "\n",
    "    train_history_train = {'loss': [], 'accuracy': []}\n",
    "    val_history_train = {'loss': [], 'accuracy': []}\n",
    "\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    # Iterate Through All Epochs\n",
    "    for epoch in range(num_epochs_train):\n",
    "\n",
    "        # For Loop Parameters\n",
    "        start_time = time.time()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        # Iterate Through The Training Dataset\n",
    "        for i, data_train in enumerate(train_loader_train, 0):\n",
    "            \n",
    "            # Flatten Images And Load Data\n",
    "            images_train, labels_train = data_train['image'].cuda(), data_train['cars'].cuda()\n",
    "\n",
    "            # Zero Collected Gradients At Each Step (basically cleaning)\n",
    "            optimizer_train.zero_grad()\n",
    "\n",
    "            # Forward Propagate\n",
    "            outputs_train = model(images_train)\n",
    "\n",
    "            # Calculate Loss\n",
    "            loss = loss_function(outputs_train, labels_train)\n",
    "\n",
    "            # Back Propagate\n",
    "            loss.backward()\n",
    "\n",
    "            # Update Weigh Gradsts\n",
    "            optimizer_train.step()\n",
    "\n",
    "            # Calculate Accuracy\n",
    "            _, predicted_train = torch.max(outputs_train.data, 1)\n",
    "            total += labels_train.size(0)\n",
    "            correct += (predicted_train == labels_train).sum().item()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_correct += 100 * correct / total\n",
    "            train_total += 1\n",
    "            \n",
    "        # Evaluate Model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_acc, val_loss = test_accuracy(model, val_loader, device_train, loss_fn)\n",
    "\n",
    "            val_history_train['loss'].append(val_loss)\n",
    "            val_history_train['accuracy'].append(val_acc)\n",
    "\n",
    "            train_history_train['loss'].append(train_loss / train_total)\n",
    "            train_history_train['accuracy'].append(train_correct / train_total)\n",
    "\n",
    "            train_loss = 0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        # At End Of Each Epoch Print Duration And Accuracy\n",
    "        print(f'{BLUE}Epoch [{epoch+1}/{num_epochs_train}]:'f' Duration: {round(time.time() - start_time, 2)}s |'f' Train Acc: {round(train_history_train[\"accuracy\"][-1], 2)} |'f' Train Loss: {round(train_history_train[\"loss\"][-1], 5)}'f' Val Acc: {round(val_history_train[\"accuracy\"][-1], 2)} |'f' Val Loss: {round(val_history_train[\"loss\"][-1], 5)}{RESET}')\n",
    "        print('------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "        # Save Checkpoint\n",
    "        #PATH = '/content/drive/My Drive/WEIGHTSAVES/run' + str('78.8save') + 'epoch' + str(epoch+1) + 'ta' + str(train_history_train[\"accuracy\"][-1])[0:6] + 'tl' + str(train_history_train[\"loss\"][-1])[0:6] + 'va' + str(val_history_train[\"accuracy\"][-1])[0:6] + 'vl' + str(val_history_train[\"loss\"][-1])[0:6] + '.pth'\n",
    "        #print(PATH)\n",
    "        #torch.save(net.state_dict(), PATH)\n",
    "\n",
    "    # Print Final Statistics\n",
    "    #print(f'{BLUE}Total Duration:{round(time.time() - total_start_time, 2)}s |',f'Final Train Acc: {round(train_history_train[\"accuracy\"][-1], 2)} |',f'Final Train Loss: {round(train_history_train[\"loss\"][-1], 5)} |',f'Final Val Acc: {round(val_history_train[\"accuracy\"][-1], 2)} |',f'Final Val Loss: {round(val_history_train[\"loss\"][-1], 5)}{RESET}')\n",
    "\n",
    "    # Return\n",
    "    return train_history_train, val_history_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T21:13:16.342302100Z",
     "start_time": "2023-11-05T21:13:16.296763800Z"
    }
   },
   "id": "c1fa020743f3bc96"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unchanging Definitions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2fc42535bf12767"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#[ batchsize(32) * num, learningrate(0.0001) * num, GPUcount num, pretrained (True=yes, False=no),  \n",
    "\n",
    "operations_dictionary = {\n",
    "    \"0\": [1, 1, 1, False],\n",
    "    \"1\": [1, 1, 3, False],\n",
    "    \"2\": [2, 1, 1, False],\n",
    "    \"3\": [2, 1, 5, False]\n",
    "                         }\n",
    "# Dataset Size\n",
    "DATASET_SIZE = 8192\n",
    "\n",
    "# Define Parameters\n",
    "input_size = (768 * 1024)\n",
    "num_classes = 49\n",
    "\n",
    "InfoSave = pd.DataFrame(columns = [\"Run\", \"BatchSize\", \"LearningRate\", \"TrainAccuracy\", \"TrainLoss\", \"TrainHistory\", \"ValAccuracy\", \"ValLoss\", \"ValHistory\", \"GPUCount\", \"GPUNumber\", \"PreTrain\", \"SavePathInput\", \"SavePathOutput\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T21:11:06.040187400Z",
     "start_time": "2023-11-05T21:11:06.015665900Z"
    }
   },
   "id": "35a43b541b55111f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model Setup and Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28c4faa3661b0397"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 37\u001B[0m\n\u001B[0;32m     34\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(net\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlr)\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# Run Train\u001B[39;00m\n\u001B[1;32m---> 37\u001B[0m train_history, val_history \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;66;03m# Save Model\u001B[39;00m\n\u001B[0;32m     40\u001B[0m PATH \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../SynchronizationProject/SaveData/Weights/\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(run_number_key) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "Cell \u001B[1;32mIn[4], line 147\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, loss_fn, optimizer_train, train_loader_train, val_loader, num_epochs_train, device_train)\u001B[0m\n\u001B[0;32m    144\u001B[0m train_total \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    146\u001B[0m \u001B[38;5;66;03m# Iterate Through The Training Dataset\u001B[39;00m\n\u001B[1;32m--> 147\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, data_train \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader_train, \u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m    148\u001B[0m     \n\u001B[0;32m    149\u001B[0m     \u001B[38;5;66;03m# Flatten Images And Load Data\u001B[39;00m\n\u001B[0;32m    150\u001B[0m     images_train, labels_train \u001B[38;5;241m=\u001B[39m data_train[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mcuda(), data_train[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcars\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[0;32m    152\u001B[0m     \u001B[38;5;66;03m# Zero Collected Gradients At Each Step (basically cleaning)\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\SynchronizationProject\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    519\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    520\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[1;32m--> 521\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    524\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    525\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\PycharmProjects\\SynchronizationProject\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    559\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    560\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 561\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    562\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    563\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data)\n",
      "File \u001B[1;32m~\\PycharmProjects\\SynchronizationProject\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\PycharmProjects\\SynchronizationProject\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[2], line 31\u001B[0m, in \u001B[0;36mCarDataSet.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     28\u001B[0m label \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcars\u001B[38;5;241m.\u001B[39miloc[idx, \u001B[38;5;241m5\u001B[39m]) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform:\n\u001B[1;32m---> 31\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# Define The Dictionary\u001B[39;00m\n\u001B[0;32m     34\u001B[0m sample \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m: image, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcars\u001B[39m\u001B[38;5;124m'\u001B[39m: label}\n",
      "File \u001B[1;32m~\\PycharmProjects\\SynchronizationProject\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:61\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 61\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32m~\\PycharmProjects\\SynchronizationProject\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\SynchronizationProject\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:304\u001B[0m, in \u001B[0;36mResize.forward\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m    296\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m    297\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    298\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    299\u001B[0m \u001B[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    302\u001B[0m \u001B[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001B[39;00m\n\u001B[0;32m    303\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 304\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minterpolation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mantialias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\SynchronizationProject\\venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:419\u001B[0m, in \u001B[0;36mresize\u001B[1;34m(img, size, interpolation, max_size, antialias)\u001B[0m\n\u001B[0;32m    415\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    416\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    417\u001B[0m         )\n\u001B[0;32m    418\u001B[0m     pil_interpolation \u001B[38;5;241m=\u001B[39m pil_modes_mapping[interpolation]\n\u001B[1;32m--> 419\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF_pil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpil_interpolation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    421\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F_t\u001B[38;5;241m.\u001B[39mresize(img, size\u001B[38;5;241m=\u001B[39msize, interpolation\u001B[38;5;241m=\u001B[39minterpolation\u001B[38;5;241m.\u001B[39mvalue, max_size\u001B[38;5;241m=\u001B[39mmax_size, antialias\u001B[38;5;241m=\u001B[39mantialias)\n",
      "File \u001B[1;32m~\\PycharmProjects\\SynchronizationProject\\venv\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py:265\u001B[0m, in \u001B[0;36mresize\u001B[1;34m(img, size, interpolation, max_size)\u001B[0m\n\u001B[0;32m    260\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m max_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    262\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_size should only be passed if size specifies the length of the smaller edge, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    263\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    264\u001B[0m     )\n\u001B[1;32m--> 265\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\SynchronizationProject\\venv\\lib\\site-packages\\PIL\\Image.py:2163\u001B[0m, in \u001B[0;36mImage.resize\u001B[1;34m(self, size, resample, box, reducing_gap)\u001B[0m\n\u001B[0;32m   2159\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[0;32m   2161\u001B[0m size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(size)\n\u001B[1;32m-> 2163\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2164\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m box \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2165\u001B[0m     box \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msize\n",
      "File \u001B[1;32m~\\PycharmProjects\\SynchronizationProject\\venv\\lib\\site-packages\\PIL\\ImageFile.py:269\u001B[0m, in \u001B[0;36mImageFile.load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    266\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(msg)\n\u001B[0;32m    268\u001B[0m b \u001B[38;5;241m=\u001B[39m b \u001B[38;5;241m+\u001B[39m s\n\u001B[1;32m--> 269\u001B[0m n, err_code \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    271\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Run Each Set of Models \n",
    "for run_number_key in operations_dictionary:\n",
    "    print(run_number_key)\n",
    "    # Batch Size\n",
    "    batch_size = 32 * (operations_dictionary[run_number_key])[0]\n",
    "    \n",
    "    # Import To Dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "    \n",
    "    # Specify Factors\n",
    "    lr = 0.0001 * (operations_dictionary[run_number_key])[1]\n",
    "    num_epochs = 10\n",
    "        \n",
    "    # Single GPU\n",
    "    if (operations_dictionary[run_number_key])[2] == 1:\n",
    "        # Model Definition and Final Layer Edit\n",
    "        net = models.resnet50()\n",
    "        net.fc = nn.Linear(net.fc.in_features, 49)\n",
    "        \n",
    "        saved_model_path = \"\"\n",
    "        \n",
    "        if (operations_dictionary[run_number_key])[3]:\n",
    "            # IF NEEDED TO IMPORT\n",
    "            saved_model_path = '../SynchronizationProject/SaveData/Weights/' + str((operations_dictionary[run_number_key])[4]) + '.pth'\n",
    "            net.load_state_dict(torch.load(saved_model_path))\n",
    "        \n",
    "        # Load Model Onto GPU\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        net.to(device)\n",
    "        \n",
    "        # Loss Function and Optimizer\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "        \n",
    "        # Run Train\n",
    "        train_history, val_history = train(net, loss_function, optimizer, train_loader, test_loader, num_epochs, device)\n",
    "        \n",
    "        # Save Model\n",
    "        PATH = '../SynchronizationProject/SaveData/Weights/' + str(run_number_key) + '.pth'\n",
    "        torch.save(net.state_dict(), PATH)\n",
    "        \n",
    "        new_row_data = {\n",
    "                \"Run\": run_number_key,\n",
    "                \"BatchSize\": batch_size,\n",
    "                \"LearningRate\": lr,\n",
    "                \"TrainAccuracy\": train_history[\"accuracy\"][-1],\n",
    "                \"TrainLoss\": train_history[\"loss\"][-1],\n",
    "                \"TrainHistory\": train_history,\n",
    "                \"ValAccuracy\": val_history[\"accuracy\"][-1],\n",
    "                \"ValLoss\": val_history[\"loss\"][-1],\n",
    "                \"ValHistory\": val_history,\n",
    "                \"GPUCount\": (operations_dictionary[run_number_key])[2],\n",
    "                \"GPUNumber\": 1,\n",
    "                \"PreTrain\": (operations_dictionary[run_number_key])[3],\n",
    "                \"SavePathInput\": saved_model_path,\n",
    "                \"SavePathOutput\": PATH\n",
    "        }\n",
    "            \n",
    "        # Adding the new row to the DataFrame\n",
    "        InfoSave.loc[len(InfoSave)] = new_row_data\n",
    "    \n",
    "    # Multiple GPU\n",
    "    else:\n",
    "        amount_of_GPUs_to_run = (operations_dictionary[run_number_key])[2]\n",
    "        \n",
    "        for run in range(amount_of_GPUs_to_run):\n",
    "            # Define the start and end indices for the current run\n",
    "            start_idx = (len(train_loader.dataset) // amount_of_GPUs_to_run) * run\n",
    "            \n",
    "            if run < (amount_of_GPUs_to_run - 1):\n",
    "                end_idx = (len(train_loader.dataset) // amount_of_GPUs_to_run) * (run + 1)\n",
    "            else:\n",
    "                end_idx = len(train_loader.dataset)\n",
    "            \n",
    "            split_data_set = torch.utils.data.Subset(train_loader.dataset, range(start_idx, end_idx))\n",
    "            split_train_loader = torch.utils.data.DataLoader(dataset=split_data_set, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            # Model Definition and Final Layer Edit\n",
    "            net = models.resnet50()\n",
    "            net.fc = nn.Linear(net.fc.in_features, 49)\n",
    "            \n",
    "            saved_model_path = \"\"\n",
    "            \n",
    "            if (operations_dictionary[run_number_key])[3]:\n",
    "                # IF NEEDED TO IMPORT\n",
    "                saved_model_path = '../SynchronizationProject/SaveData/Weights/' + str((operations_dictionary[run_number_key])[4]) + '.pth'\n",
    "                net.load_state_dict(torch.load(saved_model_path))\n",
    "            \n",
    "            # Load Model Onto GPU\n",
    "            device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "            net.to(device)\n",
    "            \n",
    "            # Loss Function and Optimizer\n",
    "            loss_function = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "            \n",
    "            #FIXTHIS TO WORK WITH MULTIPLE RUNS\n",
    "            train_history, val_history = train(net, loss_function, optimizer, split_train_loader, test_loader, num_epochs, device)\n",
    "            \n",
    "            # Save Individual Weights\n",
    "            PATH = '../SynchronizationProject/SaveData/Weights/' + str(run_number_key) + \"T\" + str(run) + '.pth'\n",
    "            torch.save(net.state_dict(), PATH)\n",
    "            \n",
    "            new_row_data = {\n",
    "                \"Run\": run_number_key,\n",
    "                \"BatchSize\": batch_size,\n",
    "                \"LearningRate\": lr,\n",
    "                \"TrainAccuracy\": train_history[\"accuracy\"][-1],\n",
    "                \"TrainLoss\": train_history[\"loss\"][-1],\n",
    "                \"TrainHistory\": train_history,\n",
    "                \"ValAccuracy\": val_history[\"accuracy\"][-1],\n",
    "                \"ValLoss\": val_history[\"loss\"][-1],\n",
    "                \"ValHistory\": val_history,\n",
    "                \"GPUCount\": (operations_dictionary[run_number_key])[2],\n",
    "                \"GPUNumber\": run + 1,\n",
    "                \"PreTrain\": (operations_dictionary[run_number_key])[3],\n",
    "                \"SavePathInput\": saved_model_path,\n",
    "                \"SavePathOutput\": PATH\n",
    "            }\n",
    "            \n",
    "            # Adding the new row to the DataFrame\n",
    "            InfoSave.loc[len(InfoSave)] = new_row_data\n",
    "\n",
    "file_path = '../SynchronizationProject/SaveData/DataFrames/InfoSave.csv'\n",
    "\n",
    "InfoSave.to_csv(file_path, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T21:11:56.394343200Z",
     "start_time": "2023-11-05T21:11:06.038686800Z"
    }
   },
   "id": "bb955564ff30a129"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluate The Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "684e622953828794"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "file_path = '../SynchronizationProject/SaveData/DataFrames/InfoSave.csv'\n",
    "\n",
    "InfoSave = pd.read_csv(file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T21:13:23.735148800Z",
     "start_time": "2023-11-05T21:13:23.713515Z"
    }
   },
   "id": "558a8f968704739e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "   Run  BatchSize  LearningRate  TrainAccuracy  TrainLoss  \\\n0    0         32        0.0001      29.766119   2.530721   \n1    1         32        0.0001      22.628991   2.912521   \n2    1         32        0.0001      19.352325   3.014417   \n3    1         32        0.0001      14.903829   3.129956   \n4    2         64        0.0001      25.569260   2.713737   \n5    3         64        0.0001      23.005913   3.016223   \n6    3         64        0.0001      25.058370   2.930626   \n7    3         64        0.0001      20.478625   2.953434   \n8    3         64        0.0001      25.610257   2.911658   \n9    3         64        0.0001      22.036984   3.032949   \n\n                                        TrainHistory  ValAccuracy   ValLoss  \\\n0  {'loss': [3.4930564347435444, 3.41162627351050...    16.673579  3.380552   \n1  {'loss': [3.5299864712883444, 3.43312999500947...     9.373704  3.575293   \n2  {'loss': [3.5531147339764764, 3.45249549641328...    12.982165  3.580856   \n3  {'loss': [3.557936040092917, 3.476790214987362...    12.567399  3.436109   \n4  {'loss': [3.4837466087192297, 3.39986238069832...    15.429282  3.255348   \n5  {'loss': [3.600028001345121, 3.422101405950693...     8.336790  3.701210   \n6  {'loss': [3.5564707059126635, 3.39519774913787...    10.991290  3.682562   \n7  {'loss': [3.598517115299518, 3.457311208431537...     7.341352  3.960901   \n8  {'loss': [3.601519465446472, 3.431241613167983...     8.212360  4.160569   \n9  {'loss': [3.5894748889482937, 3.40311352106241...     9.290751  3.795039   \n\n                                          ValHistory  GPUCount  GPUNumber  \\\n0  {'loss': [3.5342165074850382, 3.59506788692976...         1          1   \n1  {'loss': [3.5124278695959794, 3.49547933904748...         3          1   \n2  {'loss': [3.520307826368432, 3.501597238214392...         3          2   \n3  {'loss': [3.5215154917616593, 3.47128234411540...         3          3   \n4  {'loss': [3.4475874649850944, 3.41609198168704...         1          1   \n5  {'loss': [3.5449120433706987, 3.51157576159427...         5          1   \n6  {'loss': [3.5440946942881535, 3.49703889144094...         5          2   \n7  {'loss': [3.4908311680743567, 3.49983194627259...         5          3   \n8  {'loss': [3.5688967642031217, 3.50579348363374...         5          4   \n9  {'loss': [3.5851886084205224, 3.53122147760893...         5          5   \n\n   PreTrain  SavePathInput                                     SavePathOutput  \n0     False            NaN   ../SynchronizationProject/SaveData/Weights/0.pth  \n1     False            NaN  ../SynchronizationProject/SaveData/Weights/1T0...  \n2     False            NaN  ../SynchronizationProject/SaveData/Weights/1T1...  \n3     False            NaN  ../SynchronizationProject/SaveData/Weights/1T2...  \n4     False            NaN   ../SynchronizationProject/SaveData/Weights/2.pth  \n5     False            NaN  ../SynchronizationProject/SaveData/Weights/3T0...  \n6     False            NaN  ../SynchronizationProject/SaveData/Weights/3T1...  \n7     False            NaN  ../SynchronizationProject/SaveData/Weights/3T2...  \n8     False            NaN  ../SynchronizationProject/SaveData/Weights/3T3...  \n9     False            NaN  ../SynchronizationProject/SaveData/Weights/3T4...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Run</th>\n      <th>BatchSize</th>\n      <th>LearningRate</th>\n      <th>TrainAccuracy</th>\n      <th>TrainLoss</th>\n      <th>TrainHistory</th>\n      <th>ValAccuracy</th>\n      <th>ValLoss</th>\n      <th>ValHistory</th>\n      <th>GPUCount</th>\n      <th>GPUNumber</th>\n      <th>PreTrain</th>\n      <th>SavePathInput</th>\n      <th>SavePathOutput</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>29.766119</td>\n      <td>2.530721</td>\n      <td>{'loss': [3.4930564347435444, 3.41162627351050...</td>\n      <td>16.673579</td>\n      <td>3.380552</td>\n      <td>{'loss': [3.5342165074850382, 3.59506788692976...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/0.pth</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>22.628991</td>\n      <td>2.912521</td>\n      <td>{'loss': [3.5299864712883444, 3.43312999500947...</td>\n      <td>9.373704</td>\n      <td>3.575293</td>\n      <td>{'loss': [3.5124278695959794, 3.49547933904748...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/1T0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>19.352325</td>\n      <td>3.014417</td>\n      <td>{'loss': [3.5531147339764764, 3.45249549641328...</td>\n      <td>12.982165</td>\n      <td>3.580856</td>\n      <td>{'loss': [3.520307826368432, 3.501597238214392...</td>\n      <td>3</td>\n      <td>2</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/1T1...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>14.903829</td>\n      <td>3.129956</td>\n      <td>{'loss': [3.557936040092917, 3.476790214987362...</td>\n      <td>12.567399</td>\n      <td>3.436109</td>\n      <td>{'loss': [3.5215154917616593, 3.47128234411540...</td>\n      <td>3</td>\n      <td>3</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/1T2...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>25.569260</td>\n      <td>2.713737</td>\n      <td>{'loss': [3.4837466087192297, 3.39986238069832...</td>\n      <td>15.429282</td>\n      <td>3.255348</td>\n      <td>{'loss': [3.4475874649850944, 3.41609198168704...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/2.pth</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>23.005913</td>\n      <td>3.016223</td>\n      <td>{'loss': [3.600028001345121, 3.422101405950693...</td>\n      <td>8.336790</td>\n      <td>3.701210</td>\n      <td>{'loss': [3.5449120433706987, 3.51157576159427...</td>\n      <td>5</td>\n      <td>1</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/3T0...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>25.058370</td>\n      <td>2.930626</td>\n      <td>{'loss': [3.5564707059126635, 3.39519774913787...</td>\n      <td>10.991290</td>\n      <td>3.682562</td>\n      <td>{'loss': [3.5440946942881535, 3.49703889144094...</td>\n      <td>5</td>\n      <td>2</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/3T1...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>20.478625</td>\n      <td>2.953434</td>\n      <td>{'loss': [3.598517115299518, 3.457311208431537...</td>\n      <td>7.341352</td>\n      <td>3.960901</td>\n      <td>{'loss': [3.4908311680743567, 3.49983194627259...</td>\n      <td>5</td>\n      <td>3</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/3T2...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>25.610257</td>\n      <td>2.911658</td>\n      <td>{'loss': [3.601519465446472, 3.431241613167983...</td>\n      <td>8.212360</td>\n      <td>4.160569</td>\n      <td>{'loss': [3.5688967642031217, 3.50579348363374...</td>\n      <td>5</td>\n      <td>4</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/3T3...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3</td>\n      <td>64</td>\n      <td>0.0001</td>\n      <td>22.036984</td>\n      <td>3.032949</td>\n      <td>{'loss': [3.5894748889482937, 3.40311352106241...</td>\n      <td>9.290751</td>\n      <td>3.795039</td>\n      <td>{'loss': [3.5851886084205224, 3.53122147760893...</td>\n      <td>5</td>\n      <td>5</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/3T4...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InfoSave"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T21:13:31.941690600Z",
     "start_time": "2023-11-05T21:13:31.833479400Z"
    }
   },
   "id": "fc156538971f59c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot Learning Curves\n",
    "plot_learning_curve(train_history, val_history)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ba09af163040460"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Put Model In Eval Mode\n",
    "net.eval()\n",
    "\n",
    "# Create Lists To Store The Predictions And True Labels\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Iterate Through The Test Data\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "\n",
    "        # Load Data\n",
    "        images, labels = data['image'].cuda(), data['cars'].cuda()\n",
    "        outputs = net(images)\n",
    "\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Get Class With Highest Probability As Predicted Class\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Convert Predictions To List And Append\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Convert Predictions List And true_labels To NumPy Arrays\n",
    "predictions = np.array(predictions)\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "# Pull Class Names\n",
    "class_names = (['AM'] + (pd.read_csv('../SynchronizationProject/stanford_cars_eec174/names_make.txt')['AM'].to_list()))\n",
    "\n",
    "# Plot Everything\n",
    "plot_confusion_matrix(true_labels, predictions, class_names)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcae55caff2b1a87"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

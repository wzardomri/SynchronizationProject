{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49fef9c58c59e904"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T06:08:30.316507100Z",
     "start_time": "2023-11-14T06:08:09.553679200Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.cpu().numpy()\n",
    " \n",
    "def maximum_weight_aggregation(models_states):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [to_numpy(model_state[param_name]) for model_state in models_states]\n",
    "        aggregated_weights[param_name] = torch.tensor(np.max(all_weights, axis=0))\n",
    "    return aggregated_weights, \"MaximumWeight\"\n",
    "\n",
    "def minimum_weight_aggregation(models_states):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [to_numpy(model_state[param_name]) for model_state in models_states]\n",
    "        aggregated_weights[param_name] = torch.tensor(np.min(all_weights, axis=0))\n",
    "    return aggregated_weights, \"MinimumWeight\"\n",
    "\n",
    "def mean_aggregation(models_states):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [to_numpy(model_state[param_name]) for model_state in models_states]\n",
    "        aggregated_weights[param_name] = torch.tensor(np.mean(all_weights, axis=0))\n",
    "    return aggregated_weights, \"MeanWeight\"\n",
    "\n",
    "def median_aggregation(models_states):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [to_numpy(model_state[param_name]) for model_state in models_states]\n",
    "        aggregated_weights[param_name] = torch.tensor(np.median(all_weights, axis=0))\n",
    "    return aggregated_weights, \"MedianWeight\"\n",
    "\n",
    "def trimmed_mean_aggregation(models_states, trim_fraction=0.2):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [to_numpy(model_state[param_name]) for model_state in models_states]\n",
    "        sorted_weights = np.sort(all_weights, axis=0)\n",
    "        trim_size = int(trim_fraction * len(sorted_weights))\n",
    "        trimmed_weights = sorted_weights[trim_size:-trim_size]\n",
    "        aggregated_weights[param_name] = torch.tensor(np.mean(trimmed_weights, axis=0))\n",
    "    return aggregated_weights, \"TrimmedMean\"\n",
    "\n",
    "def geometric_mean_aggregation(models_states):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [to_numpy(model_state[param_name]) for model_state in models_states]\n",
    "        num_negatives = np.sum(np.array(all_weights) < 0, axis=0)\n",
    "        if np.sum(num_negatives) == 0:\n",
    "            aggregated_weights[param_name] = torch.tensor(np.exp(np.mean(np.log(all_weights), axis=0)))\n",
    "        else:\n",
    "            product = np.prod(all_weights, axis=0)\n",
    "            aggregated_weights[param_name] = torch.tensor(np.sign(product) * np.abs(product)**(1/num_negatives))\n",
    "        \n",
    "    return aggregated_weights, \"GeometricMean\"\n",
    "\n",
    "def harmonic_mean_aggregation(models_states):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [to_numpy(model_state[param_name]) for model_state in models_states]\n",
    "        all_weights_np = np.array(all_weights)  # Convert the list to a NumPy array\n",
    "        aggregated_weights[param_name] = torch.tensor(len(all_weights_np) / np.sum(1.0 / all_weights_np, axis=0))\n",
    "    return aggregated_weights, \"HarmonicMean\"\n",
    "\n",
    "def weighted_average_aggregation(models_states, model_weights):\n",
    "    aggregated_weights = {}\n",
    "    for param_name in models_states[0]:\n",
    "        all_weights = [to_numpy(model_state[param_name]) for model_state in models_states]\n",
    "        weights_array = np.array(model_weights)\n",
    "        normalized_weights = weights_array / np.sum(weights_array)\n",
    "        aggregated_weights[param_name] = torch.tensor(np.sum(normalized_weights * all_weights, axis=0))\n",
    "    return aggregated_weights, \"WeightedAverage\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T06:08:30.332020700Z",
     "start_time": "2023-11-14T06:08:30.326015200Z"
    }
   },
   "id": "1b1b7bab0cfc1d9"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Define CarDataSet Class\n",
    "class CarDataSet():\n",
    "\n",
    "    # Define The Initialization\n",
    "    def __init__(self, csv_file, root_dir, transform=None, target_transform=None):\n",
    "        self.cars = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.resize = transforms.Resize((150,150))  # Resize images to a uniform size\n",
    "\n",
    "    # Define The Length Function\n",
    "    def __len__(self):\n",
    "        return len(self.cars)\n",
    "\n",
    "    # Define The Get Item Function\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Pull The Image And Check Settings\n",
    "        img_name = os.path.join(self.root_dir, self.cars.iloc[idx, 0])\n",
    "\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        if image.mode != 'RGB':\n",
    "          image = image.convert('RGB')\n",
    "\n",
    "        # Pull The Label, -1 To Normalize To 0\n",
    "        label = (self.cars.iloc[idx, 5]) - 1\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Define The Dictionary\n",
    "        sample = {'image': image, 'cars': label}\n",
    "\n",
    "        # Return\n",
    "        return sample\n",
    "    \n",
    "# Test Accuracy\n",
    "def test_accuracy(model, test_loader_internal, passed_device, loss_fn):\n",
    "\n",
    "    # Set Parameters\n",
    "    model.to(passed_device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    run = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    # Run Tests\n",
    "    with torch.no_grad():\n",
    "        for test_data_internal in test_loader_internal:\n",
    "            images_test_acc, labels_test_acc = test_data_internal['image'].cuda(), test_data_internal['cars'].cuda()\n",
    "            outputs_test_acc = model(images_test_acc)\n",
    "            _, predicted_test_acc = torch.max(outputs_test_acc.data, 1)\n",
    "            val_loss += (loss_fn(outputs_test_acc, labels_test_acc)).item()\n",
    "            total += labels_test_acc.size(0)\n",
    "            correct += (predicted_test_acc == labels_test_acc).sum().item()\n",
    "            run += 1\n",
    "\n",
    "    # Return\n",
    "    return (100 * correct / total), val_loss/run"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T06:08:30.362045800Z",
     "start_time": "2023-11-14T06:08:30.332521100Z"
    }
   },
   "id": "1402e23ca647c2fd"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Define Transform\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "    \n",
    "# Load The Data\n",
    "test_data = CarDataSet(csv_file='../SynchronizationProject/stanford_cars_eec174/val_make.csv',\n",
    "                                        root_dir='../SynchronizationProject/stanford_cars_eec174/images/val', transform=transform)\n",
    "\n",
    "train_data = CarDataSet(csv_file='../SynchronizationProject/stanford_cars_eec174/train_make.csv',\n",
    "                                        root_dir='../SynchronizationProject/stanford_cars_eec174/images/train', transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T06:08:30.379060900Z",
     "start_time": "2023-11-14T06:08:30.350035900Z"
    }
   },
   "id": "b13de7699fa1b606"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "InternalSave = pd.read_csv(\"../SynchronizationProject/SaveData/DataFrames/InfoSave.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T06:08:30.410087500Z",
     "start_time": "2023-11-14T06:08:30.381062700Z"
    }
   },
   "id": "aecd1efa679fa3f5"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "    Run  BatchSize  LearningRate  Epochs  TrainAccuracy  TrainLoss  \\\n0     0         32        0.0001      50      98.891674   0.046048   \n1     1         32        0.0001      50      99.954355   0.006749   \n2     1         32        0.0001      50      99.989637   0.011231   \n3     1         32        0.0001      50      99.990760   0.004735   \n4     2         32        0.0001      50      99.991356   0.025019   \n..  ...        ...           ...     ...            ...        ...   \n75   15         64        0.0010      50      94.594299   0.204781   \n76   15         64        0.0010      50      94.982639   0.172121   \n77   15         64        0.0010      50      96.576995   0.137701   \n78   15         64        0.0010      50      97.842856   0.118109   \n79   15         64        0.0010      50      99.275489   0.030685   \n\n                                         TrainHistory  ValAccuracy    ValLoss  \\\n0   {'loss': [3.4871723436841777, 3.42232744647007...    18.830361   5.840313   \n1   {'loss': [3.5152197248795454, 3.43448365435880...    12.691829   5.167353   \n2   {'loss': [3.5432352767271156, 3.46236025024862...    11.489009   4.928459   \n3   {'loss': [3.547464345483219, 3.464133195316090...    11.198673   5.037350   \n4   {'loss': [3.5846809398296267, 3.42887252985045...     8.917462   5.132797   \n..                                                ...          ...        ...   \n75  {'loss': [4.1057514777550335, 3.74745035171508...     3.815844  10.712571   \n76  {'loss': [4.1091077877924995, 3.72035032052260...     7.009540   8.698870   \n77  {'loss': [4.296231306516207, 3.586264573610746...     6.885110   9.257041   \n78  {'loss': [4.12328287271353, 3.6071051634274998...     5.723766   8.788563   \n79  {'loss': [4.1716841000777025, 3.59731839253352...     5.350477  10.164243   \n\n                                           ValHistory  GPUCount  GPUNumber  \\\n0   {'loss': [3.490916973666141, 3.408613242601093...         1          1   \n1   {'loss': [3.5044365682099996, 3.46959972381591...         3          1   \n2   {'loss': [3.492495696795614, 3.476980432083732...         3          2   \n3   {'loss': [3.549210316256473, 3.463777341340717...         3          3   \n4   {'loss': [3.548779399771439, 3.598160019046382...         6          1   \n..                                                ...       ...        ...   \n75  {'loss': [3.54023992387872, 3.5036307259609827...        10          6   \n76  {'loss': [3.5532967040413306, 3.48865530992809...        10          7   \n77  {'loss': [3.596015873708223, 3.476873178231089...        10          8   \n78  {'loss': [3.575199949113946, 3.524588001401801...        10          9   \n79  {'loss': [3.5736288396935714, 3.61792811594511...        10         10   \n\n    PreTrain    TrainTime  SavePathInput  \\\n0      False  6513.744140            NaN   \n1      False  2789.132076            NaN   \n2      False  2805.588549            NaN   \n3      False  2776.819473            NaN   \n4      False  1857.319781            NaN   \n..       ...          ...            ...   \n75     False  1980.333326            NaN   \n76     False  1936.754572            NaN   \n77     False  1963.545982            NaN   \n78     False  1930.976732            NaN   \n79     False  1963.606548            NaN   \n\n                                       SavePathOutput  \n0    ../SynchronizationProject/SaveData/Weights/0.pth  \n1   ../SynchronizationProject/SaveData/Weights/1T0...  \n2   ../SynchronizationProject/SaveData/Weights/1T1...  \n3   ../SynchronizationProject/SaveData/Weights/1T2...  \n4   ../SynchronizationProject/SaveData/Weights/2T0...  \n..                                                ...  \n75  ../SynchronizationProject/SaveData/Weights/15T...  \n76  ../SynchronizationProject/SaveData/Weights/15T...  \n77  ../SynchronizationProject/SaveData/Weights/15T...  \n78  ../SynchronizationProject/SaveData/Weights/15T...  \n79  ../SynchronizationProject/SaveData/Weights/15T...  \n\n[80 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Run</th>\n      <th>BatchSize</th>\n      <th>LearningRate</th>\n      <th>Epochs</th>\n      <th>TrainAccuracy</th>\n      <th>TrainLoss</th>\n      <th>TrainHistory</th>\n      <th>ValAccuracy</th>\n      <th>ValLoss</th>\n      <th>ValHistory</th>\n      <th>GPUCount</th>\n      <th>GPUNumber</th>\n      <th>PreTrain</th>\n      <th>TrainTime</th>\n      <th>SavePathInput</th>\n      <th>SavePathOutput</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>98.891674</td>\n      <td>0.046048</td>\n      <td>{'loss': [3.4871723436841777, 3.42232744647007...</td>\n      <td>18.830361</td>\n      <td>5.840313</td>\n      <td>{'loss': [3.490916973666141, 3.408613242601093...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>False</td>\n      <td>6513.744140</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/0.pth</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>99.954355</td>\n      <td>0.006749</td>\n      <td>{'loss': [3.5152197248795454, 3.43448365435880...</td>\n      <td>12.691829</td>\n      <td>5.167353</td>\n      <td>{'loss': [3.5044365682099996, 3.46959972381591...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>False</td>\n      <td>2789.132076</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/1T0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>99.989637</td>\n      <td>0.011231</td>\n      <td>{'loss': [3.5432352767271156, 3.46236025024862...</td>\n      <td>11.489009</td>\n      <td>4.928459</td>\n      <td>{'loss': [3.492495696795614, 3.476980432083732...</td>\n      <td>3</td>\n      <td>2</td>\n      <td>False</td>\n      <td>2805.588549</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/1T1...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>99.990760</td>\n      <td>0.004735</td>\n      <td>{'loss': [3.547464345483219, 3.464133195316090...</td>\n      <td>11.198673</td>\n      <td>5.037350</td>\n      <td>{'loss': [3.549210316256473, 3.463777341340717...</td>\n      <td>3</td>\n      <td>3</td>\n      <td>False</td>\n      <td>2776.819473</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/1T2...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>32</td>\n      <td>0.0001</td>\n      <td>50</td>\n      <td>99.991356</td>\n      <td>0.025019</td>\n      <td>{'loss': [3.5846809398296267, 3.42887252985045...</td>\n      <td>8.917462</td>\n      <td>5.132797</td>\n      <td>{'loss': [3.548779399771439, 3.598160019046382...</td>\n      <td>6</td>\n      <td>1</td>\n      <td>False</td>\n      <td>1857.319781</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/2T0...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>15</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>94.594299</td>\n      <td>0.204781</td>\n      <td>{'loss': [4.1057514777550335, 3.74745035171508...</td>\n      <td>3.815844</td>\n      <td>10.712571</td>\n      <td>{'loss': [3.54023992387872, 3.5036307259609827...</td>\n      <td>10</td>\n      <td>6</td>\n      <td>False</td>\n      <td>1980.333326</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/15T...</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>15</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>94.982639</td>\n      <td>0.172121</td>\n      <td>{'loss': [4.1091077877924995, 3.72035032052260...</td>\n      <td>7.009540</td>\n      <td>8.698870</td>\n      <td>{'loss': [3.5532967040413306, 3.48865530992809...</td>\n      <td>10</td>\n      <td>7</td>\n      <td>False</td>\n      <td>1936.754572</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/15T...</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>15</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>96.576995</td>\n      <td>0.137701</td>\n      <td>{'loss': [4.296231306516207, 3.586264573610746...</td>\n      <td>6.885110</td>\n      <td>9.257041</td>\n      <td>{'loss': [3.596015873708223, 3.476873178231089...</td>\n      <td>10</td>\n      <td>8</td>\n      <td>False</td>\n      <td>1963.545982</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/15T...</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>15</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>97.842856</td>\n      <td>0.118109</td>\n      <td>{'loss': [4.12328287271353, 3.6071051634274998...</td>\n      <td>5.723766</td>\n      <td>8.788563</td>\n      <td>{'loss': [3.575199949113946, 3.524588001401801...</td>\n      <td>10</td>\n      <td>9</td>\n      <td>False</td>\n      <td>1930.976732</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/15T...</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>15</td>\n      <td>64</td>\n      <td>0.0010</td>\n      <td>50</td>\n      <td>99.275489</td>\n      <td>0.030685</td>\n      <td>{'loss': [4.1716841000777025, 3.59731839253352...</td>\n      <td>5.350477</td>\n      <td>10.164243</td>\n      <td>{'loss': [3.5736288396935714, 3.61792811594511...</td>\n      <td>10</td>\n      <td>10</td>\n      <td>False</td>\n      <td>1963.606548</td>\n      <td>NaN</td>\n      <td>../SynchronizationProject/SaveData/Weights/15T...</td>\n    </tr>\n  </tbody>\n</table>\n<p>80 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InternalSave"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T06:08:30.455126100Z",
     "start_time": "2023-11-14T06:08:30.411088800Z"
    }
   },
   "id": "39cb2c50cdfb5588"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "Start\n",
      "MaximumWeight\n",
      "MinimumWeight\n",
      "MedianWeight\n",
      "MeanWeight\n",
      "2\n",
      "Start\n",
      "MaximumWeight\n",
      "MinimumWeight\n",
      "MedianWeight\n",
      "MeanWeight\n",
      "3\n",
      "Start\n",
      "MaximumWeight\n",
      "MinimumWeight\n",
      "MedianWeight\n",
      "MeanWeight\n",
      "4\n",
      "5\n",
      "Start\n",
      "MaximumWeight\n",
      "MinimumWeight\n",
      "MedianWeight\n",
      "MeanWeight\n",
      "6\n",
      "Start\n",
      "MaximumWeight\n",
      "MinimumWeight\n",
      "MedianWeight\n",
      "MeanWeight\n",
      "7\n",
      "Start\n",
      "MaximumWeight\n",
      "MinimumWeight\n",
      "MedianWeight\n",
      "MeanWeight\n",
      "8\n",
      "9\n",
      "Start\n",
      "MaximumWeight\n",
      "MinimumWeight\n",
      "MedianWeight\n",
      "MeanWeight\n",
      "10\n",
      "Start\n",
      "MaximumWeight\n",
      "MinimumWeight\n",
      "MedianWeight\n",
      "MeanWeight\n",
      "11\n",
      "Start\n",
      "MaximumWeight\n",
      "MinimumWeight\n",
      "MedianWeight\n",
      "MeanWeight\n",
      "12\n",
      "13\n",
      "Start\n",
      "MaximumWeight\n",
      "MinimumWeight\n",
      "MedianWeight\n",
      "MeanWeight\n",
      "14\n",
      "Start\n",
      "MaximumWeight\n",
      "MinimumWeight\n",
      "MedianWeight\n",
      "MeanWeight\n",
      "15\n",
      "Start\n",
      "MaximumWeight\n",
      "MinimumWeight\n",
      "MedianWeight\n",
      "MeanWeight\n"
     ]
    }
   ],
   "source": [
    "ProcessedData = pd.DataFrame(columns = [\"Run\", \"BatchSize\", \"LearningRate\", \"Epochs\", \"TrainAccuracy\", \"TrainLoss\", \"ValAccuracy\", \"ValLoss\", \"GPUCount\", \"MergeType\", \"PreTrain\", \"TrainTime\", \"SavePathOutput\"])\n",
    "\n",
    "total_runs = InternalSave['Run'].iloc[-1]\n",
    "\n",
    "for run_number in range(0, total_runs + 1):\n",
    "    print(run_number)\n",
    "    specific_run_rows = InternalSave[InternalSave['Run'] == run_number]\n",
    "    learning_rate = int(specific_run_rows[\"LearningRate\"].iloc[0])\n",
    "    batch_size = int(specific_run_rows[\"BatchSize\"].iloc[0])\n",
    "    epochs = specific_run_rows[\"Epochs\"].iloc[0]\n",
    "    train_time = specific_run_rows[\"TrainTime\"].iloc[0]\n",
    "    pretrain = specific_run_rows[\"PreTrain\"].iloc[0]\n",
    "    \n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "    if len(specific_run_rows) == 1:\n",
    "        \n",
    "        # Model Definition and Final Layer Edit\n",
    "        net = models.resnet50()\n",
    "        net.fc = nn.Linear(net.fc.in_features, 49)\n",
    "        net.load_state_dict(torch.load(list(specific_run_rows[\"SavePathOutput\"])[0]))\n",
    "        \n",
    "         # Load Model Onto GPU\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        net.to(device)\n",
    "\n",
    "        # Loss Function and Optimizer\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "        trainacc, trainloss = test_accuracy(net, train_loader, device, loss_function)\n",
    "        valacc, valloss = test_accuracy(net, test_loader, device, loss_function)\n",
    "\n",
    "        PATH = '../SynchronizationProject/SaveData/ProcessedWeights/' + str(run_number) + '.pth'\n",
    "        torch.save(net.state_dict(), PATH)\n",
    "\n",
    "        new_data_row = {\n",
    "            \"Run\": run_number,\n",
    "            \"BatchSize\": batch_size,\n",
    "            \"LearningRate\": learning_rate, \n",
    "            \"Epochs\": epochs,\n",
    "            \"TrainAccuracy\": trainacc,\n",
    "            \"TrainLoss\": trainloss,\n",
    "            \"ValAccuracy\": valacc,\n",
    "            \"ValLoss\": valloss,\n",
    "            \"GPUCount\": 1,\n",
    "            \"MergeType\": \"NONE\",\n",
    "            \"PreTrain\": pretrain, \n",
    "            \"TrainTime\": train_time,\n",
    "            \"SavePathOutput\": PATH\n",
    "        }\n",
    "\n",
    "        ProcessedData.loc[len(ProcessedData)] = new_data_row\n",
    "        \n",
    "    else:\n",
    "        weightsList = []\n",
    "        for pathindex in range(0,len(specific_run_rows)):\n",
    "            weightsList.append(torch.load(list(specific_run_rows[\"SavePathOutput\"])[pathindex]))\n",
    "        \n",
    "        print(\"Start\")\n",
    "        aggregatedData = []\n",
    "        # print(1)\n",
    "        aggregatedData.append(maximum_weight_aggregation(weightsList))\n",
    "        # print(2)\n",
    "        aggregatedData.append(minimum_weight_aggregation(weightsList))\n",
    "        # print(3)\n",
    "        aggregatedData.append(median_aggregation(weightsList))\n",
    "        aggregatedData.append(mean_aggregation(weightsList))\n",
    "        # print(4)\n",
    "        #aggregatedData.append(trimmed_mean_aggregation(weightsList, trim_fraction=0.2)) this causes issues and also like kinda doesnt work the weay i want it to. Easy to fix, just dont think i wanna use\n",
    "        # print(5)\n",
    "        #aggregatedData.append(geometric_mean_aggregation(weightsList)) #signed if there are negative numbers, regular if there are not NEED TO MAYBE WORK ON THIS, SOMETHING WITH THE num_negatives CAn hasve a 0 INDICE,. AND THAT NEED TO BE HANDELED PROPERLY\n",
    "        #print(6)\n",
    "        #aggregatedData.append(harmonic_mean_aggregation(weightsList))\n",
    "        #aggregatedData.append(weighted_average_aggregation(weightsList, model_weights))  Decide on this later    \n",
    "        \n",
    "        for item in aggregatedData:\n",
    "            print(item[1])\n",
    "            # Model Definition and Final Layer Edit\n",
    "            net = models.resnet50()\n",
    "            net.fc = nn.Linear(net.fc.in_features, 49)\n",
    "            \n",
    "            net.load_state_dict(item[0])\n",
    "            \n",
    "             # Load Model Onto GPU\n",
    "            device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "            net.to(device)\n",
    "            \n",
    "            # Loss Function and Optimizer\n",
    "            loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "            trainacc, trainloss = test_accuracy(net, train_loader, device, loss_function)\n",
    "            valacc, valloss = test_accuracy(net, test_loader, device, loss_function)\n",
    "\n",
    "            PATH = '../SynchronizationProject/SaveData/ProcessedWeights/' + str(run_number) + str(item[1]) + '.pth'\n",
    "            torch.save(net.state_dict(), PATH)\n",
    "\n",
    "            new_data_row = {\n",
    "                \"Run\": run_number, \n",
    "                \"BatchSize\": batch_size, \n",
    "                \"LearningRate\": learning_rate, \n",
    "                \"Epochs\": epochs, \n",
    "                \"TrainAccuracy\": trainacc, \n",
    "                \"TrainLoss\": trainloss, \n",
    "                \"ValAccuracy\": valacc, \n",
    "                \"ValLoss\": valloss, \n",
    "                \"GPUCount\": len(specific_run_rows), \n",
    "                \"MergeType\": item[1], \n",
    "                \"PreTrain\": pretrain, \n",
    "                \"TrainTime\": train_time,\n",
    "                \"SavePathOutput\": PATH\n",
    "            }\n",
    "\n",
    "            ProcessedData.loc[len(ProcessedData)] = new_data_row\n",
    "\n",
    "file_path = '../SynchronizationProject/SaveData/DataFrames/ProcessedData.csv'\n",
    "\n",
    "ProcessedData.to_csv(file_path, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T07:12:39.774797400Z",
     "start_time": "2023-11-14T06:08:30.451122600Z"
    }
   },
   "id": "36dd4a79de1b0fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9b7527e29f1a5008"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
